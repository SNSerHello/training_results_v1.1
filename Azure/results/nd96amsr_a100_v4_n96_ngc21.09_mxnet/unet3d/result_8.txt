+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019055607572659362
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019055607572659362
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019055607572659362
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019055607572659362
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07365/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019055607572659362_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0411
Clearing cache on ip-0A0C0419
Clearing cache on ip-0A0C040A
Clearing cache on ip-0A0C0423
Clearing cache on ip-0A0C041B
Clearing cache on ip-0A0C0417
Clearing cache on ip-0A0C0407
Clearing cache on ip-0A0C0427
Clearing cache on ip-0A0C0431
Clearing cache on ip-0A0C0412
Clearing cache on ip-0A0C042E
Clearing cache on ip-0A0C0416
Clearing cache on ip-0A0C0409
Clearing cache on ip-0A0C041D
Clearing cache on ip-0A0C0424
Clearing cache on ip-0A0C0410
Clearing cache on ip-0A0C042F
Clearing cache on ip-0A0C042B
Clearing cache on ip-0A0C0413
Clearing cache on ip-0A0C040E
Clearing cache on ip-0A0C040B
Clearing cache on ip-0A0C0408
Clearing cache on ip-0A0C042C
Clearing cache on ip-0A0C040C
Clearing cache on ip-0A0C0418
Clearing cache on ip-0A0C041A
Clearing cache on ip-0A0C0518
Clearing cache on ip-0A0C0433
Clearing cache on ip-0A0C041C
Clearing cache on ip-0A0C0414
Clearing cache on ip-0A0C045C
Clearing cache on ip-0A0C0421
Clearing cache on ip-0A0C0425
Clearing cache on ip-0A0C047C
Clearing cache on ip-0A0C045A
Clearing cache on ip-0A0C0446
Clearing cache on ip-0A0C0481
Clearing cache on ip-0A0C0475
Clearing cache on ip-0A0C045E
Clearing cache on ip-0A0C0450
Clearing cache on ip-0A0C046D
Clearing cache on ip-0A0C047A
Clearing cache on ip-0A0C043F
Clearing cache on ip-0A0C0420
Clearing cache on ip-0A0C0455
Clearing cache on ip-0A0C041F
Clearing cache on ip-0A0C0474
Clearing cache on ip-0A0C0459
Clearing cache on ip-0A0C0426
Clearing cache on ip-0A0C0453
Clearing cache on ip-0A0C044F
Clearing cache on ip-0A0C0445
Clearing cache on ip-0A0C043D
Clearing cache on ip-0A0C0457
Clearing cache on ip-0A0C045B
Clearing cache on ip-0A0C0447
Clearing cache on ip-0A0C0439
Clearing cache on ip-0A0C0463
Clearing cache on ip-0A0C0442
Clearing cache on ip-0A0C043C
Clearing cache on ip-0A0C0440
Clearing cache on ip-0A0C0422
Clearing cache on ip-0A0C0480
Clearing cache on ip-0A0C0479
Clearing cache on ip-0A0C045D
Clearing cache on ip-0A0C0467
Clearing cache on ip-0A0C0462
Clearing cache on ip-0A0C043B
Clearing cache on ip-0A0C0432
Clearing cache on ip-0A0C047B
Clearing cache on ip-0A0C047D
Clearing cache on ip-0A0C044C
Clearing cache on ip-0A0C0470
Clearing cache on ip-0A0C0428
Clearing cache on ip-0A0C042A
Clearing cache on ip-0A0C0458
Clearing cache on ip-0A0C043E
Clearing cache on ip-0A0C0448
Clearing cache on ip-0A0C0456
Clearing cache on ip-0A0C041E
Clearing cache on ip-0A0C047F
Clearing cache on ip-0A0C0454
Clearing cache on ip-0A0C0465
Clearing cache on ip-0A0C0452
Clearing cache on ip-0A0C0434
Clearing cache on ip-0A0C044B
Clearing cache on ip-0A0C043A
Clearing cache on ip-0A0C0451
Clearing cache on ip-0A0C0471
Clearing cache on ip-0A0C0430
Clearing cache on ip-0A0C046A
Clearing cache on ip-0A0C044A
Clearing cache on ip-0A0C0469
Clearing cache on ip-0A0C044E
Clearing cache on ip-0A0C0477
Clearing cache on ip-0A0C0476
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:56:10 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634622975.103449] [ip-0A0C0410:79592:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.103491] [ip-0A0C0410:79590:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.142881] [ip-0A0C040C:75412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.150235] [ip-0A0C040E:50902:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.168150] [ip-0A0C0410:79586:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.174105] [ip-0A0C040E:50911:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.181195] [ip-0A0C0409:26970:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.196592] [ip-0A0C041B:80916:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.221515] [ip-0A0C0410:79589:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.222146] [ip-0A0C0453:48647:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.222956] [ip-0A0C040E:50901:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.235236] [ip-0A0C040B:8946 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.240404] [ip-0A0C040C:75407:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.240639] [ip-0A0C0409:26967:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.244925] [ip-0A0C040C:75408:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.246141] [ip-0A0C041B:80920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.249625] [ip-0A0C0453:48646:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.270641] [ip-0A0C0410:79588:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.271341] [ip-0A0C0416:50038:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.276772] [ip-0A0C0410:79587:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.277356] [ip-0A0C040C:75409:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.282787] [ip-0A0C0457:43233:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.285181] [ip-0A0C041B:80917:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.287341] [ip-0A0C0410:79591:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.287348] [ip-0A0C042E:62003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.288911] [ip-0A0C043F:44042:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.290074] [ip-0A0C0416:50021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.298242] [ip-0A0C0410:79593:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.300702] [ip-0A0C040A:64512:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.303094] [ip-0A0C0409:26963:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.311980] [ip-0A0C0409:26971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.313702] [ip-0A0C0518:61568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.313866] [ip-0A0C0453:48650:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.313975] [ip-0A0C0470:28074:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.314601] [ip-0A0C0416:50022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.315279] [ip-0A0C0465:35071:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.318577] [ip-0A0C040C:75411:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.324455] [ip-0A0C040E:50905:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.329743] [ip-0A0C040B:8949 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.330164] [ip-0A0C040E:50904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.333020] [ip-0A0C0446:50897:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.332523] [ip-0A0C046A:38158:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.333711] [ip-0A0C040C:75410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.333053] [ip-0A0C0442:42731:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.338028] [ip-0A0C0450:18701:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.337689] [ip-0A0C0445:53522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.338350] [ip-0A0C0457:43231:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.338644] [ip-0A0C040E:50900:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.339063] [ip-0A0C0448:54718:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.340048] [ip-0A0C040A:64484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.340657] [ip-0A0C047D:30724:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.341993] [ip-0A0C0409:26969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.343302] [ip-0A0C0442:42738:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.344616] [ip-0A0C0518:61567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.344993] [ip-0A0C040E:50898:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.345736] [ip-0A0C040C:75413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.345970] [ip-0A0C040E:50899:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.346165] [ip-0A0C047B:36837:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.347998] [ip-0A0C0431:53419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.349735] [ip-0A0C0428:51725:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.350316] [ip-0A0C040C:75414:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.351367] [ip-0A0C040B:8950 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.353442] [ip-0A0C0450:18704:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.355033] [ip-0A0C040A:64485:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.355627] [ip-0A0C0409:26968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.355711] [ip-0A0C0445:53525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.356259] [ip-0A0C043B:48397:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.356558] [ip-0A0C041B:80918:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.358151] [ip-0A0C047F:33498:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.360157] [ip-0A0C0414:79597:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.360152] [ip-0A0C0414:79595:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.360150] [ip-0A0C047B:36834:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.360381] [ip-0A0C040B:8943 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.364039] [ip-0A0C0427:59815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.365080] [ip-0A0C0458:51063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.365544] [ip-0A0C043C:49120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.369144] [ip-0A0C0409:26966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.369243] [ip-0A0C0409:26965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.374361] [ip-0A0C042E:61989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.374940] [ip-0A0C0412:42420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.376605] [ip-0A0C046D:31791:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.376908] [ip-0A0C0474:29080:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.377361] [ip-0A0C043F:44039:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.378539] [ip-0A0C0446:50902:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.378460] [ip-0A0C0458:51064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.381580] [ip-0A0C041B:80915:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.384335] [ip-0A0C041F:66757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.384177] [ip-0A0C0412:42425:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.386631] [ip-0A0C0422:88547:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.388079] [ip-0A0C043C:49122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.388938] [ip-0A0C0448:54713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.387719] [ip-0A0C042E:61986:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.390608] [ip-0A0C041B:80913:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.390970] [ip-0A0C0447:50002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.397069] [ip-0A0C044B:46396:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.397398] [ip-0A0C0457:43230:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.397570] [ip-0A0C0467:33240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.397816] [ip-0A0C047F:33501:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.398704] [ip-0A0C0453:48648:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.398671] [ip-0A0C046A:38155:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.401022] [ip-0A0C043F:44040:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.401471] [ip-0A0C0428:51728:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.402423] [ip-0A0C0418:59570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.404621] [ip-0A0C045D:34546:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.405760] [ip-0A0C040B:8945 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.406751] [ip-0A0C044C:45121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.406682] [ip-0A0C043E:36799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.407549] [ip-0A0C0430:91949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.407615] [ip-0A0C047D:30726:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.411154] [ip-0A0C0467:33232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.412741] [ip-0A0C0426:61448:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.412055] [ip-0A0C043A:48342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.412805] [ip-0A0C041B:80919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.412846] [ip-0A0C0421:64902:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.414706] [ip-0A0C047B:36832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.414961] [ip-0A0C0408:87202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.415233] [ip-0A0C041B:80914:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.416210] [ip-0A0C0419:67235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.417366] [ip-0A0C0453:48649:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.417905] [ip-0A0C0431:53417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.418478] [ip-0A0C0453:48645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.419403] [ip-0A0C040B:8947 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.418367] [ip-0A0C042E:61991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.419154] [ip-0A0C0442:42735:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.419815] [ip-0A0C042F:58119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.421170] [ip-0A0C040B:8944 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.422705] [ip-0A0C0450:18700:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.422780] [ip-0A0C0518:61566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.425582] [ip-0A0C040B:8948 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.425858] [ip-0A0C047F:33505:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.425800] [ip-0A0C0453:48643:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.427535] [ip-0A0C0428:51724:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.428748] [ip-0A0C0448:54719:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.431711] [ip-0A0C045A:47058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.432123] [ip-0A0C0431:53422:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.433563] [ip-0A0C0470:28071:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.434211] [ip-0A0C0418:59576:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.434200] [ip-0A0C0422:88548:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.437420] [ip-0A0C0447:50004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.437349] [ip-0A0C043F:44043:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.437600] [ip-0A0C0480:29419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.438727] [ip-0A0C041F:66755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.439480] [ip-0A0C0439:40227:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.439506] [ip-0A0C0439:40226:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.440629] [ip-0A0C0453:48644:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.439871] [ip-0A0C0412:42426:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.441529] [ip-0A0C0418:59571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.441093] [ip-0A0C040A:64482:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.441566] [ip-0A0C0424:51754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.441558] [ip-0A0C0465:35075:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.441826] [ip-0A0C042B:60812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.442942] [ip-0A0C0416:50023:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.443209] [ip-0A0C0426:61449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.443299] [ip-0A0C041C:68791:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.443190] [ip-0A0C043A:48340:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.443626] [ip-0A0C044E:40321:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.445211] [ip-0A0C0430:91942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.445380] [ip-0A0C0425:66023:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.445739] [ip-0A0C0454:47533:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.446566] [ip-0A0C0416:50024:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.446174] [ip-0A0C0442:42734:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.447472] [ip-0A0C044C:45146:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.449253] [ip-0A0C044B:46390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.449641] [ip-0A0C0474:29077:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.451428] [ip-0A0C0470:28073:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.451554] [ip-0A0C0474:29076:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.451466] [ip-0A0C045C:43802:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.452261] [ip-0A0C041D:59624:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.452836] [ip-0A0C0457:43236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.453463] [ip-0A0C045A:47055:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.453601] [ip-0A0C0416:50026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.454504] [ip-0A0C040A:64483:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.455039] [ip-0A0C0425:66021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.457103] [ip-0A0C043B:48396:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.457825] [ip-0A0C0407:67711:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.457739] [ip-0A0C0465:35066:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.457877] [ip-0A0C0422:88550:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.459568] [ip-0A0C0446:50901:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.460863] [ip-0A0C0414:79596:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.460087] [ip-0A0C043F:44046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.460191] [ip-0A0C043B:48401:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.460659] [ip-0A0C0419:67257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.461592] [ip-0A0C0416:50027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.462612] [ip-0A0C0414:79594:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.463103] [ip-0A0C044B:46389:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.462489] [ip-0A0C046A:38161:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.465002] [ip-0A0C0446:50898:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.464944] [ip-0A0C045E:44426:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.466290] [ip-0A0C047B:36836:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.467366] [ip-0A0C0416:50025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.467205] [ip-0A0C0457:43259:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.466172] [ip-0A0C042E:61987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.467957] [ip-0A0C0424:51783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.471235] [ip-0A0C043E:36780:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.470927] [ip-0A0C042F:58122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.471464] [ip-0A0C046D:31795:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.471341] [ip-0A0C043E:36777:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.472153] [ip-0A0C045C:43805:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.473475] [ip-0A0C047D:30722:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.475832] [ip-0A0C047B:36838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.477170] [ip-0A0C0432:84129:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.477823] [ip-0A0C043C:49124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.478548] [ip-0A0C041D:59631:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.481200] [ip-0A0C0448:54715:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.480858] [ip-0A0C043F:44045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.482572] [ip-0A0C045D:34525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.482373] [ip-0A0C043D:39278:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.482206] [ip-0A0C042E:61984:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.484140] [ip-0A0C045D:34521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.484382] [ip-0A0C043F:44044:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.484269] [ip-0A0C040A:64488:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.485869] [ip-0A0C044C:45119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.485137] [ip-0A0C042E:61985:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.486676] [ip-0A0C046D:31789:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.489423] [ip-0A0C045B:50930:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.488840] [ip-0A0C044F:40951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.489171] [ip-0A0C0470:28068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.489516] [ip-0A0C0467:33238:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.489618] [ip-0A0C041F:66753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.490119] [ip-0A0C040A:64486:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.490735] [ip-0A0C045D:34524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.490793] [ip-0A0C0470:28067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.490871] [ip-0A0C042A:53501:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.491062] [ip-0A0C0471:32497:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.491531] [ip-0A0C0518:61593:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.491000] [ip-0A0C0459:43239:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.492128] [ip-0A0C0518:61571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.491136] [ip-0A0C046A:38157:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.491900] [ip-0A0C040A:64487:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.492654] [ip-0A0C0463:37577:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.493140] [ip-0A0C0446:50899:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.492526] [ip-0A0C047C:40176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.493014] [ip-0A0C0431:53416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.493612] [ip-0A0C0480:29421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.494037] [ip-0A0C0450:18705:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.494180] [ip-0A0C0454:47528:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.493712] [ip-0A0C042E:61983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.496064] [ip-0A0C043E:36778:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.496445] [ip-0A0C0418:59574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.496384] [ip-0A0C0477:35008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.496587] [ip-0A0C0411:62585:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.497441] [ip-0A0C0475:29275:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.498545] [ip-0A0C045B:50926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.497341] [ip-0A0C045C:43808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.498370] [ip-0A0C044F:40950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.498751] [ip-0A0C043F:44041:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.500604] [ip-0A0C0423:56742:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.499973] [ip-0A0C0427:59814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.500208] [ip-0A0C0474:29078:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.500386] [ip-0A0C0465:35068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.500881] [ip-0A0C0427:59821:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.502166] [ip-0A0C044B:46391:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.502816] [ip-0A0C0455:41976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.503138] [ip-0A0C041E:62762:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.503280] [ip-0A0C0428:51723:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.503955] [ip-0A0C045A:47059:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.504873] [ip-0A0C0430:91947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.505503] [ip-0A0C043A:48336:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.507454] [ip-0A0C047F:33504:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.507909] [ip-0A0C0469:32645:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.507597] [ip-0A0C0457:43232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.508511] [ip-0A0C043B:48400:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.508887] [ip-0A0C0457:43235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.509420] [ip-0A0C0518:61570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.508917] [ip-0A0C0442:42737:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.509993] [ip-0A0C0420:64112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.510205] [ip-0A0C0440:42305:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.511180] [ip-0A0C047D:30723:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.511350] [ip-0A0C0465:35067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.511710] [ip-0A0C0465:35070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.513733] [ip-0A0C0447:50005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.514216] [ip-0A0C041C:68790:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.514570] [ip-0A0C043C:49125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.515623] [ip-0A0C0419:67232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.516991] [ip-0A0C0423:56743:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.517054] [ip-0A0C0518:61569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.516816] [ip-0A0C0477:35006:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.516971] [ip-0A0C0432:84131:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.517242] [ip-0A0C0518:61581:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.516971] [ip-0A0C0419:67238:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.517708] [ip-0A0C0474:29081:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.516938] [ip-0A0C045C:43806:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.518282] [ip-0A0C0431:53418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.517891] [ip-0A0C042F:58121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.518765] [ip-0A0C0457:43234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.518816] [ip-0A0C0445:53520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.519600] [ip-0A0C042B:60815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.520172] [ip-0A0C0407:67727:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.520308] [ip-0A0C0447:49999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.521359] [ip-0A0C047D:30728:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.521694] [ip-0A0C042A:53500:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.522833] [ip-0A0C045E:44421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.523879] [ip-0A0C044A:42186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.524019] [ip-0A0C047B:36835:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.524229] [ip-0A0C047B:36833:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.525558] [ip-0A0C0414:79592:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.524062] [ip-0A0C0442:42732:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.525036] [ip-0A0C0470:28072:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.525018] [ip-0A0C047A:35196:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.524165] [ip-0A0C0442:42733:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.527325] [ip-0A0C0450:18707:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.527464] [ip-0A0C0450:18706:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.527519] [ip-0A0C0439:40231:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.527585] [ip-0A0C0428:51726:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.527808] [ip-0A0C0450:18702:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.526654] [ip-0A0C0442:42736:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.527220] [ip-0A0C047D:30727:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.527128] [ip-0A0C046A:38160:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.528193] [ip-0A0C0470:28070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.528696] [ip-0A0C0456:50665:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.529010] [ip-0A0C0427:59816:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.529273] [ip-0A0C0422:88544:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.530413] [ip-0A0C047F:33502:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.530807] [ip-0A0C0421:64898:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.531144] [ip-0A0C0454:47530:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.531215] [ip-0A0C0470:28069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.531664] [ip-0A0C0465:35072:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.532531] [ip-0A0C0450:18703:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.532354] [ip-0A0C0463:37579:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.532409] [ip-0A0C0431:53420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.532380] [ip-0A0C0445:53526:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.532803] [ip-0A0C047B:36831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.532975] [ip-0A0C044B:46393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.533142] [ip-0A0C043B:48394:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.533966] [ip-0A0C043C:49126:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.535153] [ip-0A0C0431:53421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.536193] [ip-0A0C0475:29278:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.536401] [ip-0A0C041F:66758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.537802] [ip-0A0C0431:53423:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.538050] [ip-0A0C0454:47527:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.537745] [ip-0A0C0476:23258:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.538429] [ip-0A0C0445:53527:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.538439] [ip-0A0C0465:35069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.538432] [ip-0A0C043C:49121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.538598] [ip-0A0C042F:58125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.539151] [ip-0A0C0440:42300:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.540664] [ip-0A0C0420:64110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.541008] [ip-0A0C041D:59626:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.541284] [ip-0A0C0481:28301:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.541994] [ip-0A0C0428:51731:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.541859] [ip-0A0C0480:29415:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.542394] [ip-0A0C0413:94878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.543876] [ip-0A0C0446:50900:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.544407] [ip-0A0C0446:50903:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.544809] [ip-0A0C0414:79593:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.543328] [ip-0A0C046A:38162:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.544450] [ip-0A0C0424:51757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.544861] [ip-0A0C047D:30725:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.545451] [ip-0A0C0448:54717:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.545316] [ip-0A0C0477:35004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.545102] [ip-0A0C047D:30729:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.544328] [ip-0A0C0412:42428:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.545469] [ip-0A0C0407:67705:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.546087] [ip-0A0C0446:50904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.546699] [ip-0A0C0458:51066:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.546797] [ip-0A0C0458:51069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.547664] [ip-0A0C046D:31793:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.547824] [ip-0A0C043B:48399:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.548997] [ip-0A0C0455:41977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.549026] [ip-0A0C043B:48395:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.548764] [ip-0A0C0417:93560:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.549642] [ip-0A0C0456:50666:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.551115] [ip-0A0C0414:79591:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.550835] [ip-0A0C047C:40177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.550376] [ip-0A0C046A:38156:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.551912] [ip-0A0C0448:54716:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.551390] [ip-0A0C0408:87201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.551412] [ip-0A0C041F:66759:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.551133] [ip-0A0C0412:42424:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.551216] [ip-0A0C0412:42422:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.552437] [ip-0A0C0475:29274:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.551948] [ip-0A0C0422:88546:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.552154] [ip-0A0C0408:87200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.552512] [ip-0A0C0445:53524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.552692] [ip-0A0C047C:40183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.553330] [ip-0A0C0428:51722:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.553403] [ip-0A0C0428:51730:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.553064] [ip-0A0C0413:94871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.553727] [ip-0A0C042C:56185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.553979] [ip-0A0C047F:33500:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.553951] [ip-0A0C0421:64904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.555164] [ip-0A0C0414:79598:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.555585] [ip-0A0C047F:33497:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.555548] [ip-0A0C0408:87203:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.555725] [ip-0A0C0420:64105:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.556249] [ip-0A0C044C:45125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.556657] [ip-0A0C0445:53523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.556961] [ip-0A0C046A:38159:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.558147] [ip-0A0C047A:35199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.558166] [ip-0A0C042A:53498:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.558446] [ip-0A0C045A:47057:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.559506] [ip-0A0C047F:33499:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.559897] [ip-0A0C0448:54714:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.559877] [ip-0A0C0469:32621:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.559787] [ip-0A0C0474:29074:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.562442] [ip-0A0C0448:54721:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.562232] [ip-0A0C0425:66020:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.562906] [ip-0A0C0467:33234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.562953] [ip-0A0C041F:66760:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.564282] [ip-0A0C043B:48398:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.564693] [ip-0A0C046D:31788:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.564729] [ip-0A0C0458:51065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.564828] [ip-0A0C046D:31792:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.565802] [ip-0A0C044E:40319:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.566054] [ip-0A0C0458:51068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.566287] [ip-0A0C0422:88545:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.567362] [ip-0A0C0445:53521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.567155] [ip-0A0C043C:49127:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.567751] [ip-0A0C0411:62583:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.568088] [ip-0A0C044E:40320:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.568191] [ip-0A0C0462:74467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.568190] [ip-0A0C0462:74490:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.569113] [ip-0A0C0424:51759:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.569373] [ip-0A0C0421:64905:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.570434] [ip-0A0C0451:45570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.570390] [ip-0A0C0421:64903:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.570297] [ip-0A0C043C:49123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.571160] [ip-0A0C0458:51070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.571519] [ip-0A0C0471:32503:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.572000] [ip-0A0C0427:59820:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.571277] [ip-0A0C045C:43807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.571504] [ip-0A0C045C:43801:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.572560] [ip-0A0C0474:29079:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.573070] [ip-0A0C0447:50006:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.573444] [ip-0A0C0447:50003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.573623] [ip-0A0C0458:51067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.574075] [ip-0A0C0407:67710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.574180] [ip-0A0C0412:42421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.574653] [ip-0A0C043A:48341:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.576544] [ip-0A0C0430:91944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.576700] [ip-0A0C0467:33236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.577173] [ip-0A0C044C:45120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.577028] [ip-0A0C0422:88549:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.577010] [ip-0A0C0412:42427:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.578535] [ip-0A0C0474:29075:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.578670] [ip-0A0C0432:84136:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.579647] [ip-0A0C045E:44423:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.580171] [ip-0A0C044A:42187:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.579906] [ip-0A0C041F:66754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.580364] [ip-0A0C0451:45575:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.580496] [ip-0A0C0427:59817:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.580669] [ip-0A0C0427:59819:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.580276] [ip-0A0C043A:48339:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.583653] [ip-0A0C046D:31794:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.583881] [ip-0A0C047A:35195:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.583386] [ip-0A0C0422:88552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.584242] [ip-0A0C041C:68812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.584819] [ip-0A0C0433:56988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.585052] [ip-0A0C0471:32504:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.586225] [ip-0A0C0427:59818:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.586859] [ip-0A0C0426:61446:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.587234] [ip-0A0C0426:61445:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.587920] [ip-0A0C044B:46394:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.588748] [ip-0A0C0418:59573:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.588648] [ip-0A0C046D:31790:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.589275] [ip-0A0C0418:59572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.590122] [ip-0A0C044C:45122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.590091] [ip-0A0C0434:51658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.590140] [ip-0A0C041F:66756:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.590650] [ip-0A0C0421:64899:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.590754] [ip-0A0C043D:39279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.591088] [ip-0A0C042B:60817:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.591525] [ip-0A0C0479:30874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.591490] [ip-0A0C0455:41981:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.592756] [ip-0A0C0467:33233:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.593033] [ip-0A0C044B:46395:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.593217] [ip-0A0C044B:46392:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.594423] [ip-0A0C041E:62764:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.594537] [ip-0A0C041E:62768:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.594995] [ip-0A0C0480:29417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.595113] [ip-0A0C0424:51758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.595576] [ip-0A0C043E:36775:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.595382] [ip-0A0C042F:58118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.595950] [ip-0A0C0481:28290:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.596271] [ip-0A0C0467:33237:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.597356] [ip-0A0C0430:91946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.598409] [ip-0A0C0476:23260:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.599165] [ip-0A0C0418:59575:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.599100] [ip-0A0C044E:40322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.599819] [ip-0A0C0421:64901:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.601254] [ip-0A0C045D:34523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.601559] [ip-0A0C041C:68789:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.601976] [ip-0A0C0418:59577:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.601785] [ip-0A0C0447:50001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.601869] [ip-0A0C0447:50000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.603378] [ip-0A0C042A:53499:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.604303] [ip-0A0C0408:87204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.604329] [ip-0A0C0408:87197:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.605158] [ip-0A0C0452:46737:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.605760] [ip-0A0C0452:46756:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.606133] [ip-0A0C042B:60813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.605664] [ip-0A0C0417:93558:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.607018] [ip-0A0C0417:93555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.607800] [ip-0A0C042F:58123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.608690] [ip-0A0C0469:32619:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.608104] [ip-0A0C041D:59629:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.608339] [ip-0A0C0459:43241:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.609505] [ip-0A0C045D:34520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.609826] [ip-0A0C0439:40228:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.610623] [ip-0A0C044C:45118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.610683] [ip-0A0C044C:45123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.610308] [ip-0A0C045C:43803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.610625] [ip-0A0C043A:48338:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.610776] [ip-0A0C043A:48335:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.610908] [ip-0A0C043A:48337:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.611975] [ip-0A0C042C:56184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.612044] [ip-0A0C0467:33235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.612890] [ip-0A0C043E:36774:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.613439] [ip-0A0C045C:43804:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.615810] [ip-0A0C045D:34522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.616200] [ip-0A0C0430:91948:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.616449] [ip-0A0C0424:51756:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.616116] [ip-0A0C042F:58120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.617223] [ip-0A0C045A:47053:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.617328] [ip-0A0C0425:66019:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.617126] [ip-0A0C042F:58124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.618038] [ip-0A0C0411:62588:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.617963] [ip-0A0C043E:36776:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.618101] [ip-0A0C043E:36779:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.619138] [ip-0A0C0432:84135:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.619992] [ip-0A0C0423:56740:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.619753] [ip-0A0C043D:39273:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.620521] [ip-0A0C045D:34527:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.621719] [ip-0A0C0426:61444:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.621505] [ip-0A0C0480:29418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.622063] [ip-0A0C0456:50664:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.623558] [ip-0A0C0463:37576:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.623466] [ip-0A0C0419:67240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.623603] [ip-0A0C0419:67236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.623945] [ip-0A0C0439:40229:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.623678] [ip-0A0C0421:64900:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.624133] [ip-0A0C0439:40225:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.624441] [ip-0A0C041C:68792:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.625353] [ip-0A0C042B:60814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.625756] [ip-0A0C0439:40232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.625433] [ip-0A0C0408:87198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.626029] [ip-0A0C0425:66022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.626470] [ip-0A0C041E:62766:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.626679] [ip-0A0C0424:51755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.626821] [ip-0A0C0430:91943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.627611] [ip-0A0C044E:40315:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.627687] [ip-0A0C044E:40318:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.628335] [ip-0A0C045A:47060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.628260] [ip-0A0C042B:60819:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.628442] [ip-0A0C0408:87199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.629370] [ip-0A0C0426:61442:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.631406] [ip-0A0C0481:28293:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.632631] [ip-0A0C044F:40957:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.632570] [ip-0A0C0459:43237:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.634392] [ip-0A0C0471:32499:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.634447] [ip-0A0C0439:40230:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.634352] [ip-0A0C0425:66045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.635460] [ip-0A0C0419:67234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.636493] [ip-0A0C0424:51760:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.636627] [ip-0A0C0407:67707:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.637233] [ip-0A0C0433:56990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.637200] [ip-0A0C0477:35010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.637412] [ip-0A0C0430:91945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.639108] [ip-0A0C041D:59628:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.640043] [ip-0A0C0419:67233:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.640674] [ip-0A0C0440:42306:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.641098] [ip-0A0C0454:47529:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.640771] [ip-0A0C041A:86406:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.640811] [ip-0A0C041A:86407:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.641558] [ip-0A0C0407:67708:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.642762] [ip-0A0C045A:47056:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.642812] [ip-0A0C0480:29420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.642907] [ip-0A0C047C:40178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.643384] [ip-0A0C0426:61443:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.643407] [ip-0A0C0426:61447:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.643898] [ip-0A0C045A:47054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.644617] [ip-0A0C044A:42190:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.644299] [ip-0A0C041C:68793:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.644415] [ip-0A0C041C:68795:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.646156] [ip-0A0C0411:62587:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.646765] [ip-0A0C045E:44420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.646708] [ip-0A0C0407:67713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.646834] [ip-0A0C0432:84132:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.646412] [ip-0A0C0417:93584:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.647197] [ip-0A0C0477:35005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.647085] [ip-0A0C041D:59625:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.648022] [ip-0A0C0480:29422:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.648204] [ip-0A0C0434:51653:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.648635] [ip-0A0C0455:41979:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.648843] [ip-0A0C042B:60816:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.649400] [ip-0A0C044F:40954:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.650569] [ip-0A0C044F:40955:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.650999] [ip-0A0C042A:53503:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.651410] [ip-0A0C0454:47525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.651665] [ip-0A0C0481:28295:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.652716] [ip-0A0C0425:66024:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.652813] [ip-0A0C0425:66018:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.653695] [ip-0A0C0407:67706:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.653751] [ip-0A0C043D:39274:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.653835] [ip-0A0C041C:68796:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.654809] [ip-0A0C045E:44424:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.654615] [ip-0A0C047A:35198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.655746] [ip-0A0C0454:47531:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.656164] [ip-0A0C0480:29416:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.656540] [ip-0A0C042B:60838:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.656756] [ip-0A0C044E:40317:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.657630] [ip-0A0C044F:40953:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.658564] [ip-0A0C0454:47526:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.659473] [ip-0A0C042C:56187:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.659028] [ip-0A0C041D:59630:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.659833] [ip-0A0C044E:40316:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.660879] [ip-0A0C0423:56746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.659907] [ip-0A0C041D:59627:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.661643] [ip-0A0C0463:37580:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.662124] [ip-0A0C0440:42299:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.664104] [ip-0A0C045B:50925:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.664236] [ip-0A0C045B:50929:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.664289] [ip-0A0C0459:43242:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.668867] [ip-0A0C0459:43238:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.669897] [ip-0A0C045E:44427:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.669918] [ip-0A0C0452:46736:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.671116] [ip-0A0C0423:56745:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.670717] [ip-0A0C0455:41983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.671690] [ip-0A0C045B:50927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.673278] [ip-0A0C0411:62584:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.673796] [ip-0A0C042A:53504:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.674112] [ip-0A0C0476:23267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.675219] [ip-0A0C0432:84128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.675361] [ip-0A0C0432:84134:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.675534] [ip-0A0C0413:94872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.675954] [ip-0A0C0475:29279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.676030] [ip-0A0C0475:29280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.676442] [ip-0A0C0420:64135:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.678285] [ip-0A0C0411:62581:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.678208] [ip-0A0C043D:39277:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.679131] [ip-0A0C042C:56181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.679506] [ip-0A0C0432:84130:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.680142] [ip-0A0C0433:56994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.680775] [ip-0A0C042A:53505:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.681600] [ip-0A0C042A:53502:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.682790] [ip-0A0C0477:35009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.682910] [ip-0A0C0476:23255:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.683906] [ip-0A0C044A:42191:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.683768] [ip-0A0C0411:62586:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.684431] [ip-0A0C043D:39275:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.685518] [ip-0A0C0479:30875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.685858] [ip-0A0C045E:44422:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.685604] [ip-0A0C043D:39276:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.685513] [ip-0A0C044F:40969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.686335] [ip-0A0C044F:40952:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.687157] [ip-0A0C0471:32502:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.687201] [ip-0A0C0413:94870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.688679] [ip-0A0C044A:42189:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.689188] [ip-0A0C047C:40182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.689557] [ip-0A0C0475:29281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.689852] [ip-0A0C0475:29277:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.691024] [ip-0A0C0469:32617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.691465] [ip-0A0C045E:44425:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.692217] [ip-0A0C041E:62761:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.693115] [ip-0A0C0471:32501:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.693193] [ip-0A0C041E:62767:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.695169] [ip-0A0C0471:32500:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.695555] [ip-0A0C0456:50667:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.695620] [ip-0A0C0420:64106:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.696097] [ip-0A0C0411:62582:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.695993] [ip-0A0C0434:51652:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.696303] [ip-0A0C0456:50669:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.696045] [ip-0A0C0420:64111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.696374] [ip-0A0C0477:35007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.696098] [ip-0A0C0462:74469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.696092] [ip-0A0C0459:43240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.697636] [ip-0A0C045B:50924:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.697209] [ip-0A0C047C:40181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.697424] [ip-0A0C047C:40179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.698189] [ip-0A0C0455:41978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.697627] [ip-0A0C0459:43235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.698794] [ip-0A0C0455:41982:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.699479] [ip-0A0C041E:62765:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.699520] [ip-0A0C0471:32498:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.699203] [ip-0A0C0440:42301:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.700130] [ip-0A0C0463:37583:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.700130] [ip-0A0C0463:37581:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.700516] [ip-0A0C0475:29276:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.700646] [ip-0A0C0455:41980:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.700730] [ip-0A0C0459:43236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.702165] [ip-0A0C0423:56739:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.701432] [ip-0A0C047C:40180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.701987] [ip-0A0C0477:35011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.702581] [ip-0A0C045B:50923:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.702580] [ip-0A0C0451:45574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.702672] [ip-0A0C0462:74473:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.705646] [ip-0A0C0469:32620:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.705988] [ip-0A0C0479:30882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.706008] [ip-0A0C041E:62763:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.707589] [ip-0A0C0420:64108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.709737] [ip-0A0C0423:56741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.710570] [ip-0A0C0423:56738:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.710345] [ip-0A0C043D:39280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.711098] [ip-0A0C045B:50928:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.712052] [ip-0A0C0463:37578:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.712240] [ip-0A0C0463:37586:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.712218] [ip-0A0C041A:86413:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.712374] [ip-0A0C041A:86410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.714680] [ip-0A0C047A:35200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.714479] [ip-0A0C0481:28288:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.715317] [ip-0A0C0420:64107:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.716373] [ip-0A0C0413:94869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.717294] [ip-0A0C0469:32624:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.717938] [ip-0A0C0440:42297:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.720008] [ip-0A0C042C:56180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.720274] [ip-0A0C0456:50668:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.719932] [ip-0A0C0413:94873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.720569] [ip-0A0C047A:35193:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.721239] [ip-0A0C044A:42185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.721619] [ip-0A0C047A:35194:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.722978] [ip-0A0C0440:42298:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.723122] [ip-0A0C0440:42304:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.724397] [ip-0A0C0451:45571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.724766] [ip-0A0C044A:42184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.724542] [ip-0A0C047A:35197:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.725807] [ip-0A0C0433:56987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.726458] [ip-0A0C0479:30878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.726911] [ip-0A0C0469:32623:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.726868] [ip-0A0C0451:45573:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.726910] [ip-0A0C0434:51657:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.729096] [ip-0A0C044A:42188:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.730561] [ip-0A0C0469:32618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.730298] [ip-0A0C0434:51656:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.732983] [ip-0A0C0481:28289:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.734078] [ip-0A0C0452:46740:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.735582] [ip-0A0C0413:94868:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.737829] [ip-0A0C0413:94866:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.738949] [ip-0A0C0417:93554:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.739098] [ip-0A0C0417:93559:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.739808] [ip-0A0C0481:28292:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.740702] [ip-0A0C0456:50670:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.742483] [ip-0A0C0462:74466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.743158] [ip-0A0C0456:50671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.743005] [ip-0A0C0476:23259:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.743785] [ip-0A0C0481:28291:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.745141] [ip-0A0C0417:93556:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.747585] [ip-0A0C0462:74474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.748483] [ip-0A0C0476:23257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.751417] [ip-0A0C0417:93553:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.752358] [ip-0A0C042C:56183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.752421] [ip-0A0C042C:56182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.753017] [ip-0A0C042C:56186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.761351] [ip-0A0C0476:23254:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.762834] [ip-0A0C0479:30880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.762229] [ip-0A0C0462:74468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.762897] [ip-0A0C0462:74470:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.764164] [ip-0A0C0451:45568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.764883] [ip-0A0C0452:46735:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.767193] [ip-0A0C0451:45572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.768131] [ip-0A0C0451:45569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.768177] [ip-0A0C0434:51654:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.770550] [ip-0A0C041A:86408:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.771700] [ip-0A0C0476:23256:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.772351] [ip-0A0C0433:56993:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.778877] [ip-0A0C0434:51659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.779328] [ip-0A0C0433:56992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.779728] [ip-0A0C0434:51655:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.781717] [ip-0A0C0433:56989:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.782797] [ip-0A0C0433:56991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.788737] [ip-0A0C0452:46739:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.800709] [ip-0A0C041A:86409:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.809719] [ip-0A0C0479:30877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.810837] [ip-0A0C0479:30879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.811382] [ip-0A0C0452:46741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.811484] [ip-0A0C0452:46738:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.815527] [ip-0A0C041A:86412:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.817460] [ip-0A0C0479:30873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634622975.819018] [ip-0A0C041A:86411:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634622976712, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634622976751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634622976751, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634622976752, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634622976752, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634622976752, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634622976752, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:56:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0409:0:26971 - context.c:584] INFO job (ID: 867538652521379051) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:26971 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:26971 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:26970 - context.c:584] INFO job (ID: 867538684132695063) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:26970 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:26970 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:26963 - context.c:584] INFO job (ID: 867538230655357454) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:26963 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:26963 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:26968 - context.c:584] INFO job (ID: 867538267139995284) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:26968 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:26968 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:26966 - context.c:584] INFO job (ID: 867538304683547833) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:26966 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:26966 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:26965 - context.c:584] INFO job (ID: 867538510183156880) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:26965 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:26965 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:26967 - context.c:584] INFO job (ID: 867538125257994970) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:26967 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:26967 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:26969 - context.c:584] INFO job (ID: 867538538570635122) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:26969 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:26969 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069647, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3324093284, "metadata": {"file": "main.py", "lineno": 72}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069648, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069648, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069648, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069648, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069648, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069648, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069648, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069648, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069648, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069649, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623069649, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:49] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:57:51] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634623093632, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634623093643, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623093647, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634623093648, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634623096236, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634623096236, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1634623096236, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634623096237, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634623097892, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 2029.3724694694736, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623097893, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623097893, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 2029.3724694694736, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634623097893, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623097893, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623098576, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4923.026155558397, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623098576, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623098576, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4923.026155558397, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623098577, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623098577, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623099214, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5273.5166203659355, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623099214, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623099214, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5273.5166203659355, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634623099214, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623099215, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623099855, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5250.405972549193, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623099855, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623099855, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5250.405972549193, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634623099855, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623099855, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623100494, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5265.835752734577, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623100494, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623100494, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5265.835752734577, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634623100494, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623100494, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623101125, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5327.376793986163, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623101126, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623101126, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5327.376793986163, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634623101126, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623101126, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623101755, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5340.084234186955, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623101756, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623101756, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5340.084234186955, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634623101756, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623101756, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623102382, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5370.544311713374, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623102382, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623102382, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5370.544311713374, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634623102382, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623102382, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623103022, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5255.7240667389915, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623103022, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623103022, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5255.7240667389915, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634623103022, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623103023, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623103650, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5356.400621505174, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623103651, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623103651, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5356.400621505174, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634623103651, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623103651, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623104274, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5391.422928061498, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623104275, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623104275, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5391.422928061498, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634623104275, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623104275, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623104898, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5393.234468514536, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623104898, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623104899, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5393.234468514536, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634623104899, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623104899, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623105520, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.589322221019, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623105520, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623105520, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.589322221019, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634623105520, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623105520, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623106147, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5361.799037659276, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623106147, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623106147, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5361.799037659276, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634623106147, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623106148, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623106772, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5380.1272872828395, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623106773, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623106773, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5380.1272872828395, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634623106773, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623106773, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623107403, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5338.275863505766, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623107403, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623107403, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5338.275863505766, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634623107403, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623107403, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623108032, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5342.841625061748, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623108033, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623108033, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5342.841625061748, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634623108033, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623108033, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623108655, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5400.749451891163, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623108656, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623108656, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5400.749451891163, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634623108656, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623108656, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623109273, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5450.124639036394, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623109273, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623109273, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5450.124639036394, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634623109274, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623109274, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623109900, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5367.189949663772, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623109900, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623109901, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5367.189949663772, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634623109901, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623109901, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623110522, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5411.9490466112675, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623110522, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623110522, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5411.9490466112675, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634623110523, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623110523, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623111143, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5417.131048734286, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623111143, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623111143, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5417.131048734286, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634623111144, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623111144, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623111768, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5383.6685403173615, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623111768, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623111768, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5383.6685403173615, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634623111769, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623111769, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623112387, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5438.980751306885, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623112387, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623112387, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5438.980751306885, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634623112387, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623112387, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623113009, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5404.873385195265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623113009, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623113010, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5404.873385195265, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634623113010, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623113010, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623113627, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5448.561156669807, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623113627, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623113627, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5448.561156669807, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634623113627, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623113627, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623114250, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5394.962549182192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623114250, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623114251, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5394.962549182192, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634623114251, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623114251, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623114873, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5404.272320021352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623114873, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623114873, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5404.272320021352, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634623114873, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623114873, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623115491, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5436.930690701071, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623115492, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623115492, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5436.930690701071, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634623115492, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623115492, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623116120, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5354.446913212502, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623116120, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623116120, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5354.446913212502, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634623116120, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623116120, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623116734, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5477.921129287673, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623116734, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623116734, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5477.921129287673, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634623116734, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623116734, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623117353, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5427.970147218721, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623117354, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623117354, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5427.970147218721, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634623117354, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623117354, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623117966, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5491.751552010824, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623117966, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623117966, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5491.751552010824, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634623117966, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623117967, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623118585, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5432.816352964986, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623118585, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623118585, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5432.816352964986, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634623118586, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623118586, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623119204, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5432.2279021827035, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623119205, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623119205, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5432.2279021827035, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634623119205, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623119205, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623119825, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5415.742522777816, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623119826, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623119826, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5415.742522777816, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634623119826, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623119826, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623120446, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5420.877580740828, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623120446, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623120446, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5420.877580740828, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634623120446, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623120447, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623121066, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5428.764700240104, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623121066, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623121066, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5428.764700240104, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634623121066, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623121066, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623121684, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5435.030991512429, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623121685, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623121685, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5435.030991512429, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634623121685, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623121685, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623122304, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5429.061671865411, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623122304, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623122305, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5429.061671865411, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634623122305, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623122305, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623122924, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5428.039138653811, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623122924, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623122924, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5428.039138653811, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634623122924, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623122924, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623123547, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5395.726808828665, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623123547, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623123548, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5395.726808828665, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634623123548, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623123548, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623124167, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5424.886140940644, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623124167, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623124168, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5424.886140940644, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634623124168, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623124168, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623124788, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5415.640545189321, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623124789, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623124789, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5415.640545189321, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634623124789, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623124789, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623125412, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5389.925921733055, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623125413, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623125413, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5389.925921733055, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634623125413, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623125413, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623126042, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5344.776736851447, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623126042, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623126042, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5344.776736851447, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634623126042, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623126043, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623126662, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5428.099768938547, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623126662, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623126662, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5428.099768938547, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634623126662, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623126662, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623127280, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5437.910414242966, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623127281, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623127281, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5437.910414242966, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634623127281, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623127281, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623127903, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5403.766699770319, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623127903, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623127903, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5403.766699770319, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634623127904, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623127904, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623128525, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5410.743901095251, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623128525, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623128525, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5410.743901095251, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634623128596, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623128597, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623128614, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623129041, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8883819580078125, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623129042, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623129206, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5516.341118576079, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623129206, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623129206, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5516.341118576079, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634623129311, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623129311, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623129319, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623129727, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.890613317489624, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623129727, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623129933, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5405.984671448848, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623129933, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623129934, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5405.984671448848, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634623130034, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623130035, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623130048, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623130460, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.893709659576416, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623130460, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623130671, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5277.588785172725, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623130672, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623130672, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5277.588785172725, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634623130735, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623130735, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623130750, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623131178, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8757086992263794, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623131178, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623131384, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5179.717507271817, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623131385, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623131385, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5179.717507271817, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634623131421, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623131421, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623131437, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623131878, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8965739011764526, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623131878, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623132077, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5124.36383522752, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623132077, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623132077, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5124.36383522752, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634623132128, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623132128, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623132142, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623132545, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8819319009780884, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623132545, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623132734, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5548.899320328945, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623132734, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623132734, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5548.899320328945, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634623132783, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623132783, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623132798, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623133227, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8722811937332153, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623133227, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623133421, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5264.415533185606, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623133422, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623133422, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5264.415533185606, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634623133502, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623133502, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623133517, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623133923, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.863166332244873, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623133923, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623134123, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5415.794553639297, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623134123, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623134123, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5415.794553639297, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634623134187, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623134187, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623134202, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623134604, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8836566209793091, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623134604, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623134802, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5465.016238544313, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623134802, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623134803, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5465.016238544313, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634623134838, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623134838, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623134853, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623135304, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8806352615356445, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623135305, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623135505, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5035.58547787615, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623135505, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623135506, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5035.58547787615, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634623135555, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623135555, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623135570, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623135990, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.885066568851471, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623135990, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623136178, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5396.13381379422, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623136178, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623136178, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5396.13381379422, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634623136226, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623136226, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623136241, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623136692, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8932817578315735, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623136692, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623136906, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4942.612606609341, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623136906, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623136906, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4942.612606609341, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634623136961, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623136961, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623136976, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623137425, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8917750120162964, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623137425, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623137629, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5031.962520383523, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623137629, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623137630, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5031.962520383523, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634623137673, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623137673, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623137688, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623138100, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8890097141265869, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623138100, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623138295, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5401.037156496332, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623138296, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623138296, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5401.037156496332, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634623138331, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623138331, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623138346, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623138769, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8872948884963989, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623138769, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623138960, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5340.286588977295, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623138961, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623138961, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5340.286588977295, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634623139008, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623139009, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623139024, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623139430, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8924799561500549, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623139430, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623139626, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5442.780700087592, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623139626, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623139627, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5442.780700087592, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634623139665, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623139665, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623139680, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623140102, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8890774846076965, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623140102, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623140296, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5327.765495867378, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623140297, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623140297, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5327.765495867378, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634623140343, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623140343, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623140358, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623140793, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8968420028686523, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623140793, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623140996, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5145.682498811512, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623140997, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623140997, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5145.682498811512, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634623141033, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623141033, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623141048, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623141508, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8950365781784058, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623141508, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623141713, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4943.156973171368, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623141713, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623141713, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4943.156973171368, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634623141774, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623141775, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623141790, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623142209, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8725986480712891, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623142209, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623142409, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5295.442387778391, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623142409, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623142410, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5295.442387778391, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634623142445, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623142445, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623142458, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623142886, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8906648755073547, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623142886, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623143085, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5251.957602314119, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623143085, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623143085, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5251.957602314119, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634623143122, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623143122, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623143137, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623143568, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8991318941116333, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623143568, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623143762, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5252.642724349274, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623143762, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623143762, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5252.642724349274, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634623143800, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623143800, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623143816, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623144235, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9001386165618896, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623144235, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623144425, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5378.663232756087, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623144425, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623144426, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5378.663232756087, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634623144462, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623144462, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623144477, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623144921, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8840823769569397, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623144921, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623145131, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5018.012195317354, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623145132, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623145132, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5018.012195317354, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634623145206, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623145206, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623145221, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623145653, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8956812024116516, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623145654, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623145861, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5135.053429785206, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623145861, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623145861, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5135.053429785206, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634623145898, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623145898, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623145913, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623146313, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.891185998916626, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623146313, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623146507, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5525.026321855844, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623146507, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623146507, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5525.026321855844, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634623146559, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623146559, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623146574, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623147012, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9016954898834229, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623147012, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623147208, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5175.108792924209, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623147208, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623147209, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5175.108792924209, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634623147266, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623147267, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623147281, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623147681, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8890959024429321, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623147681, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623147901, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5293.598502918213, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623147902, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623147902, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5293.598502918213, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634623147969, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623147969, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623147984, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623148384, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8879481554031372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623148384, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623148582, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5480.445486472581, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623148582, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623148583, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5480.445486472581, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634623148617, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623148617, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623148631, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623149058, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8952391147613525, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623149058, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623149272, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5131.489639517775, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623149273, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623149273, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5131.489639517775, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634623149307, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623149307, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623149322, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623149724, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8659291863441467, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623149724, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623149923, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5460.2097550146045, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623149923, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623149923, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5460.2097550146045, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634623149960, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623149960, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623149975, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623150392, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8900386095046997, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623150392, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623150585, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5375.235167420407, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623150585, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623150585, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5375.235167420407, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634623150667, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623150667, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623150682, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623151083, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8861021399497986, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623151083, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623151290, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5393.498667207566, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623151291, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623151291, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5393.498667207566, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634623151327, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623151327, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623151342, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623151766, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8915927410125732, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623151766, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623151957, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5332.636122049814, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623151958, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623151958, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5332.636122049814, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634623151997, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623151997, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623152011, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623152410, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8975232839584351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623152410, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623152608, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5501.405106953082, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623152608, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623152608, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5501.405106953082, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634623152663, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623152663, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623152678, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623153077, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.897222638130188, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623153077, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623153266, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5574.5182086368895, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623153266, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623153266, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5574.5182086368895, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634623153302, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623153302, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623153317, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623153751, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8885998725891113, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623153751, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623153944, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5235.926035733791, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623153944, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623153944, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5235.926035733791, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634623153988, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623153988, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623154003, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623154404, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8922149538993835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623154404, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623154592, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5559.755010160569, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623154593, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623154593, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5559.755010160569, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634623154676, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623154677, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623154692, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623155092, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8949495553970337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623155092, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623155291, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5469.8418065676005, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623155291, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623155291, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5469.8418065676005, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634623155329, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623155329, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623155344, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623155744, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8954820036888123, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623155745, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623155928, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5609.983280946967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623155928, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623155928, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5609.983280946967, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634623155974, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623155974, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623155989, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623156412, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8954241871833801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623156412, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623156612, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5271.743186187171, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623156612, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623156612, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5271.743186187171, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634623156668, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623156668, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623156683, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623157118, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8966706991195679, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623157118, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623157322, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5138.614678868976, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623157322, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623157322, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5138.614678868976, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634623157359, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623157359, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623157374, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623157815, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9006530046463013, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623157815, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623158022, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5068.052914257973, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623158022, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623158023, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5068.052914257973, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634623158059, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623158059, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623158074, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623158529, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9006978273391724, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623158530, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623158738, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4952.398259246348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623158738, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623158739, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4952.398259246348, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634623158778, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623158778, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623158793, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623159243, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8987909555435181, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623159243, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623159449, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5013.558206300361, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623159449, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623159449, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5013.558206300361, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634623159486, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623159486, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623159502, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634623159937, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8905543088912964, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634623159937, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634623160136, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5170.6581399316465, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623160137, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623160137, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5170.6581399316465, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634623160174, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623160174, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623160189, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634623160631, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9044324159622192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634623160632, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634623160832, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5112.562901865978, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623160832, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623160832, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5112.562901865978, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634623160878, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623160878, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623160892, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634623161320, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8944461345672607, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634623161320, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634623161510, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5314.489182930264, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623161510, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623161511, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5314.489182930264, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634623161546, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623161546, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623161560, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634623162011, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8941571712493896, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634623162011, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634623162208, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5076.481693075341, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623162208, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623162208, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5076.481693075341, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634623162245, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623162245, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623162259, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1980, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1980}}
:::MLLOG {"namespace": "", "time_ms": 1634623162692, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9028749465942383, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1980}}
:::MLLOG {"namespace": "", "time_ms": 1634623162692, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1980, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1980}}
:::MLLOG {"namespace": "", "time_ms": 1634623162889, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5214.228898500427, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623162890, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623162890, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5214.228898500427, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1980}}
:::MLLOG {"namespace": "", "time_ms": 1634623162927, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 2000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623162927, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 2000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634623162943, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 2000}}
:::MLLOG {"namespace": "", "time_ms": 1634623163343, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9099910259246826, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 2000}}
:::MLLOG {"namespace": "", "time_ms": 1634623163343, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 2000}}
:::MLLOG {"namespace": "", "time_ms": 1634623163343, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634623163529, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5582.014369283212, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634623163530, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634623163530, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5582.014369283212, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 2000}}
ENDING TIMING RUN AT 2021-10-19 05:59:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:31 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:40 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:41 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:42 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:43 AM
RESULT,image_segmentation,,213,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:43 AM
RESULT,image_segmentation,,213,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:43 AM
RESULT,image_segmentation,,213,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:43 AM
RESULT,image_segmentation,,213,nvidia,2021-10-19 05:56:10 AM
ENDING TIMING RUN AT 2021-10-19 05:59:43 AM
RESULT,image_segmentation,,213,nvidia,2021-10-19 05:56:10 AM
