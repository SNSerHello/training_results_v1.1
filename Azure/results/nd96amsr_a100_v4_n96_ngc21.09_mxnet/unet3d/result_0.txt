+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019051309001798935
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051307
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019051309001798935
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051307/unet3d_96x8x1_211019051309001798935
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051307/unet3d_96x8x1_211019051309001798935
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051307:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07354/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051307
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051307
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051307/unet3d_96x8x1_211019051309001798935_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0412
Clearing cache on ip-0A0C0417
Clearing cache on ip-0A0C040C
Clearing cache on ip-0A0C042B
Clearing cache on ip-0A0C0418
Clearing cache on ip-0A0C0410
Clearing cache on ip-0A0C044B
Clearing cache on ip-0A0C0428
Clearing cache on ip-0A0C0414
Clearing cache on ip-0A0C0416
Clearing cache on ip-0A0C041A
Clearing cache on ip-0A0C0427
Clearing cache on ip-0A0C040E
Clearing cache on ip-0A0C043D
Clearing cache on ip-0A0C040A
Clearing cache on ip-0A0C044E
Clearing cache on ip-0A0C041C
Clearing cache on ip-0A0C0409
Clearing cache on ip-0A0C043F
Clearing cache on ip-0A0C041B
Clearing cache on ip-0A0C0421
Clearing cache on ip-0A0C040B
Clearing cache on ip-0A0C0423
Clearing cache on ip-0A0C041D
Clearing cache on ip-0A0C0440
Clearing cache on ip-0A0C0411
Clearing cache on ip-0A0C0413
Clearing cache on ip-0A0C044F
Clearing cache on ip-0A0C0419
Clearing cache on ip-0A0C0408
Clearing cache on ip-0A0C0442
Clearing cache on ip-0A0C0434
Clearing cache on ip-0A0C045E
Clearing cache on ip-0A0C0448
Clearing cache on ip-0A0C0456
Clearing cache on ip-0A0C0431
Clearing cache on ip-0A0C0465
Clearing cache on ip-0A0C0458
Clearing cache on ip-0A0C0469
Clearing cache on ip-0A0C0471
Clearing cache on ip-0A0C0430
Clearing cache on ip-0A0C041E
Clearing cache on ip-0A0C0477
Clearing cache on ip-0A0C0454
Clearing cache on ip-0A0C0450
Clearing cache on ip-0A0C0439
Clearing cache on ip-0A0C0518
Clearing cache on ip-0A0C044C
Clearing cache on ip-0A0C042F
Clearing cache on ip-0A0C046A
Clearing cache on ip-0A0C043B
Clearing cache on ip-0A0C0445
Clearing cache on ip-0A0C0433
Clearing cache on ip-0A0C0420
Clearing cache on ip-0A0C0480
Clearing cache on ip-0A0C041F
Clearing cache on ip-0A0C047B
Clearing cache on ip-0A0C0470
Clearing cache on ip-0A0C0474
Clearing cache on ip-0A0C0422
Clearing cache on ip-0A0C0407
Clearing cache on ip-0A0C0425
Clearing cache on ip-0A0C0481
Clearing cache on ip-0A0C043C
Clearing cache on ip-0A0C045A
Clearing cache on ip-0A0C042C
Clearing cache on ip-0A0C0446
Clearing cache on ip-0A0C042A
Clearing cache on ip-0A0C043E
Clearing cache on ip-0A0C043A
Clearing cache on ip-0A0C047A
Clearing cache on ip-0A0C042E
Clearing cache on ip-0A0C046D
Clearing cache on ip-0A0C0452
Clearing cache on ip-0A0C045C
Clearing cache on ip-0A0C0462
Clearing cache on ip-0A0C0457
Clearing cache on ip-0A0C0476
Clearing cache on ip-0A0C0455
Clearing cache on ip-0A0C047D
Clearing cache on ip-0A0C0432
Clearing cache on ip-0A0C0451
Clearing cache on ip-0A0C045D
Clearing cache on ip-0A0C0459
Clearing cache on ip-0A0C0447
Clearing cache on ip-0A0C0424
Clearing cache on ip-0A0C0479
Clearing cache on ip-0A0C045B
Clearing cache on ip-0A0C0426
Clearing cache on ip-0A0C044A
Clearing cache on ip-0A0C0453
Clearing cache on ip-0A0C047F
Clearing cache on ip-0A0C0463
Clearing cache on ip-0A0C047C
Clearing cache on ip-0A0C0467
Clearing cache on ip-0A0C0475
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051307:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
STARTING TIMING RUN AT 2021-10-19 05:13:13 AM
running benchmark
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634620397.911756] [ip-0A0C040E:25869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620397.948746] [ip-0A0C0409:93514:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620397.959839] [ip-0A0C0409:93515:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620397.974262] [ip-0A0C040E:25862:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620397.991857] [ip-0A0C040B:81674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.002990] [ip-0A0C0409:93522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.011015] [ip-0A0C040C:50196:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.011021] [ip-0A0C040C:50199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.026878] [ip-0A0C040B:81675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.028813] [ip-0A0C0410:54623:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.034482] [ip-0A0C0410:54625:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.041312] [ip-0A0C0433:31850:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.041312] [ip-0A0C0433:31847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.044527] [ip-0A0C040E:25870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.045509] [ip-0A0C040A:39602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.057240] [ip-0A0C0446:25794:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.058370] [ip-0A0C040B:81676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.059977] [ip-0A0C0475:4112 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.060240] [ip-0A0C043B:22735:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.076460] [ip-0A0C040E:25866:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.078941] [ip-0A0C043B:22737:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.079886] [ip-0A0C041B:55967:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.082374] [ip-0A0C0409:93521:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.092887] [ip-0A0C040E:25864:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.095740] [ip-0A0C0445:28474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.106725] [ip-0A0C0428:26906:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.107063] [ip-0A0C041F:41619:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.107146] [ip-0A0C045A:21909:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.107432] [ip-0A0C043C:23999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.108321] [ip-0A0C043C:24001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.111215] [ip-0A0C0446:25796:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.112862] [ip-0A0C040E:25863:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.115569] [ip-0A0C0409:93513:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.118440] [ip-0A0C0475:4114 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.118518] [ip-0A0C0413:69745:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.118532] [ip-0A0C0413:69747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.124366] [ip-0A0C0433:31852:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.124496] [ip-0A0C0433:31848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.124729] [ip-0A0C040E:25865:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.127632] [ip-0A0C040E:25867:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.128172] [ip-0A0C0455:17118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.129629] [ip-0A0C042E:36966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.129608] [ip-0A0C042E:36964:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.130594] [ip-0A0C0428:26902:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.131526] [ip-0A0C041B:55966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.132936] [ip-0A0C042A:28609:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.133738] [ip-0A0C040B:81678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.133969] [ip-0A0C040A:39603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.136707] [ip-0A0C0409:93519:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.137176] [ip-0A0C041F:41616:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.136686] [ip-0A0C0417:68451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.138237] [ip-0A0C0409:93518:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.138426] [ip-0A0C0409:93516:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.138706] [ip-0A0C043B:22734:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.138644] [ip-0A0C044E:15062:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.140104] [ip-0A0C043F:19149:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.142720] [ip-0A0C041A:61147:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.143242] [ip-0A0C047B:5469 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.145639] [ip-0A0C046A:13028:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.151442] [ip-0A0C045A:21913:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.152131] [ip-0A0C045D:9554 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.152554] [ip-0A0C0481:94485:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.153165] [ip-0A0C0475:4116 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.155508] [ip-0A0C046A:13037:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.157128] [ip-0A0C0446:25798:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.157147] [ip-0A0C040B:81671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.157098] [ip-0A0C043E:11684:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.157766] [ip-0A0C040A:39604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.161796] [ip-0A0C041A:61144:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.161713] [ip-0A0C041B:55968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.161364] [ip-0A0C0410:54627:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.170065] [ip-0A0C045E:19323:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.170506] [ip-0A0C0465:10012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.171269] [ip-0A0C0463:12472:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.171472] [ip-0A0C040C:50197:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.171627] [ip-0A0C040C:50198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.172067] [ip-0A0C0462:49295:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.172294] [ip-0A0C0408:62005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.177858] [ip-0A0C040C:50200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.179095] [ip-0A0C0445:28470:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.179349] [ip-0A0C0445:28471:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.181157] [ip-0A0C043C:23995:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.182667] [ip-0A0C042E:36962:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.182370] [ip-0A0C0432:52748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.182940] [ip-0A0C040B:81672:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.183144] [ip-0A0C040B:81673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.185490] [ip-0A0C0439:15399:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.185285] [ip-0A0C0410:54621:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.187665] [ip-0A0C040B:81677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.187579] [ip-0A0C0417:68453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.188856] [ip-0A0C043D:14447:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.189251] [ip-0A0C0481:94484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.191286] [ip-0A0C0408:62000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.191614] [ip-0A0C0463:12473:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.193452] [ip-0A0C040A:39600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.194134] [ip-0A0C0418:34427:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.194136] [ip-0A0C0418:34433:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.195636] [ip-0A0C0459:18445:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.197334] [ip-0A0C041F:41615:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.201249] [ip-0A0C043D:14453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.202205] [ip-0A0C044F:16051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.202400] [ip-0A0C040C:50202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.202492] [ip-0A0C040C:50195:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.203923] [ip-0A0C0477:3535 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.206399] [ip-0A0C0428:26900:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.206637] [ip-0A0C040C:50201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.207430] [ip-0A0C0419:41579:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.207307] [ip-0A0C0447:25175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.208899] [ip-0A0C0417:68452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.209182] [ip-0A0C0410:54622:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.209623] [ip-0A0C044C:20279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.210718] [ip-0A0C0433:31849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.213477] [ip-0A0C043E:11688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.213459] [ip-0A0C0475:4119 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.214104] [ip-0A0C043F:19147:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.214378] [ip-0A0C043E:11682:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.214110] [ip-0A0C0413:69743:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.216200] [ip-0A0C0432:52747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.216303] [ip-0A0C0413:69748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.217634] [ip-0A0C0445:28472:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.217835] [ip-0A0C0421:39224:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.218873] [ip-0A0C0433:31851:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.218985] [ip-0A0C0433:31846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.219361] [ip-0A0C0455:17120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.220389] [ip-0A0C044C:20282:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.222187] [ip-0A0C0412:17326:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.221930] [ip-0A0C0451:20449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.222239] [ip-0A0C0442:17932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.223153] [ip-0A0C042C:31065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.223142] [ip-0A0C042C:31070:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.225693] [ip-0A0C0410:54620:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.226692] [ip-0A0C040A:39601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.229066] [ip-0A0C047B:5465 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.229265] [ip-0A0C041C:43654:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.229716] [ip-0A0C043B:22738:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.229830] [ip-0A0C040A:39599:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.230405] [ip-0A0C043B:22747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.230381] [ip-0A0C040A:39598:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.232392] [ip-0A0C0455:17117:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.232824] [ip-0A0C0410:54624:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.233132] [ip-0A0C0413:69750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.233703] [ip-0A0C044C:20285:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.234130] [ip-0A0C0446:25791:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.234871] [ip-0A0C042E:36959:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.236275] [ip-0A0C0446:25817:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.236399] [ip-0A0C0422:57104:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.236372] [ip-0A0C0410:54626:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.237134] [ip-0A0C0439:15398:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.237176] [ip-0A0C041B:55965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.238097] [ip-0A0C0420:39067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.238755] [ip-0A0C0421:39220:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.238925] [ip-0A0C0471:7374 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.239294] [ip-0A0C045A:21907:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.241766] [ip-0A0C042A:28612:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.242176] [ip-0A0C041F:41629:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.242038] [ip-0A0C0424:26918:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.241774] [ip-0A0C040A:39605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.242139] [ip-0A0C042A:28605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.242539] [ip-0A0C0462:49291:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.244120] [ip-0A0C0433:31867:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.244412] [ip-0A0C041E:37691:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.244401] [ip-0A0C041E:37688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.245109] [ip-0A0C0455:17119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.245337] [ip-0A0C0417:68448:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.246404] [ip-0A0C0424:26919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.246315] [ip-0A0C0452:21652:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.247968] [ip-0A0C041A:61143:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.248008] [ip-0A0C0469:7772 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.247370] [ip-0A0C0463:12474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.248667] [ip-0A0C044E:15057:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.248761] [ip-0A0C044E:15055:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.249591] [ip-0A0C0427:34710:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.250706] [ip-0A0C0446:25795:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.250776] [ip-0A0C0446:25792:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.251656] [ip-0A0C043B:22741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.252074] [ip-0A0C0474:4167 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.252424] [ip-0A0C0407:42776:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.253246] [ip-0A0C0428:26905:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.253446] [ip-0A0C043D:14451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.253148] [ip-0A0C0442:17933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.253266] [ip-0A0C041C:43671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.253161] [ip-0A0C047D:97158:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.253906] [ip-0A0C0465:10010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.254883] [ip-0A0C0422:57109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.256077] [ip-0A0C0446:25793:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.256966] [ip-0A0C0414:54673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.259640] [ip-0A0C047B:5467 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.260106] [ip-0A0C0476:95982:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.260328] [ip-0A0C0470:2972 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.260705] [ip-0A0C041B:55970:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.262936] [ip-0A0C045A:21906:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.263364] [ip-0A0C0475:4118 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.263403] [ip-0A0C0475:4113 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.263678] [ip-0A0C0459:18443:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.264008] [ip-0A0C0447:25177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.265098] [ip-0A0C043E:11689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.265156] [ip-0A0C0447:25173:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.265447] [ip-0A0C0479:96961:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.266074] [ip-0A0C0462:49293:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.266806] [ip-0A0C0475:4117 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.268182] [ip-0A0C0448:29606:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.268469] [ip-0A0C0434:26559:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.268359] [ip-0A0C047D:97157:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.269312] [ip-0A0C0450:91323:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.271713] [ip-0A0C042C:31063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.271539] [ip-0A0C043F:19146:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.272928] [ip-0A0C044F:16050:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.273509] [ip-0A0C044F:16054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.273757] [ip-0A0C045D:9553 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.275645] [ip-0A0C0445:28477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.275919] [ip-0A0C042A:28611:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.276097] [ip-0A0C043B:22739:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.278018] [ip-0A0C0432:52750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.278483] [ip-0A0C0465:10008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.278695] [ip-0A0C043C:23994:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.279062] [ip-0A0C045E:19322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.279063] [ip-0A0C043B:22736:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.279351] [ip-0A0C044A:17374:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.281121] [ip-0A0C041F:41618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.281863] [ip-0A0C0423:31963:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.283362] [ip-0A0C044A:17375:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.283725] [ip-0A0C045D:9549 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.284594] [ip-0A0C047A:3789 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.285186] [ip-0A0C0452:21659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.285112] [ip-0A0C046D:6982 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.285935] [ip-0A0C041B:55969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.286510] [ip-0A0C041A:61148:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.286061] [ip-0A0C044E:15056:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.286992] [ip-0A0C0428:26904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.286830] [ip-0A0C0411:37463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.287199] [ip-0A0C045E:19327:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.287252] [ip-0A0C0479:96967:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.287066] [ip-0A0C0428:26901:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.286869] [ip-0A0C047D:97160:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.289647] [ip-0A0C041D:34774:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.290253] [ip-0A0C0442:17930:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.290815] [ip-0A0C043C:24000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.291169] [ip-0A0C041B:55964:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.291261] [ip-0A0C041B:55971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.291841] [ip-0A0C0427:34712:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.292308] [ip-0A0C047B:5468 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.292615] [ip-0A0C0412:17327:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.292806] [ip-0A0C045B:25820:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.293297] [ip-0A0C042E:36963:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.293572] [ip-0A0C043E:11686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.293559] [ip-0A0C0475:4115 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.295512] [ip-0A0C047F:1830 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.296111] [ip-0A0C0470:2974 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.296640] [ip-0A0C046A:13025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.296962] [ip-0A0C045A:21938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.297071] [ip-0A0C0439:15417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.297859] [ip-0A0C0427:34711:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.298636] [ip-0A0C0445:28476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.299343] [ip-0A0C043F:19145:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.300401] [ip-0A0C0423:31965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.300473] [ip-0A0C043A:23233:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.300888] [ip-0A0C0421:39219:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.301181] [ip-0A0C042E:36961:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.301666] [ip-0A0C0445:28473:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.301461] [ip-0A0C046A:13026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.302106] [ip-0A0C043D:14449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.302110] [ip-0A0C045A:21910:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.302005] [ip-0A0C0451:20450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.302443] [ip-0A0C043C:23996:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.302828] [ip-0A0C0455:17122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.303101] [ip-0A0C042A:28606:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.303331] [ip-0A0C044E:15060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.304236] [ip-0A0C0471:7380 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.304636] [ip-0A0C043C:23997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.305142] [ip-0A0C0412:17328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.305268] [ip-0A0C0428:26903:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.305773] [ip-0A0C0428:26899:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.305774] [ip-0A0C0420:39063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.305898] [ip-0A0C0420:39061:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.306071] [ip-0A0C0465:10007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.306415] [ip-0A0C0452:21657:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.306915] [ip-0A0C0445:28475:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.307774] [ip-0A0C045A:21908:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.309670] [ip-0A0C042E:36960:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.310386] [ip-0A0C0416:24987:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.310394] [ip-0A0C0416:24988:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.310758] [ip-0A0C0451:20452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.311004] [ip-0A0C045D:9552 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.311260] [ip-0A0C0459:18444:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.311897] [ip-0A0C042F:32977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.312274] [ip-0A0C0413:69746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.312670] [ip-0A0C0413:69744:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.313184] [ip-0A0C045E:19319:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.313610] [ip-0A0C0454:22539:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.314552] [ip-0A0C0413:69749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.315377] [ip-0A0C043F:19144:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.315342] [ip-0A0C0422:57112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.315575] [ip-0A0C0481:94481:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.316035] [ip-0A0C041A:61151:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.316391] [ip-0A0C042B:35691:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.316418] [ip-0A0C042B:35692:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.316728] [ip-0A0C045A:21911:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.317500] [ip-0A0C0455:17121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.318962] [ip-0A0C0474:4166 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.319526] [ip-0A0C044E:15061:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.319750] [ip-0A0C044E:15058:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.320522] [ip-0A0C041F:41623:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.320772] [ip-0A0C041F:41621:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.320956] [ip-0A0C045D:9548 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.321206] [ip-0A0C047B:5466 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.321386] [ip-0A0C0426:36368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.322164] [ip-0A0C0408:61998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.322550] [ip-0A0C0408:62003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.322861] [ip-0A0C041A:61146:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.323011] [ip-0A0C041F:41617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.323000] [ip-0A0C0477:3530 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.323208] [ip-0A0C0477:3531 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.323281] [ip-0A0C0481:94483:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.323888] [ip-0A0C0456:25540:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.323288] [ip-0A0C043A:23231:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.323652] [ip-0A0C043F:19151:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.323916] [ip-0A0C0439:15397:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.324610] [ip-0A0C0455:17123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.324855] [ip-0A0C042E:36965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.324684] [ip-0A0C0467:8111 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.324855] [ip-0A0C0447:25178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.325213] [ip-0A0C0471:7378 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.325557] [ip-0A0C045B:25822:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.325960] [ip-0A0C0419:41574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.326205] [ip-0A0C045C:18948:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.326745] [ip-0A0C0476:95983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.328087] [ip-0A0C0462:49294:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.328216] [ip-0A0C043C:23998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.328251] [ip-0A0C0414:54671:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.328756] [ip-0A0C0422:57110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.329130] [ip-0A0C0455:17116:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.329157] [ip-0A0C0476:95986:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.329454] [ip-0A0C042A:28608:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.330239] [ip-0A0C042A:28610:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.330352] [ip-0A0C042A:28607:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.330063] [ip-0A0C046A:13027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.331144] [ip-0A0C0463:12501:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.331146] [ip-0A0C046A:13031:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.331121] [ip-0A0C044E:15059:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.331917] [ip-0A0C0463:12471:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.333659] [ip-0A0C0417:68447:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.336392] [ip-0A0C047A:3783 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.336654] [ip-0A0C0411:37464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.337301] [ip-0A0C041A:61145:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.336649] [ip-0A0C046D:6957 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.337019] [ip-0A0C0434:26566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.337974] [ip-0A0C0419:41575:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.337770] [ip-0A0C043F:19150:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.338455] [ip-0A0C0412:17325:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.338165] [ip-0A0C0417:68450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.338225] [ip-0A0C0417:68446:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.339030] [ip-0A0C041D:34773:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.338985] [ip-0A0C047B:5477 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.339227] [ip-0A0C043F:19148:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.339430] [ip-0A0C0450:91322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.339389] [ip-0A0C0459:18447:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.339585] [ip-0A0C042C:31068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.340666] [ip-0A0C041A:61150:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.340689] [ip-0A0C0481:94486:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.340880] [ip-0A0C044B:21485:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.341435] [ip-0A0C047B:5470 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.342200] [ip-0A0C0456:25537:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.344851] [ip-0A0C0453:23565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.345219] [ip-0A0C0439:15420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.344977] [ip-0A0C0417:68449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.346509] [ip-0A0C0448:29605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.346846] [ip-0A0C041D:34768:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.348242] [ip-0A0C045E:19320:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.348175] [ip-0A0C0467:8107 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.348638] [ip-0A0C043E:11687:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.349658] [ip-0A0C0426:36373:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.349984] [ip-0A0C0408:62002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.350711] [ip-0A0C0463:12475:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.351678] [ip-0A0C045D:9555 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.351792] [ip-0A0C045D:9550 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.352086] [ip-0A0C042C:31064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.352354] [ip-0A0C0418:34432:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.352468] [ip-0A0C0418:34441:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.352675] [ip-0A0C0418:34428:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.352537] [ip-0A0C046D:6959 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.354606] [ip-0A0C045C:18947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.354942] [ip-0A0C047B:5464 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.355246] [ip-0A0C044C:20286:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.355500] [ip-0A0C046A:13032:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.356026] [ip-0A0C0462:49290:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.356030] [ip-0A0C046A:13030:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.356895] [ip-0A0C045D:9551 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.357150] [ip-0A0C047A:3780 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.358451] [ip-0A0C045E:19325:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.358449] [ip-0A0C0469:7775 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.358768] [ip-0A0C0432:52752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.359227] [ip-0A0C0465:10009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.359931] [ip-0A0C044C:20280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.360216] [ip-0A0C044F:16048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.360270] [ip-0A0C0481:94487:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.360410] [ip-0A0C041C:43656:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.360673] [ip-0A0C046D:6952 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.361048] [ip-0A0C047F:1825 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.361785] [ip-0A0C043E:11685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.361822] [ip-0A0C043E:11683:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.361920] [ip-0A0C0459:18448:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.362186] [ip-0A0C045E:19326:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.362065] [ip-0A0C0480:95570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.362654] [ip-0A0C0451:20454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.363955] [ip-0A0C045E:19321:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.364467] [ip-0A0C0434:26560:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.364503] [ip-0A0C0477:3533 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.365493] [ip-0A0C0425:41298:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.365617] [ip-0A0C0465:10006:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.365894] [ip-0A0C0457:18474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.366011] [ip-0A0C0481:94482:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.366011] [ip-0A0C0418:34430:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.366056] [ip-0A0C0462:49292:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.365891] [ip-0A0C0457:18479:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.366077] [ip-0A0C0481:94480:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.365889] [ip-0A0C0457:18478:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.366803] [ip-0A0C042C:31067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.366847] [ip-0A0C042C:31069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.367474] [ip-0A0C047C:8839 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.367865] [ip-0A0C0477:3537 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.368834] [ip-0A0C0465:10005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.369045] [ip-0A0C041E:37695:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.370065] [ip-0A0C0424:26916:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.370343] [ip-0A0C0419:41578:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.370316] [ip-0A0C0424:26920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.370518] [ip-0A0C0431:28523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.370593] [ip-0A0C0432:52746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.370946] [ip-0A0C0408:62001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.371489] [ip-0A0C047F:1827 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.372742] [ip-0A0C0408:61999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.373059] [ip-0A0C0412:17330:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.373001] [ip-0A0C0463:12477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.373214] [ip-0A0C0518:36440:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.374133] [ip-0A0C0459:18442:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.374429] [ip-0A0C044C:20283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.374471] [ip-0A0C0422:57105:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.374446] [ip-0A0C0469:7773 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.376023] [ip-0A0C0465:10011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.375740] [ip-0A0C0458:25912:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.376194] [ip-0A0C0418:34431:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.376452] [ip-0A0C0463:12476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.377493] [ip-0A0C0425:41302:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.377397] [ip-0A0C0430:60601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.377662] [ip-0A0C0418:34429:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.378250] [ip-0A0C0408:62004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.378861] [ip-0A0C043D:14450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.378965] [ip-0A0C043D:14452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.379102] [ip-0A0C0407:42781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.379641] [ip-0A0C0442:17931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.381506] [ip-0A0C043D:14448:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.381612] [ip-0A0C0480:95594:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.381671] [ip-0A0C042B:35690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.381648] [ip-0A0C041C:43655:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.383409] [ip-0A0C0419:41576:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.384108] [ip-0A0C0420:39062:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.384090] [ip-0A0C0474:4168 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.384483] [ip-0A0C043D:14454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.384679] [ip-0A0C043A:23232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.385801] [ip-0A0C041E:37690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.387462] [ip-0A0C0432:52771:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.388178] [ip-0A0C0419:41577:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.388066] [ip-0A0C0440:17489:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.388575] [ip-0A0C0411:37461:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.389675] [ip-0A0C042C:31066:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.389814] [ip-0A0C044C:20281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.389677] [ip-0A0C0474:4162 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.389674] [ip-0A0C0459:18446:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.390101] [ip-0A0C0459:18441:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.390504] [ip-0A0C0421:39221:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.390851] [ip-0A0C044C:20284:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.391707] [ip-0A0C0462:49288:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.392751] [ip-0A0C0462:49289:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.392967] [ip-0A0C0432:52749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.393339] [ip-0A0C0434:26562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.393104] [ip-0A0C0469:7778 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.393148] [ip-0A0C047D:97155:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.395346] [ip-0A0C0456:25536:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.395531] [ip-0A0C0414:54695:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.396209] [ip-0A0C0411:37466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.396981] [ip-0A0C0439:15400:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.397131] [ip-0A0C0439:15396:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.397242] [ip-0A0C044F:16052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.397371] [ip-0A0C0439:15403:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.397980] [ip-0A0C0477:3538 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.398221] [ip-0A0C0430:60600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.398697] [ip-0A0C0452:21658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.400112] [ip-0A0C0420:39060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.400476] [ip-0A0C041C:43658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.401529] [ip-0A0C0427:34705:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.401722] [ip-0A0C041E:37693:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.401847] [ip-0A0C044F:16049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.401963] [ip-0A0C0432:52745:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.402932] [ip-0A0C0450:91321:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.402959] [ip-0A0C044F:16055:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.403828] [ip-0A0C0419:41573:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.404769] [ip-0A0C0470:2969 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.404815] [ip-0A0C0451:20457:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.404948] [ip-0A0C047F:1826 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.405436] [ip-0A0C0448:29607:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.405938] [ip-0A0C0421:39226:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.406136] [ip-0A0C0476:95984:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.407103] [ip-0A0C0477:3534 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.407148] [ip-0A0C0471:7375 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.407997] [ip-0A0C0477:3532 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.408088] [ip-0A0C0453:23567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.408385] [ip-0A0C0419:41572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.409499] [ip-0A0C0447:25181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.409339] [ip-0A0C042F:32971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.409545] [ip-0A0C0447:25179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.410513] [ip-0A0C0407:42777:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.410531] [ip-0A0C0421:39222:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.411542] [ip-0A0C044A:17379:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.412459] [ip-0A0C041C:43660:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.412701] [ip-0A0C0450:91327:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.413026] [ip-0A0C0431:28526:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.413121] [ip-0A0C0452:21654:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.413891] [ip-0A0C041C:43657:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.414390] [ip-0A0C0451:20453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.414750] [ip-0A0C0451:20451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.415197] [ip-0A0C044F:16060:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.415485] [ip-0A0C047A:3784 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.415507] [ip-0A0C0454:22552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.415839] [ip-0A0C0427:34706:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.415751] [ip-0A0C0422:57108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.416242] [ip-0A0C0422:57106:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.417661] [ip-0A0C041C:43653:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.418093] [ip-0A0C0412:17329:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.418702] [ip-0A0C0422:57107:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.419519] [ip-0A0C0412:17332:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.419600] [ip-0A0C0412:17331:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.419589] [ip-0A0C0447:25174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.420289] [ip-0A0C041E:37694:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.420751] [ip-0A0C0421:39225:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.420811] [ip-0A0C0414:54678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.421441] [ip-0A0C0470:2973 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.421849] [ip-0A0C0447:25180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.422482] [ip-0A0C0442:17927:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.422951] [ip-0A0C047C:8844 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.423131] [ip-0A0C047D:97154:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.423925] [ip-0A0C0479:96966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.423998] [ip-0A0C0469:7779 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.424518] [ip-0A0C0442:17934:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.425533] [ip-0A0C0457:18480:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.425407] [ip-0A0C0427:34708:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.425706] [ip-0A0C0451:20455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.426070] [ip-0A0C0467:8105 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.427258] [ip-0A0C0423:31960:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.428718] [ip-0A0C042F:32976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.431060] [ip-0A0C0457:18475:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.431121] [ip-0A0C041E:37692:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.430776] [ip-0A0C047D:97156:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.431560] [ip-0A0C0426:36371:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.431599] [ip-0A0C046D:6953 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.432784] [ip-0A0C0471:7373 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.433135] [ip-0A0C044A:17381:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.433145] [ip-0A0C0421:39223:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.433409] [ip-0A0C0407:42779:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.433615] [ip-0A0C0474:4160 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.433945] [ip-0A0C0424:26917:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.434021] [ip-0A0C0424:26915:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.434444] [ip-0A0C0474:4165 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.434264] [ip-0A0C047D:97159:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.434789] [ip-0A0C046D:6954 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.435395] [ip-0A0C0420:39065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.435736] [ip-0A0C047C:8842 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.436028] [ip-0A0C0407:42778:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.436076] [ip-0A0C0420:39064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.436466] [ip-0A0C0442:17928:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.436921] [ip-0A0C0454:22542:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.437023] [ip-0A0C047A:3787 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.437022] [ip-0A0C0424:26914:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.437187] [ip-0A0C044A:17386:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.439698] [ip-0A0C041E:37689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.440088] [ip-0A0C0427:34709:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.441411] [ip-0A0C0424:26921:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.441518] [ip-0A0C0479:96968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.441392] [ip-0A0C045C:18949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.441603] [ip-0A0C0448:29603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.441293] [ip-0A0C047D:97153:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.442763] [ip-0A0C0452:21656:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.443509] [ip-0A0C0420:39068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.443672] [ip-0A0C0450:91324:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.444286] [ip-0A0C0416:24985:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.444452] [ip-0A0C0442:17929:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.444455] [ip-0A0C0416:24999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.444845] [ip-0A0C0426:36370:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.445912] [ip-0A0C0423:31966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.445833] [ip-0A0C0427:34707:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.445860] [ip-0A0C043A:23234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.446999] [ip-0A0C0423:31964:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.446867] [ip-0A0C0474:4164 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.447502] [ip-0A0C0452:21655:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.447848] [ip-0A0C0450:91328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.448002] [ip-0A0C0452:21653:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.448128] [ip-0A0C0479:96962:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.448343] [ip-0A0C0440:17486:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.448726] [ip-0A0C046D:6958 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.450479] [ip-0A0C0476:95985:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.450784] [ip-0A0C0414:54675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.450818] [ip-0A0C0414:54670:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.452881] [ip-0A0C0407:42774:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.452793] [ip-0A0C0448:29604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.453336] [ip-0A0C045B:25819:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.454260] [ip-0A0C0448:29609:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.454881] [ip-0A0C045B:25823:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.455016] [ip-0A0C0434:26567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.455077] [ip-0A0C0434:26561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.455324] [ip-0A0C0474:4161 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.456812] [ip-0A0C0471:7377 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.457098] [ip-0A0C0407:42780:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.458240] [ip-0A0C042B:35686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.458761] [ip-0A0C0416:24986:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.459139] [ip-0A0C041D:34778:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.459565] [ip-0A0C0411:37460:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.460348] [ip-0A0C0423:31962:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.460714] [ip-0A0C0479:96965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.460628] [ip-0A0C0471:7379 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.460311] [ip-0A0C0469:7777 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.460830] [ip-0A0C0450:91325:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.461383] [ip-0A0C0470:2971 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.461914] [ip-0A0C0458:25913:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.462415] [ip-0A0C0469:7774 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.463684] [ip-0A0C0407:42775:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.464094] [ip-0A0C0518:36443:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.464351] [ip-0A0C0469:7776 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.464848] [ip-0A0C044A:17378:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.465281] [ip-0A0C0430:60602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.466106] [ip-0A0C041D:34766:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.466028] [ip-0A0C0453:23562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.466125] [ip-0A0C0431:28527:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.466126] [ip-0A0C0458:25911:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.466761] [ip-0A0C046D:6956 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.468034] [ip-0A0C0456:25535:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.468119] [ip-0A0C041D:34767:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.468297] [ip-0A0C045C:18944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.469815] [ip-0A0C044A:17376:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.470616] [ip-0A0C0434:26565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.471024] [ip-0A0C044B:21486:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.471264] [ip-0A0C045B:25825:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.471288] [ip-0A0C044B:21483:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.471350] [ip-0A0C0434:26564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.472747] [ip-0A0C044A:17377:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.472987] [ip-0A0C0423:31961:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.473374] [ip-0A0C047F:1828 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.474213] [ip-0A0C0467:8110 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.474566] [ip-0A0C0423:31959:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.474888] [ip-0A0C0426:36372:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.475268] [ip-0A0C0479:96963:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.474868] [ip-0A0C043A:23230:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.475560] [ip-0A0C0479:96964:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.475823] [ip-0A0C0440:17491:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.475944] [ip-0A0C0454:22538:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.477072] [ip-0A0C0476:95978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.476967] [ip-0A0C042F:32975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.477405] [ip-0A0C0471:7376 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.477889] [ip-0A0C047F:1831 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.478447] [ip-0A0C0411:37462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.478643] [ip-0A0C0411:37467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.478829] [ip-0A0C0411:37465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.479430] [ip-0A0C0450:91326:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.479451] [ip-0A0C042B:35689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.480625] [ip-0A0C0470:2975 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.481455] [ip-0A0C0448:29602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.482261] [ip-0A0C0414:54674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.482745] [ip-0A0C0456:25534:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.482535] [ip-0A0C0476:95980:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.482478] [ip-0A0C0414:54676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.482946] [ip-0A0C0448:29601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.483240] [ip-0A0C0470:2976 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.483449] [ip-0A0C047F:1829 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.484227] [ip-0A0C0470:2977 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.484853] [ip-0A0C044B:21487:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.485089] [ip-0A0C043A:23235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.485567] [ip-0A0C0476:95979:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.486288] [ip-0A0C042B:35701:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.486692] [ip-0A0C0454:22540:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.487096] [ip-0A0C047A:3779 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.488825] [ip-0A0C042F:32978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.489741] [ip-0A0C047F:1832 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.490592] [ip-0A0C047A:3781 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.490744] [ip-0A0C047A:3782 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.491251] [ip-0A0C0518:36445:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.491382] [ip-0A0C0425:41303:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.491939] [ip-0A0C0426:36367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.493064] [ip-0A0C0458:25917:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.495029] [ip-0A0C045B:25824:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.495316] [ip-0A0C045C:18950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.496835] [ip-0A0C0416:24991:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.497744] [ip-0A0C0426:36374:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.500659] [ip-0A0C0416:24992:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.500748] [ip-0A0C0416:24990:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.503231] [ip-0A0C041D:34769:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.503567] [ip-0A0C041D:34772:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.503445] [ip-0A0C0426:36369:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.503451] [ip-0A0C043A:23228:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.504208] [ip-0A0C0518:36441:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.506972] [ip-0A0C045B:25849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.507276] [ip-0A0C0453:23563:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.507295] [ip-0A0C0457:18472:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.508101] [ip-0A0C045B:25821:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.509463] [ip-0A0C042B:35688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.510579] [ip-0A0C0456:25538:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.510656] [ip-0A0C0457:18476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.510886] [ip-0A0C0457:18477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.511175] [ip-0A0C0425:41300:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.512872] [ip-0A0C042B:35687:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.514758] [ip-0A0C0454:22541:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.514831] [ip-0A0C0454:22545:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.514789] [ip-0A0C0467:8109 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.515733] [ip-0A0C0456:25541:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.516192] [ip-0A0C0456:25555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.516279] [ip-0A0C044B:21488:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.515875] [ip-0A0C043A:23229:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.517259] [ip-0A0C042F:32973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.517918] [ip-0A0C042F:32974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.518158] [ip-0A0C045C:18945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.522215] [ip-0A0C045C:18951:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.522074] [ip-0A0C042F:32972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.522955] [ip-0A0C047C:8840 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.523602] [ip-0A0C0454:22544:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.523701] [ip-0A0C045C:18946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.526082] [ip-0A0C0431:28522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.526994] [ip-0A0C0440:17484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.529148] [ip-0A0C0467:8106 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.529490] [ip-0A0C0453:23580:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.529866] [ip-0A0C0518:36447:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.530044] [ip-0A0C0480:95572:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.530513] [ip-0A0C0480:95574:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.530686] [ip-0A0C0480:95575:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.531188] [ip-0A0C0431:28525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.532429] [ip-0A0C044B:21482:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.532521] [ip-0A0C044B:21484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.533193] [ip-0A0C0518:36448:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.534617] [ip-0A0C0453:23564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.534687] [ip-0A0C0467:8104 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.535317] [ip-0A0C044B:21489:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.535509] [ip-0A0C0467:8108 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.540093] [ip-0A0C0425:41304:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.545384] [ip-0A0C0425:41299:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.545949] [ip-0A0C0440:17492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.549421] [ip-0A0C0430:60599:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.550207] [ip-0A0C0458:25910:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.551713] [ip-0A0C0431:28528:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.553280] [ip-0A0C0425:41297:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.553548] [ip-0A0C0458:25914:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.554840] [ip-0A0C0425:41301:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.555382] [ip-0A0C0453:23569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.555774] [ip-0A0C0480:95569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.557988] [ip-0A0C0458:25915:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.558800] [ip-0A0C0453:23566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.562345] [ip-0A0C047C:8843 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.565805] [ip-0A0C0430:60598:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.567337] [ip-0A0C0431:28520:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.568948] [ip-0A0C0431:28524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.569343] [ip-0A0C047C:8841 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.570094] [ip-0A0C0430:60595:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.570341] [ip-0A0C0480:95579:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.570555] [ip-0A0C0480:95571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.570783] [ip-0A0C0518:36446:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.570923] [ip-0A0C0518:36442:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.572490] [ip-0A0C0440:17485:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.577707] [ip-0A0C047C:8845 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.577895] [ip-0A0C047C:8838 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.577772] [ip-0A0C0458:25916:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.580209] [ip-0A0C0430:60597:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.580774] [ip-0A0C0430:60596:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.598745] [ip-0A0C0440:17487:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634620398.600344] [ip-0A0C0440:17488:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634620399492, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634620399532, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634620399533, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634620399533, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634620399534, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634620399534, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634620399534, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:30] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:31] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[05:13:32] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0409:0:93519 - context.c:584] INFO job (ID: 867538644773151643) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:93519 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:93519 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:93518 - context.c:584] INFO job (ID: 867538815794624290) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:93518 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:93518 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:93513 - context.c:584] INFO job (ID: 867538103433499879) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:93513 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:93513 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:93514 - context.c:584] INFO job (ID: 867537852903533097) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:93514 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:93514 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:93522 - context.c:584] INFO job (ID: 867537786861641390) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:93522 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:93522 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:93515 - context.c:584] INFO job (ID: 867538234269330423) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:93515 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:93515 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:93516 - context.c:584] INFO job (ID: 867538412413862426) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:93516 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:93516 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:93521 - context.c:584] INFO job (ID: 867538709081935807) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:93521 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:93521 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493218, "event_type": "POINT_IN_TIME", "key": "seed", "value": 693259640, "metadata": {"file": "main.py", "lineno": 72}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493219, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493219, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493219, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493219, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493219, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493219, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493219, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493219, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493219, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493220, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620493220, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:53] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[05:14:54] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634620517115, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634620517122, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620517127, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634620517127, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634620519678, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634620519679, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634620519679, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634620519679, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634620521275, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 2105.6922742462175, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620521276, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620521276, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 2105.6922742462175, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634620521276, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620521276, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620521952, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4972.170474479492, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620521952, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620521952, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4972.170474479492, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620521952, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620521953, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620522627, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4979.8836025128285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620522628, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620522628, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4979.8836025128285, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634620522628, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620522628, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620523289, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5086.775783345184, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620523289, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620523290, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5086.775783345184, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634620523290, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620523290, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620523930, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5255.132199961741, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620523930, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620523930, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5255.132199961741, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634620523930, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620523930, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620524572, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5236.680923103677, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620524572, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620524573, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5236.680923103677, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634620524573, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620524573, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620525210, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5271.567683060944, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620525211, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620525211, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5271.567683060944, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634620525211, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620525211, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620525839, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5357.699812271469, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620525839, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620525839, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5357.699812271469, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634620525839, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620525839, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620526472, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5315.395199916721, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620526472, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620526472, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5315.395199916721, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634620526472, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620526472, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620527105, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5309.6576361788175, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620527105, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620527106, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5309.6576361788175, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634620527106, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620527106, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620527730, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5382.430730074537, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620527731, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620527731, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5382.430730074537, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634620527731, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620527731, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620528352, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5407.623408067963, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620528353, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620528353, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5407.623408067963, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634620528353, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620528353, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620528983, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5334.816270119322, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620528984, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620528984, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5334.816270119322, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634620528984, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620528984, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620529606, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5403.449699266484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620529606, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620529606, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5403.449699266484, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634620529606, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620529607, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620530225, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5433.109578874931, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620530226, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620530226, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5433.109578874931, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634620530226, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620530226, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620530846, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5423.506160144515, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620530846, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620530846, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5423.506160144515, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634620530846, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620530846, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620531483, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5278.424929182715, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620531483, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620531483, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5278.424929182715, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634620531483, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620531484, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620532118, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5300.529170498333, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620532118, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620532118, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5300.529170498333, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634620532118, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620532118, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620532745, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5363.684624574541, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620532745, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620532745, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5363.684624574541, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634620532745, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620532745, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620533375, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5341.444353185398, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620533375, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620533375, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5341.444353185398, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634620533375, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620533375, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620534008, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5308.207685432211, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620534009, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620534009, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5308.207685432211, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634620534009, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620534009, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620534640, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5324.297380781111, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620534641, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620534641, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5324.297380781111, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634620534641, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620534641, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620535271, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5334.198380534621, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620535272, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620535272, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5334.198380534621, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634620535272, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620535272, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620535900, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5350.7408227229325, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620535900, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620535901, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5350.7408227229325, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634620535901, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620535901, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620536527, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5364.299155208784, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620536528, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620536528, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5364.299155208784, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634620536528, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620536528, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620537165, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5280.580753437207, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620537165, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620537165, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5280.580753437207, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634620537165, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620537165, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620537796, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5324.679597960635, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620537797, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620537797, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5324.679597960635, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634620537797, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620537797, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620538423, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5364.864811158176, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620538424, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620538424, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5364.864811158176, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634620538424, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620538424, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620539050, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5371.291432300024, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620539050, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620539050, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5371.291432300024, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634620539050, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620539051, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620539683, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5314.000222471507, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620539683, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620539683, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5314.000222471507, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634620539683, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620539684, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620540309, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5374.382420452218, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620540309, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620540309, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5374.382420452218, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634620540309, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620540310, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620540938, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5347.618114157762, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620540938, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620540939, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5347.618114157762, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634620540939, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620540939, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620541566, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5356.123759486281, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620541567, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620541567, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5356.123759486281, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634620541567, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620541567, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620542189, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5404.585271756326, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620542189, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620542189, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5404.585271756326, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634620542189, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620542189, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620542814, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5376.652227557512, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620542815, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620542815, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5376.652227557512, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634620542815, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620542815, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620543434, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.798685763907, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620543435, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620543435, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.798685763907, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634620543435, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620543435, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620544077, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5236.270377458811, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620544077, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620544077, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5236.270377458811, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634620544077, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620544077, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620544703, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5370.014289187238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620544704, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620544704, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5370.014289187238, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634620544704, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620544704, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620545322, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5435.892611039989, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620545323, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620545323, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5435.892611039989, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634620545323, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620545323, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620545950, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5363.019211997937, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620545950, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620545950, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5363.019211997937, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634620545950, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620545951, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620546579, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5351.78931067076, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620546579, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620546579, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5351.78931067076, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634620546579, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620546579, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620547197, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5441.599605844072, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620547197, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620547197, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5441.599605844072, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634620547197, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620547197, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620547818, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5416.706296555382, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620547818, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620547818, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5416.706296555382, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634620547818, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620547819, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620548439, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5416.750017968889, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620548439, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620548439, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5416.750017968889, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634620548440, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620548440, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620549056, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5452.707824381849, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620549056, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620549057, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5452.707824381849, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634620549057, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620549057, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620549674, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5448.133567760402, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620549674, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620549674, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5448.133567760402, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634620549674, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620549675, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620550295, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5418.841141762939, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620550295, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620550295, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5418.841141762939, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634620550295, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620550296, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620550914, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5434.446252131016, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620550914, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620550914, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5434.446252131016, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634620550914, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620550915, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620551536, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5412.231710159911, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620551536, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620551536, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5412.231710159911, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634620551536, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620551536, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620552160, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5391.577624866768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620552160, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620552160, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5391.577624866768, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634620552231, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620552232, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620552253, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620552681, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.881869375705719, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620552681, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620552843, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5495.7156316488945, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620552844, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620552844, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5495.7156316488945, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634620552960, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620552961, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620552977, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620553372, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8807142376899719, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620553372, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620553576, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5462.686049256524, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620553576, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620553577, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5462.686049256524, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634620553617, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620553617, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620553632, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620554087, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8747950792312622, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620554088, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620554299, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4927.758290257712, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620554300, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620554300, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4927.758290257712, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634620554335, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620554335, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620554350, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620554749, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8824224472045898, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620554749, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620554943, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5524.088579139335, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620554944, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620554944, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5524.088579139335, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634620554978, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620554978, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620554993, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620555413, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8834091424942017, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620555413, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620555607, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5341.654909704942, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620555608, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620555608, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5341.654909704942, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634620555647, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620555647, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620555662, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620556081, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8850263953208923, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620556081, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620556277, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5339.260808387113, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620556277, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620556277, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5339.260808387113, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634620556313, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620556313, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620556331, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620556749, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8812741041183472, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620556749, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620556947, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5303.9244609555735, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620556947, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620556948, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5303.9244609555735, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634620556983, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620556983, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620557000, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620557396, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8908065557479858, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620557397, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620557595, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5491.359951994451, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620557596, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620557596, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5491.359951994451, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634620557631, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620557632, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620557649, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620558046, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8810367584228516, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620558046, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620558239, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5533.345651590936, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620558239, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620558240, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5533.345651590936, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634620558275, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620558275, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620558291, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620558714, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8970816135406494, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620558714, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620558909, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5305.581791958763, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620558909, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620558909, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5305.581791958763, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634620558998, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620558998, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620559013, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620559410, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8851770758628845, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620559410, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620559620, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5401.587813364548, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620559621, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620559621, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5401.587813364548, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634620559656, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620559656, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620559673, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620560091, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8696445226669312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620560091, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620560294, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5264.639727444908, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620560295, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620560295, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5264.639727444908, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634620560330, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620560331, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620560348, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620560769, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8873032927513123, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620560769, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620560965, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5300.078653895406, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620560965, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620560965, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5300.078653895406, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634620561001, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620561001, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620561018, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620561435, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8991779088973999, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620561435, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620561631, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5338.476059012086, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620561631, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620561631, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5338.476059012086, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634620561658, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620561659, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620561675, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634620562086, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8929154872894287, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634620562086, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634620562287, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5344.655117880897, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620562288, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620562288, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5344.655117880897, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634620562323, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620562323, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620562338, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634620562737, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8954876065254211, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634620562737, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634620562941, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5436.848888218476, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620562942, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620562942, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5436.848888218476, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634620562977, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620562977, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620562992, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634620563411, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8838977813720703, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634620563412, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634620563608, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5321.125085615339, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620563609, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620563609, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5321.125085615339, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634620563645, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620563645, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620563662, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634620564080, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9037129878997803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634620564081, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634620564284, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5266.5048196111875, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620564284, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620564284, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5266.5048196111875, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634620564319, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620564320, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620564335, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634620564753, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8901310563087463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634620564753, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634620564952, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5313.9220772952685, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620564953, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620564953, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5313.9220772952685, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634620564987, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620564988, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620565003, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634620565423, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8959024548530579, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634620565423, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634620565623, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5288.274961481106, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620565624, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620565624, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5288.274961481106, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634620565659, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620565659, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620565675, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634620566093, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8779067993164062, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634620566093, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634620566293, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5301.322744376529, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620566294, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620566294, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5301.322744376529, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634620566329, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620566330, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620566345, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634620566744, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8786681890487671, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634620566744, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634620566942, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5486.143949922338, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620566942, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620566943, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5486.143949922338, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634620566978, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620566978, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620566995, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634620567400, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8928343653678894, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634620567401, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634620567600, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5404.9086241988725, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620567601, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620567601, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5404.9086241988725, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634620567640, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620567640, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620567658, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634620568066, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8824425935745239, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634620568066, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634620568264, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5390.193919611771, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620568264, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620568264, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5390.193919611771, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634620568471, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620568472, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620568486, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634620568885, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8981221914291382, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634620568885, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634620569124, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5154.927878583484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620569124, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620569124, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5154.927878583484, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634620569159, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620569159, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620569174, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634620569584, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8864515423774719, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634620569584, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634620569773, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5467.231760455849, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620569774, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620569774, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5467.231760455849, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634620569809, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620569809, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620569825, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634620570246, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8692939281463623, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634620570246, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634620570443, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5307.324103954227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620570443, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620570443, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5307.324103954227, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634620570482, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620570482, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620570498, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634620570896, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8766998052597046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634620570896, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634620571132, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5173.27178059503, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620571132, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620571132, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5173.27178059503, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634620571167, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620571167, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620571183, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634620571581, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8796040415763855, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634620571581, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634620571790, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5393.317027825626, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620571791, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620571791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5393.317027825626, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634620571825, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620571825, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620571842, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634620572239, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8822479248046875, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634620572239, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634620572446, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5416.033908479541, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620572446, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620572446, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5416.033908479541, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634620572481, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620572482, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620572496, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634620572895, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8924164772033691, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634620572896, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634620573094, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5484.929019813737, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620573095, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620573095, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5484.929019813737, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634620573130, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620573130, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620573145, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634620573565, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8879928588867188, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634620573565, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634620573759, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5343.157631334801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620573759, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620573759, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5343.157631334801, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634620573794, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620573795, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620573809, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634620574209, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8935546875, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634620574209, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634620574403, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5524.302954137604, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620574403, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620574403, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5524.302954137604, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634620574441, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620574441, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620574455, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634620574877, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8956548571586609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634620574877, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634620575071, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5338.447747624892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620575071, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620575071, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5338.447747624892, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634620575106, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620575107, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620575124, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634620575541, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8918167352676392, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634620575541, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634620575735, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5350.978525252326, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620575735, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620575735, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5350.978525252326, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634620575770, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620575771, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620575785, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634620576206, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8904977440834045, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634620576206, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634620576401, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5335.420163263267, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620576401, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620576401, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5335.420163263267, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634620576436, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620576436, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620576450, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634620576872, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8840609788894653, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634620576873, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634620577079, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5232.020831763241, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620577079, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620577079, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5232.020831763241, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634620577115, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620577115, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620577129, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634620577550, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8879316449165344, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634620577550, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634620577747, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5316.3316083598465, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620577748, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620577748, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5316.3316083598465, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634620577783, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620577783, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620577798, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634620578219, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8990001678466797, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634620578219, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634620578418, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5293.855018393257, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620578418, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620578418, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5293.855018393257, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634620578453, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620578453, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620578467, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634620578869, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8992361426353455, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634620578869, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634620579070, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5445.0792080776355, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620579071, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620579071, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5445.0792080776355, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634620579106, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620579107, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620579123, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634620579540, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9032659530639648, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634620579540, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634620579739, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5313.1948362763615, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620579740, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620579740, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5313.1948362763615, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634620579775, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620579775, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620579792, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634620580218, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8949546813964844, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634620580218, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634620580418, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5228.111751291831, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620580418, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620580418, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5228.111751291831, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634620580453, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620580454, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620580468, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634620580867, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8902133107185364, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634620580868, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634620581066, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5492.72972604288, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620581066, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620581066, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5492.72972604288, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634620581102, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620581102, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620581115, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634620581545, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8915896415710449, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634620581545, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634620581746, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5217.856418493008, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620581747, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620581747, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5217.856418493008, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634620581782, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620581782, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620581795, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634620582213, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8984785676002502, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634620582213, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634620582416, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5302.284124407189, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620582416, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620582416, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5302.284124407189, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634620582451, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620582452, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620582465, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634620582864, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8912967443466187, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634620582865, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634620583065, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5479.207515215418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620583065, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620583066, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5479.207515215418, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634620583100, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620583101, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620583115, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634620583514, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8921222686767578, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634620583514, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634620583719, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5435.760520245929, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620583719, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620583719, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5435.760520245929, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634620583754, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620583755, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620583772, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634620584168, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8981496095657349, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634620584168, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634620584366, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5500.715822178333, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620584366, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620584366, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5500.715822178333, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634620584402, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620584402, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634620584418, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634620584815, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9096285104751587, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634620584816, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634620584816, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634620585013, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5504.845522586573, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634620585013, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634620585013, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5504.845522586573, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1960}}
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:31 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:32 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:32 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:32 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:32 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:33 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:33 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
ENDING TIMING RUN AT 2021-10-19 05:16:34 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,201,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:35 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:36 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:37 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:37 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:37 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:38 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:39 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:40 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:41 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:42 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:43 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
ENDING TIMING RUN AT 2021-10-19 05:16:44 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 05:13:13 AM
