+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019062506366944721
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019062506366944721
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019062506366944721
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019062506366944721
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07396/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459/unet3d_96x8x1_211019062506366944721_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C0437
Clearing cache on ip-0A0C04C2
Clearing cache on ip-0A0C0473
Clearing cache on ip-0A0C0483
Clearing cache on ip-0A0C0488
Clearing cache on ip-0A0C0441
Clearing cache on ip-0A0C048A
Clearing cache on ip-0A0C045F
Clearing cache on ip-0A0C0435
Clearing cache on ip-0A0C0495
Clearing cache on ip-0A0C048C
Clearing cache on ip-0A0C0449
Clearing cache on ip-0A0C0492
Clearing cache on ip-0A0C049B
Clearing cache on ip-0A0C0472
Clearing cache on ip-0A0C0489
Clearing cache on ip-0A0C04A4
Clearing cache on ip-0A0C0487
Clearing cache on ip-0A0C0498
Clearing cache on ip-0A0C042D
Clearing cache on ip-0A0C046C
Clearing cache on ip-0A0C0436
Clearing cache on ip-0A0C047E
Clearing cache on ip-0A0C0491
Clearing cache on ip-0A0C04AF
Clearing cache on ip-0A0C04A1
Clearing cache on ip-0A0C040F
Clearing cache on ip-0A0C0496
Clearing cache on ip-0A0C0438
Clearing cache on ip-0A0C0493
Clearing cache on ip-0A0C046E
Clearing cache on ip-0A0C0466
Clearing cache on ip-0A0C04B0
Clearing cache on ip-0A0C04AE
Clearing cache on ip-0A0C044D
Clearing cache on ip-0A0C0460
Clearing cache on ip-0A0C04A2
Clearing cache on ip-0A0C0490
Clearing cache on ip-0A0C04B5
Clearing cache on ip-0A0C04CC
Clearing cache on ip-0A0C04A9
Clearing cache on ip-0A0C049C
Clearing cache on ip-0A0C04D3
Clearing cache on ip-0A0C0499
Clearing cache on ip-0A0C04C9
Clearing cache on ip-0A0C0497
Clearing cache on ip-0A0C04C7
Clearing cache on ip-0A0C04C3
Clearing cache on ip-0A0C04B1
Clearing cache on ip-0A0C04CD
Clearing cache on ip-0A0C04C0
Clearing cache on ip-0A0C04A8
Clearing cache on ip-0A0C04AB
Clearing cache on ip-0A0C04A0
Clearing cache on ip-0A0C048B
Clearing cache on ip-0A0C04B2
Clearing cache on ip-0A0C04DB
Clearing cache on ip-0A0C04C8
Clearing cache on ip-0A0C049D
Clearing cache on ip-0A0C0443
Clearing cache on ip-0A0C049F
Clearing cache on ip-0A0C04B3
Clearing cache on ip-0A0C0486
Clearing cache on ip-0A0C04BC
Clearing cache on ip-0A0C04AC
Clearing cache on ip-0A0C04D9
Clearing cache on ip-0A0C04A5
Clearing cache on ip-0A0C0484
Clearing cache on ip-0A0C048D
Clearing cache on ip-0A0C04CF
Clearing cache on ip-0A0C04C6
Clearing cache on ip-0A0C0464
Clearing cache on ip-0A0C04B7
Clearing cache on ip-0A0C04C5
Clearing cache on ip-0A0C04D8
Clearing cache on ip-0A0C04BE
Clearing cache on ip-0A0C04A7
Clearing cache on ip-0A0C0461
Clearing cache on ip-0A0C04BB
Clearing cache on ip-0A0C04AD
Clearing cache on ip-0A0C0485
Clearing cache on ip-0A0C04A6
Clearing cache on ip-0A0C0482
Clearing cache on ip-0A0C04B9
Clearing cache on ip-0A0C0429
Clearing cache on ip-0A0C04D4
Clearing cache on ip-0A0C04BA
Clearing cache on ip-0A0C04C4
Clearing cache on ip-0A0C04BD
Clearing cache on ip-0A0C04AA
Clearing cache on ip-0A0C04B6
Clearing cache on ip-0A0C0494
Clearing cache on ip-0A0C04B4
Clearing cache on ip-0A0C048F
Clearing cache on ip-0A0C046B
Clearing cache on ip-0A0C04DA
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051459:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:25:10 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634624715.541231] [ip-0A0C04BA:15593:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.554674] [ip-0A0C0437:59454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.564900] [ip-0A0C04C5:82407:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.566073] [ip-0A0C0498:58079:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.571142] [ip-0A0C04BA:15588:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.576950] [ip-0A0C0498:58072:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.581936] [ip-0A0C0466:65781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.595274] [ip-0A0C0482:50555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.606891] [ip-0A0C0482:50552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.608037] [ip-0A0C047E:59788:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.610690] [ip-0A0C04BA:15587:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.613136] [ip-0A0C04CC:81052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.615696] [ip-0A0C0437:59450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.617569] [ip-0A0C04CC:81054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.628090] [ip-0A0C04AD:25091:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.628878] [ip-0A0C04AA:22893:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.631349] [ip-0A0C04BA:15590:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.631377] [ip-0A0C04D8:81065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.631143] [ip-0A0C04A9:47308:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.635890] [ip-0A0C0495:47303:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.636438] [ip-0A0C0435:2151 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.641861] [ip-0A0C047E:59783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.643396] [ip-0A0C0495:47305:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.649150] [ip-0A0C047E:59786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.650110] [ip-0A0C04B7:19079:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.652010] [ip-0A0C0464:63260:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.654180] [ip-0A0C04AD:25092:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.654529] [ip-0A0C04A6:47851:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.658843] [ip-0A0C04D8:81064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.659824] [ip-0A0C048B:45977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.660744] [ip-0A0C04AA:22890:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.661246] [ip-0A0C04A4:48232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.661679] [ip-0A0C049D:50421:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.663294] [ip-0A0C048C:44000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.663993] [ip-0A0C04C8:81219:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.666605] [ip-0A0C04A6:47854:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.672556] [ip-0A0C046E:65155:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.672967] [ip-0A0C0437:59451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.673689] [ip-0A0C048C:44002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.676633] [ip-0A0C0473:61465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.677509] [ip-0A0C0496:29736:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.680055] [ip-0A0C045F:60919:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.681674] [ip-0A0C0435:2133 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.684321] [ip-0A0C04C3:79468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.684315] [ip-0A0C04C3:79466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.684315] [ip-0A0C04C3:79464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.687831] [ip-0A0C04B0:34630:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.689681] [ip-0A0C04A7:25052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.690076] [ip-0A0C0466:65782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.690242] [ip-0A0C04A9:47281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.691095] [ip-0A0C04D8:81062:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.695162] [ip-0A0C0495:47302:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.695320] [ip-0A0C04BD:78355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.695513] [ip-0A0C04C6:76350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.695580] [ip-0A0C04A8:53270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.696736] [ip-0A0C049D:50418:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.698857] [ip-0A0C0437:59452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.700397] [ip-0A0C0464:63261:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.700729] [ip-0A0C04DA:74756:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.702292] [ip-0A0C04B5:48815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.702646] [ip-0A0C0482:50550:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.703303] [ip-0A0C0491:45114:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.706110] [ip-0A0C045F:60920:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.708592] [ip-0A0C04C5:82410:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.709156] [ip-0A0C04B0:34632:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.709502] [ip-0A0C04BA:15612:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.709314] [ip-0A0C04B7:19081:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.709686] [ip-0A0C04C5:82408:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.710748] [ip-0A0C04AA:22896:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.714112] [ip-0A0C0496:29733:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.714080] [ip-0A0C046E:65154:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.715315] [ip-0A0C048A:59846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.716960] [ip-0A0C048B:45974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.717804] [ip-0A0C0499:49452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.718786] [ip-0A0C04B7:19076:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.719310] [ip-0A0C0466:65779:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.719364] [ip-0A0C04A6:47852:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.720680] [ip-0A0C04BE:81208:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.721623] [ip-0A0C04C5:82409:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.722055] [ip-0A0C0473:61472:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.723252] [ip-0A0C0487:63240:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.722155] [ip-0A0C049B:36973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.726303] [ip-0A0C0436:63466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.726577] [ip-0A0C04B4:57642:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.727648] [ip-0A0C0436:63470:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.728027] [ip-0A0C0466:65778:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.730789] [ip-0A0C04AD:25094:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.732828] [ip-0A0C04A5:50802:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.733539] [ip-0A0C04CC:81050:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.735061] [ip-0A0C049F:35979:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.737444] [ip-0A0C0461:26934:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.737437] [ip-0A0C0461:26947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.736125] [ip-0A0C049C:34806:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.737824] [ip-0A0C0498:58075:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.739039] [ip-0A0C04BA:15592:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.740343] [ip-0A0C04BA:15591:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.740365] [ip-0A0C04B0:34633:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.740831] [ip-0A0C046C:58513:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.740766] [ip-0A0C048C:44001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.742109] [ip-0A0C0437:59453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.742233] [ip-0A0C0483:62181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.742721] [ip-0A0C047E:59781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.746936] [ip-0A0C04BA:15589:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.748020] [ip-0A0C0435:2135 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.749474] [ip-0A0C04AB:38505:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.750577] [ip-0A0C0461:26933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.751406] [ip-0A0C04C5:82432:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.752072] [ip-0A0C04A9:47287:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.753635] [ip-0A0C04CC:81047:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.754867] [ip-0A0C04A6:47848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.755406] [ip-0A0C048F:52584:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.753990] [ip-0A0C049C:34805:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.756089] [ip-0A0C0491:45118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.756171] [ip-0A0C0491:45113:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.756432] [ip-0A0C0460:62797:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.756135] [ip-0A0C0493:44598:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.756909] [ip-0A0C0489:63445:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.755935] [ip-0A0C049B:36975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.757376] [ip-0A0C0489:63444:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.758032] [ip-0A0C046B:56944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.758164] [ip-0A0C04CD:77974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.758662] [ip-0A0C0498:58073:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.760996] [ip-0A0C04D9:4287 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.761718] [ip-0A0C047E:59784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.763555] [ip-0A0C048A:59841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.763158] [ip-0A0C0498:58078:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.764072] [ip-0A0C0485:28195:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.764231] [ip-0A0C04C3:79467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.763625] [ip-0A0C0482:50549:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.765360] [ip-0A0C045F:60923:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.765247] [ip-0A0C04C0:80086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.765971] [ip-0A0C040F:33183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.766539] [ip-0A0C0437:59449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.766889] [ip-0A0C0464:63259:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.768102] [ip-0A0C04C5:82406:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.768605] [ip-0A0C0466:65784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.768063] [ip-0A0C0498:58077:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.769336] [ip-0A0C0497:95744:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.770636] [ip-0A0C0435:2152 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.770911] [ip-0A0C04C5:82405:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.771220] [ip-0A0C0499:49453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.771989] [ip-0A0C0437:59456:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.771875] [ip-0A0C0494:44740:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.772587] [ip-0A0C04DB:73360:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.772820] [ip-0A0C0460:62800:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.773225] [ip-0A0C0437:59455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.772951] [ip-0A0C04B5:48819:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.773693] [ip-0A0C04C4:80481:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.773718] [ip-0A0C04C4:80483:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.774351] [ip-0A0C04DA:74758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.774500] [ip-0A0C0499:49449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.774438] [ip-0A0C04DA:74759:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.776287] [ip-0A0C04C5:82404:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.776452] [ip-0A0C04A4:48231:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.776762] [ip-0A0C04C2:5737 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.777605] [ip-0A0C04DA:74754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.777907] [ip-0A0C0497:95742:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.777995] [ip-0A0C04B9:81357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.779215] [ip-0A0C04CC:81051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.780108] [ip-0A0C04C6:76349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.780983] [ip-0A0C04B7:19078:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.780768] [ip-0A0C0498:58076:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.780814] [ip-0A0C0498:58074:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.781904] [ip-0A0C0441:56233:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.782549] [ip-0A0C0493:44600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.783373] [ip-0A0C047E:59782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.784228] [ip-0A0C04C8:81244:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.784171] [ip-0A0C048D:49050:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.784370] [ip-0A0C046E:65160:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.785003] [ip-0A0C0466:65783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.785081] [ip-0A0C04C8:81216:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.785138] [ip-0A0C0487:63241:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.783184] [ip-0A0C049C:34803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.785657] [ip-0A0C04A7:25051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.786877] [ip-0A0C04A5:50800:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.787281] [ip-0A0C0487:63244:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.787792] [ip-0A0C0472:65203:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.787430] [ip-0A0C0482:50553:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.788368] [ip-0A0C04A5:50826:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.788798] [ip-0A0C04CD:77978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.789583] [ip-0A0C04D8:81069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.789836] [ip-0A0C04A4:48233:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.789568] [ip-0A0C04B5:48816:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.789352] [ip-0A0C0449:60698:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.790224] [ip-0A0C0495:47301:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.790216] [ip-0A0C04BD:78353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.790012] [ip-0A0C0438:63968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.790494] [ip-0A0C0492:43210:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.789997] [ip-0A0C04C6:76343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.790301] [ip-0A0C049F:35978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.791764] [ip-0A0C049D:50441:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.792289] [ip-0A0C0466:65785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.791994] [ip-0A0C04B6:17344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.793046] [ip-0A0C0436:63467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.795410] [ip-0A0C04A2:39568:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.793492] [ip-0A0C0482:50548:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.795576] [ip-0A0C046E:65159:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.796248] [ip-0A0C0496:29732:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.795916] [ip-0A0C048A:59842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.796768] [ip-0A0C0496:29737:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.796476] [ip-0A0C0485:28200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.797212] [ip-0A0C04C9:80215:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.796888] [ip-0A0C04B7:19077:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.796628] [ip-0A0C04A9:47283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.798732] [ip-0A0C04D9:4289 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.799012] [ip-0A0C0466:65780:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.799469] [ip-0A0C048B:45978:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.799969] [ip-0A0C04CC:81049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.800793] [ip-0A0C046B:56940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.801625] [ip-0A0C0449:60704:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.802601] [ip-0A0C0495:47304:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.802871] [ip-0A0C04CC:81053:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.802326] [ip-0A0C0482:50551:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.803450] [ip-0A0C04CC:81048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.803160] [ip-0A0C0443:26597:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.803579] [ip-0A0C04DB:73361:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.804237] [ip-0A0C0473:61470:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.804288] [ip-0A0C04DB:73364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.804672] [ip-0A0C04D8:81063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.804829] [ip-0A0C046C:58511:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.806061] [ip-0A0C04A0:37686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.805035] [ip-0A0C042D:34560:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.805869] [ip-0A0C04B4:57637:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.805798] [ip-0A0C04C0:80087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.806724] [ip-0A0C04AD:25096:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.807192] [ip-0A0C046C:58508:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.806910] [ip-0A0C0482:50554:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.805914] [ip-0A0C04B2:24216:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.806680] [ip-0A0C0484:65156:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.809001] [ip-0A0C048B:45972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.809094] [ip-0A0C04B9:81354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.809200] [ip-0A0C04A8:53272:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.809708] [ip-0A0C04C6:76344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.810761] [ip-0A0C04C3:79463:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.811390] [ip-0A0C0435:2138 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.811659] [ip-0A0C04B0:34636:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.811831] [ip-0A0C04B4:57640:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.812551] [ip-0A0C04B6:17347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.813152] [ip-0A0C0436:63468:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.813247] [ip-0A0C0464:63263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.813101] [ip-0A0C0494:44747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.813987] [ip-0A0C04AA:22891:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.813555] [ip-0A0C0490:49259:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.815400] [ip-0A0C04C3:79469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.815656] [ip-0A0C0495:47300:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.816842] [ip-0A0C042D:34557:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.818861] [ip-0A0C04AB:38500:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.819181] [ip-0A0C047E:59785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.820315] [ip-0A0C04D3:79480:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.821082] [ip-0A0C0435:2134 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.821435] [ip-0A0C0495:47299:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.821077] [ip-0A0C04A9:47282:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.822188] [ip-0A0C04AB:38506:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.822631] [ip-0A0C0495:47306:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.822375] [ip-0A0C04AA:22903:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.822748] [ip-0A0C04D8:81066:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.823125] [ip-0A0C047E:59787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.823058] [ip-0A0C04BD:78350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.822592] [ip-0A0C0429:53662:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.822891] [ip-0A0C049F:35981:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.824194] [ip-0A0C04A6:47849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.824224] [ip-0A0C044D:60338:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.826042] [ip-0A0C04BB:23176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.825935] [ip-0A0C04AA:22895:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.826209] [ip-0A0C04AA:22894:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.827591] [ip-0A0C04A0:37683:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.826615] [ip-0A0C04D8:81068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.826998] [ip-0A0C04D4:77419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.826883] [ip-0A0C048C:44003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.825890] [ip-0A0C04B2:24220:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.827833] [ip-0A0C04AD:25089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.828037] [ip-0A0C04C3:79465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.828105] [ip-0A0C04BC:76275:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.829308] [ip-0A0C0487:63242:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.829832] [ip-0A0C04BE:81212:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.830254] [ip-0A0C0499:49462:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.830458] [ip-0A0C04C9:80211:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.831242] [ip-0A0C04A7:25050:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.831072] [ip-0A0C04C3:79492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.831414] [ip-0A0C04C8:81218:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.830625] [ip-0A0C0494:44744:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.832926] [ip-0A0C0435:2132 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.832989] [ip-0A0C0435:2139 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.832491] [ip-0A0C04A8:53271:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.833315] [ip-0A0C0473:61471:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.833776] [ip-0A0C045F:60925:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.833640] [ip-0A0C04AD:25090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.834080] [ip-0A0C04A4:48236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.834177] [ip-0A0C0429:53666:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.835096] [ip-0A0C04B3:55914:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.835794] [ip-0A0C0483:62179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.835782] [ip-0A0C04C8:81220:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.836049] [ip-0A0C0483:62182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.836194] [ip-0A0C04AA:22892:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.836261] [ip-0A0C048B:45976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.837017] [ip-0A0C04A7:25049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.836276] [ip-0A0C048F:52587:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.838239] [ip-0A0C04A6:47853:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.838503] [ip-0A0C04B9:81356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.839265] [ip-0A0C049D:50419:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.839876] [ip-0A0C048D:49045:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.840509] [ip-0A0C0473:61466:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.841792] [ip-0A0C046E:65161:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.842074] [ip-0A0C04BD:78352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.841879] [ip-0A0C0464:63258:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.842298] [ip-0A0C04CD:77977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.842585] [ip-0A0C0460:62795:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.842728] [ip-0A0C04CD:77973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.843331] [ip-0A0C04AD:25095:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.843665] [ip-0A0C04A9:47284:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.843729] [ip-0A0C04A9:47280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.844560] [ip-0A0C04D8:81067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.844977] [ip-0A0C0491:45111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.844941] [ip-0A0C0486:63192:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.845311] [ip-0A0C04B7:19080:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.845634] [ip-0A0C04AE:53251:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.846552] [ip-0A0C04A6:47847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.846984] [ip-0A0C04A9:47285:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.848476] [ip-0A0C04A4:48237:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.849053] [ip-0A0C04BE:81209:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.848868] [ip-0A0C04D3:79477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.849578] [ip-0A0C04AD:25093:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.849111] [ip-0A0C04CF:6622 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.848322] [ip-0A0C049B:36974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.849774] [ip-0A0C04A6:47850:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.850193] [ip-0A0C04A4:48260:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.851753] [ip-0A0C046B:56949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.852677] [ip-0A0C04C7:82171:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.852749] [ip-0A0C0438:63970:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.853107] [ip-0A0C049D:50424:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.855090] [ip-0A0C04A2:39567:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.853653] [ip-0A0C044D:60332:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.854940] [ip-0A0C0486:63198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.855501] [ip-0A0C0438:63969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.854965] [ip-0A0C04CF:6595 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.856065] [ip-0A0C04D9:4291 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.854841] [ip-0A0C0488:61781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.856548] [ip-0A0C04B1:51278:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.857526] [ip-0A0C0473:61469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.857738] [ip-0A0C0464:63257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.858029] [ip-0A0C048D:49049:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.858918] [ip-0A0C04A7:25047:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.858715] [ip-0A0C04B7:19084:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.859714] [ip-0A0C048B:45973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.859436] [ip-0A0C0497:95740:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.860231] [ip-0A0C048B:45975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.860724] [ip-0A0C04C6:76345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.861631] [ip-0A0C04D4:77401:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.861547] [ip-0A0C04B7:19083:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.862298] [ip-0A0C04A4:48234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.862423] [ip-0A0C04A4:48235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.862235] [ip-0A0C048C:43997:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.862909] [ip-0A0C04B0:34634:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.862937] [ip-0A0C04B4:57638:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.863326] [ip-0A0C0464:63262:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.863651] [ip-0A0C04C8:81215:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.863605] [ip-0A0C0491:45115:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.864483] [ip-0A0C0496:29730:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.864086] [ip-0A0C04C8:81217:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.864518] [ip-0A0C0496:29734:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.862688] [ip-0A0C049B:36967:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.864745] [ip-0A0C04B3:55910:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.864415] [ip-0A0C048C:43998:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.865612] [ip-0A0C048A:59845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.866035] [ip-0A0C04C2:5736 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.865879] [ip-0A0C0436:63469:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.866264] [ip-0A0C04C8:81224:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.866606] [ip-0A0C0429:53659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.867412] [ip-0A0C045F:60922:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.867462] [ip-0A0C04B5:48821:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.868080] [ip-0A0C04BE:81207:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.867879] [ip-0A0C04B6:17346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.868889] [ip-0A0C04A1:65812:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.869028] [ip-0A0C04BD:78351:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.868311] [ip-0A0C04A8:53273:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.869491] [ip-0A0C0443:26591:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.869438] [ip-0A0C0490:49262:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.870383] [ip-0A0C046E:65156:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.870275] [ip-0A0C049B:36969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.872698] [ip-0A0C045F:60926:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.872485] [ip-0A0C0491:45112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.873447] [ip-0A0C0487:63246:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.874126] [ip-0A0C04D4:77405:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.875235] [ip-0A0C0464:63256:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.875394] [ip-0A0C0461:26935:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.875676] [ip-0A0C0473:61467:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.874087] [ip-0A0C0484:65158:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.877729] [ip-0A0C0492:43207:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.878058] [ip-0A0C0492:43203:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.876967] [ip-0A0C0436:63471:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.878664] [ip-0A0C04B0:34635:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.878690] [ip-0A0C0489:63449:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.878605] [ip-0A0C048C:44004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.879770] [ip-0A0C046E:65157:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.880128] [ip-0A0C046E:65158:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.880038] [ip-0A0C040F:33184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.880178] [ip-0A0C040F:33186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.879775] [ip-0A0C04A8:53274:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.880941] [ip-0A0C048B:45979:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.881331] [ip-0A0C04B0:34631:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.881402] [ip-0A0C049D:50420:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.881445] [ip-0A0C049D:50422:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.881605] [ip-0A0C048C:43999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.881824] [ip-0A0C0461:26936:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.881752] [ip-0A0C04D3:79479:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.882575] [ip-0A0C04BC:76276:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.883620] [ip-0A0C0483:62177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.884337] [ip-0A0C0496:29735:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.882446] [ip-0A0C049C:34811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.884935] [ip-0A0C0473:61473:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.884847] [ip-0A0C04A8:53268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.885664] [ip-0A0C04DA:74757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.886427] [ip-0A0C0496:29729:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.886565] [ip-0A0C045F:60921:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.886376] [ip-0A0C04A5:50806:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.886787] [ip-0A0C045F:60924:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.886972] [ip-0A0C046C:58512:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.887145] [ip-0A0C04B0:34637:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.887519] [ip-0A0C04B5:48814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.887934] [ip-0A0C0461:26932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.888406] [ip-0A0C04DA:74760:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.888548] [ip-0A0C04C2:5735 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.888395] [ip-0A0C04C6:76342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.889132] [ip-0A0C049D:50417:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.889072] [ip-0A0C04B5:48817:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.889765] [ip-0A0C048A:59848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.888899] [ip-0A0C04A8:53269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.888935] [ip-0A0C049F:35984:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.889483] [ip-0A0C0461:26930:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.889009] [ip-0A0C049F:35980:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.890123] [ip-0A0C04A1:65808:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.891043] [ip-0A0C04BB:23172:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.890103] [ip-0A0C04A8:53275:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.891251] [ip-0A0C04BB:23169:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.891145] [ip-0A0C0493:44601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.892368] [ip-0A0C04BD:78356:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.892457] [ip-0A0C04C2:5742 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.892431] [ip-0A0C04BD:78354:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.892984] [ip-0A0C046C:58507:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.894006] [ip-0A0C04C9:80212:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.893843] [ip-0A0C04B4:57641:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.893155] [ip-0A0C0494:44743:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.894066] [ip-0A0C04DA:74761:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.894297] [ip-0A0C04DA:74755:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.896737] [ip-0A0C0449:60701:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.895551] [ip-0A0C049C:34804:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.898624] [ip-0A0C048F:52589:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.898758] [ip-0A0C0489:63457:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.899704] [ip-0A0C0436:63464:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.900007] [ip-0A0C0436:63465:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.901420] [ip-0A0C04AC:41599:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.901673] [ip-0A0C04C6:76347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.901985] [ip-0A0C04C6:76346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.903163] [ip-0A0C04BD:78357:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.903444] [ip-0A0C0441:56236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.905481] [ip-0A0C04A0:37684:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.903992] [ip-0A0C04B5:48820:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.905066] [ip-0A0C0485:28194:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.904730] [ip-0A0C04B5:48818:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.906038] [ip-0A0C04D9:4290 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.905544] [ip-0A0C0461:26931:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.906585] [ip-0A0C04B1:51271:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.907270] [ip-0A0C04A7:25054:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.907279] [ip-0A0C0485:28196:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.906197] [ip-0A0C04B2:24215:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.909378] [ip-0A0C0499:49450:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.909712] [ip-0A0C0499:49451:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.910334] [ip-0A0C04A7:25048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.910297] [ip-0A0C04C4:80487:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.910607] [ip-0A0C04C4:80485:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.911396] [ip-0A0C04A7:25053:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.911127] [ip-0A0C04C0:80089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.911681] [ip-0A0C0491:45116:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.911693] [ip-0A0C0491:45117:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.910956] [ip-0A0C0488:61784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.912351] [ip-0A0C048A:59847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.912688] [ip-0A0C04A5:50824:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.913666] [ip-0A0C04D4:77400:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.913213] [ip-0A0C0497:95739:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.913921] [ip-0A0C040F:33185:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.914592] [ip-0A0C0492:43205:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.913613] [ip-0A0C0490:49257:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.914587] [ip-0A0C04BE:81210:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.913825] [ip-0A0C0494:44745:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.913405] [ip-0A0C04B2:24221:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.915515] [ip-0A0C046B:56945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.916409] [ip-0A0C048A:59843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.916011] [ip-0A0C0486:63194:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.916057] [ip-0A0C0460:62798:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.917423] [ip-0A0C04BE:81211:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.917739] [ip-0A0C0460:62813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.918518] [ip-0A0C048A:59844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.918453] [ip-0A0C04B4:57644:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.918696] [ip-0A0C0438:63972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.920656] [ip-0A0C0487:63247:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.920216] [ip-0A0C049F:35982:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.920254] [ip-0A0C049F:35977:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.919908] [ip-0A0C049B:36971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.921861] [ip-0A0C0487:63245:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.920348] [ip-0A0C0484:65151:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.922310] [ip-0A0C04C4:80484:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.924375] [ip-0A0C04A2:39571:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.923272] [ip-0A0C046B:56941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.924232] [ip-0A0C04B4:57643:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.924858] [ip-0A0C04B4:57639:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.925299] [ip-0A0C0483:62180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.925318] [ip-0A0C0494:44741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.926266] [ip-0A0C046C:58514:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.926982] [ip-0A0C04A5:50805:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.927591] [ip-0A0C0441:56237:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.927919] [ip-0A0C0443:26596:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.927075] [ip-0A0C049B:36968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.926537] [ip-0A0C049C:34809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.929085] [ip-0A0C0499:49454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.927057] [ip-0A0C049C:34810:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.929491] [ip-0A0C04BE:81206:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.929522] [ip-0A0C0487:63243:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.930212] [ip-0A0C0483:62178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.930816] [ip-0A0C04BE:81205:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.929986] [ip-0A0C04BC:76271:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.931166] [ip-0A0C0460:62799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.931747] [ip-0A0C0499:49448:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.931868] [ip-0A0C049F:35983:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.930719] [ip-0A0C049C:34807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.933379] [ip-0A0C0441:56234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.936028] [ip-0A0C04A2:39566:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.933422] [ip-0A0C04B2:24219:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.934929] [ip-0A0C04B6:17345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.936380] [ip-0A0C04A5:50804:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.936796] [ip-0A0C0472:65207:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.937020] [ip-0A0C04C9:80230:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.937309] [ip-0A0C046C:58509:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.937431] [ip-0A0C046C:58510:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.937892] [ip-0A0C0489:63443:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.938354] [ip-0A0C04A5:50801:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.938602] [ip-0A0C0472:65201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.937866] [ip-0A0C04CF:6592 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.937165] [ip-0A0C049B:36970:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.939587] [ip-0A0C046B:56939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.939641] [ip-0A0C048F:52588:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.940172] [ip-0A0C048D:49052:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.941482] [ip-0A0C04C7:82173:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.941770] [ip-0A0C04C7:82176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.942496] [ip-0A0C0485:28198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.942495] [ip-0A0C04DB:73363:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.942746] [ip-0A0C0443:26593:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.942887] [ip-0A0C0497:95741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.944237] [ip-0A0C04A1:65814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.943510] [ip-0A0C04B6:17348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.943831] [ip-0A0C0489:63447:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.944452] [ip-0A0C0483:62183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.944625] [ip-0A0C0489:63442:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.944751] [ip-0A0C0489:63446:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.945639] [ip-0A0C0492:43209:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.945272] [ip-0A0C04DB:73367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.944989] [ip-0A0C0493:44595:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.945368] [ip-0A0C04DB:73369:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.945853] [ip-0A0C04C0:80084:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.946119] [ip-0A0C0472:65204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.946664] [ip-0A0C04D9:4286 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.947534] [ip-0A0C048F:52590:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.948034] [ip-0A0C040F:33190:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.948482] [ip-0A0C0460:62796:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.948548] [ip-0A0C0449:60705:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.949152] [ip-0A0C040F:33191:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.949964] [ip-0A0C04AB:38502:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.949871] [ip-0A0C042D:34558:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.949910] [ip-0A0C0488:61789:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.951317] [ip-0A0C0449:60700:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.950456] [ip-0A0C0484:65157:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.952253] [ip-0A0C0483:62184:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.952137] [ip-0A0C0460:62803:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.952720] [ip-0A0C04AB:38499:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.952689] [ip-0A0C0493:44604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.953577] [ip-0A0C0497:95743:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.956151] [ip-0A0C0492:43208:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.955743] [ip-0A0C04C0:80085:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.956082] [ip-0A0C04AB:38503:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.956082] [ip-0A0C04AE:53246:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.956349] [ip-0A0C04B1:51275:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.956140] [ip-0A0C048F:52586:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.956813] [ip-0A0C040F:33187:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.957070] [ip-0A0C04C2:5734 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.956933] [ip-0A0C040F:33188:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.957755] [ip-0A0C04C4:80482:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.958031] [ip-0A0C04AE:53250:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.959451] [ip-0A0C04C2:5739 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.960249] [ip-0A0C04C4:80488:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.959528] [ip-0A0C04BC:76272:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.961338] [ip-0A0C04CD:77971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.961425] [ip-0A0C04CD:77976:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.961912] [ip-0A0C046B:56938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.962003] [ip-0A0C046B:56943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.961631] [ip-0A0C048F:52612:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.962413] [ip-0A0C04C4:80486:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.963258] [ip-0A0C04C0:80091:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.963533] [ip-0A0C0449:60702:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.964109] [ip-0A0C0472:65202:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.963552] [ip-0A0C0497:95737:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.964897] [ip-0A0C0472:65208:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.965170] [ip-0A0C0485:28203:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.965105] [ip-0A0C04DB:73368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.964596] [ip-0A0C0494:44746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.965141] [ip-0A0C0494:44742:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.965613] [ip-0A0C04B6:17349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.966095] [ip-0A0C04CD:77972:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.966902] [ip-0A0C04D9:4288 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.967218] [ip-0A0C04CD:77975:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.968525] [ip-0A0C04C2:5733 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.968386] [ip-0A0C04DB:73362:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.970460] [ip-0A0C0492:43206:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.969705] [ip-0A0C04B6:17343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.970867] [ip-0A0C04B9:81352:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.970988] [ip-0A0C04B9:81353:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.971108] [ip-0A0C04CF:6590 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.972317] [ip-0A0C0438:63966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.972723] [ip-0A0C04C9:80214:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.972114] [ip-0A0C044D:60334:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.972675] [ip-0A0C0490:49256:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.973838] [ip-0A0C04C0:80090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.974662] [ip-0A0C042D:34562:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.974676] [ip-0A0C0493:44596:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.974010] [ip-0A0C04B2:24222:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.976813] [ip-0A0C04D9:4284 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.977019] [ip-0A0C04B3:55907:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.976725] [ip-0A0C048F:52585:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.977769] [ip-0A0C0429:53664:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.978288] [ip-0A0C048D:49048:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.978499] [ip-0A0C04D9:4285 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.978547] [ip-0A0C04B3:55909:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.979260] [ip-0A0C0485:28197:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.979823] [ip-0A0C04C0:80088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.980218] [ip-0A0C04C2:5767 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.978459] [ip-0A0C0484:65152:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.980345] [ip-0A0C04AB:38504:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.980973] [ip-0A0C04C9:80223:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.982174] [ip-0A0C04A0:37681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.980335] [ip-0A0C04AF:39747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.981073] [ip-0A0C04AB:38501:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.982932] [ip-0A0C04A2:39569:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.981323] [ip-0A0C0441:56230:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.981216] [ip-0A0C0497:95738:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.982378] [ip-0A0C0485:28199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.981756] [ip-0A0C04B9:81358:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.982998] [ip-0A0C0492:43204:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.982699] [ip-0A0C0441:56235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.983412] [ip-0A0C0438:63973:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.983868] [ip-0A0C0449:60703:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.985696] [ip-0A0C04B6:17350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.985949] [ip-0A0C04B9:81359:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.986913] [ip-0A0C0449:60699:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.988151] [ip-0A0C0438:63974:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.988934] [ip-0A0C04AE:53244:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.989392] [ip-0A0C04D4:77399:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.988977] [ip-0A0C044D:60336:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.989683] [ip-0A0C04B2:24217:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.990581] [ip-0A0C0490:49260:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.992711] [ip-0A0C0429:53663:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.992038] [ip-0A0C04B2:24218:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.995211] [ip-0A0C04A0:37685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.992586] [ip-0A0C0484:65155:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.994347] [ip-0A0C0472:65205:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.994407] [ip-0A0C0472:65206:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.994315] [ip-0A0C0493:44599:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.994459] [ip-0A0C04B9:81355:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.994400] [ip-0A0C0493:44602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.994807] [ip-0A0C0488:61788:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.996288] [ip-0A0C04C9:80210:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.996212] [ip-0A0C048D:49046:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.998143] [ip-0A0C048D:49047:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.998150] [ip-0A0C042D:34564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.998623] [ip-0A0C0441:56232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.999881] [ip-0A0C04D4:77404:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624715.999947] [ip-0A0C048D:49051:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.000492] [ip-0A0C0441:56231:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.001473] [ip-0A0C0486:63201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.003089] [ip-0A0C04BB:23193:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.002970] [ip-0A0C04C7:82174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.002870] [ip-0A0C04AF:39753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.003803] [ip-0A0C04C9:80213:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.003189] [ip-0A0C042D:34559:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.004242] [ip-0A0C04D4:77398:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.003997] [ip-0A0C04D3:79481:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.004238] [ip-0A0C0443:26590:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.004415] [ip-0A0C04D3:79482:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.007308] [ip-0A0C04A2:39564:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.005354] [ip-0A0C0484:65154:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.007137] [ip-0A0C0438:63971:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.007346] [ip-0A0C042D:34561:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.009508] [ip-0A0C04A2:39570:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.007799] [ip-0A0C044D:60335:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.008762] [ip-0A0C044D:60339:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.010136] [ip-0A0C0443:26594:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.012671] [ip-0A0C04A2:39565:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.011097] [ip-0A0C0486:63195:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.012700] [ip-0A0C04D4:77402:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.012771] [ip-0A0C0443:26592:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.013891] [ip-0A0C04BC:76277:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.016384] [ip-0A0C04A0:37709:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.014584] [ip-0A0C04CF:6593 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.016595] [ip-0A0C04BB:23171:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.014939] [ip-0A0C0484:65153:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.016160] [ip-0A0C04CF:6594 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.018293] [ip-0A0C04A0:37680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.019221] [ip-0A0C04A0:37682:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.017189] [ip-0A0C0490:49261:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.017781] [ip-0A0C0429:53660:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.017834] [ip-0A0C0429:53661:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.018507] [ip-0A0C042D:34563:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.020034] [ip-0A0C0443:26595:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.021405] [ip-0A0C04A1:65813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.022594] [ip-0A0C04D3:79478:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.022744] [ip-0A0C04D3:79475:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.024501] [ip-0A0C04D3:79474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.024829] [ip-0A0C04BC:76274:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.025101] [ip-0A0C04BC:76279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.026042] [ip-0A0C04BC:76273:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.026339] [ip-0A0C0490:49263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.027502] [ip-0A0C0490:49258:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.029091] [ip-0A0C0486:63197:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.029834] [ip-0A0C04BB:23170:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.030341] [ip-0A0C0486:63193:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.032640] [ip-0A0C04B1:51273:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.033386] [ip-0A0C04AC:41601:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.033633] [ip-0A0C04AC:41602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.036394] [ip-0A0C0429:53667:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.037647] [ip-0A0C04A1:65810:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.037638] [ip-0A0C0486:63196:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.038896] [ip-0A0C044D:60333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.039236] [ip-0A0C044D:60337:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.041239] [ip-0A0C04B3:55911:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.042776] [ip-0A0C04B3:55908:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.047573] [ip-0A0C04CF:6591 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.049346] [ip-0A0C04BB:23173:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.049437] [ip-0A0C04BB:23175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.048926] [ip-0A0C04CF:6589 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.051392] [ip-0A0C0488:61787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.053953] [ip-0A0C04AE:53245:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.054635] [ip-0A0C04B1:51276:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.054948] [ip-0A0C04B1:51277:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.055492] [ip-0A0C04AE:53248:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.056172] [ip-0A0C04AE:53247:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.061013] [ip-0A0C04B3:55932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.062013] [ip-0A0C04C7:82200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.062104] [ip-0A0C04C7:82177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.064127] [ip-0A0C0488:61782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.065796] [ip-0A0C04AF:39754:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.066871] [ip-0A0C04C7:82172:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.068297] [ip-0A0C04B3:55912:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.068028] [ip-0A0C0488:61783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.068112] [ip-0A0C0488:61790:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.069816] [ip-0A0C04A1:65811:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.069916] [ip-0A0C04AE:53249:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.071799] [ip-0A0C04B1:51274:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.072187] [ip-0A0C04A1:65809:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.072806] [ip-0A0C04C7:82175:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.073257] [ip-0A0C04A1:65807:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.075113] [ip-0A0C04B1:51272:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.089245] [ip-0A0C04AC:41605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.104255] [ip-0A0C04AC:41598:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.104329] [ip-0A0C04AC:41600:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.121908] [ip-0A0C04AC:41604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.123399] [ip-0A0C04AC:41603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.129836] [ip-0A0C04AF:39750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.151420] [ip-0A0C04AF:39749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.153845] [ip-0A0C04AF:39752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.167710] [ip-0A0C04AF:39751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634624716.167837] [ip-0A0C04AF:39748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634624717062, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634624717103, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634624717104, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634624717104, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634624717104, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634624717104, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634624717104, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[06:25:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:27] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:28] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:25:29] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0435:0:2135 - context.c:584] INFO job (ID: 867564279854276751) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:2135 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:2135 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:2152 - context.c:584] INFO job (ID: 867565064611775458) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:2152 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:2152 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:2139 - context.c:584] INFO job (ID: 867564590434086947) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:2139 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:2139 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:2151 - context.c:584] INFO job (ID: 867564489095367837) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:2151 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:2151 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:2134 - context.c:584] INFO job (ID: 867564754262038018) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:2134 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:2134 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:2133 - context.c:584] INFO job (ID: 867564975559449216) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:2133 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:2133 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:2132 - context.c:584] INFO job (ID: 867565011173241414) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:2132 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:2132 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0435:0:2138 - context.c:584] INFO job (ID: 867565040346828915) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0435:0:2138 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0435:0:2138 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810596, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1891573444, "metadata": {"file": "main.py", "lineno": 72}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810597, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810597, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810597, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810597, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810597, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810597, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810597, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810597, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810598, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810598, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624810598, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:50] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:26:52] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634624834606, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634624834623, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624834628, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634624834628, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634624837329, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634624837330, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634624837330, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634624837330, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634624838798, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 2290.372530081242, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624838798, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624838798, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 2290.372530081242, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634624838798, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624838798, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624839488, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4876.372023819822, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624839488, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624839488, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4876.372023819822, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624839488, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624839488, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624840142, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5137.355881588488, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624840143, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624840143, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5137.355881588488, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634624840143, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624840143, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624840791, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5189.936087258661, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624840791, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624840791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5189.936087258661, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634624840792, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624840792, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624841408, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5454.087931923273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624841408, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624841408, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5454.087931923273, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634624841408, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624841409, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624842025, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5451.47391618349, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624842025, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624842026, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5451.47391618349, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634624842026, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624842026, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624842641, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5464.264005682611, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624842641, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624842641, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5464.264005682611, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634624842642, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624842642, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624843252, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5503.089320160881, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624843253, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624843253, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5503.089320160881, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634624843253, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624843253, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624843874, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5408.4535270617225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624843875, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624843875, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5408.4535270617225, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634624843875, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624843875, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624844501, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5369.324802577362, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624844502, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624844502, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5369.324802577362, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634624844502, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624844502, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624845123, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5412.348109615241, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624845123, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624845123, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5412.348109615241, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634624845124, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624845124, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624845744, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5418.780718132965, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624845744, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624845744, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5418.780718132965, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634624845745, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624845745, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624846359, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5474.12725596817, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624846359, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624846359, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5474.12725596817, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634624846359, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624846359, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624846978, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.493041276747, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624846979, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624846979, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.493041276747, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634624846979, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624846979, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624847595, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5456.104481963086, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624847595, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624847595, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5456.104481963086, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634624847596, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624847596, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624848221, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5377.569305617828, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624848221, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624848221, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5377.569305617828, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634624848221, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624848221, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624848837, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5456.265025396099, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624848838, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624848838, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5456.265025396099, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634624848838, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624848838, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624849468, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5338.684359164061, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624849468, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624849468, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5338.684359164061, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634624849468, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624849468, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624850085, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5454.8690361821655, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624850085, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624850085, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5454.8690361821655, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634624850085, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624850085, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624850704, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5434.5845662142365, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624850704, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624850704, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5434.5845662142365, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634624850704, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624850704, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624851325, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5420.00612270956, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624851325, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624851325, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5420.00612270956, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634624851325, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624851325, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624851944, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5430.932084742333, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624851945, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624851945, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5430.932084742333, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634624851945, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624851945, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624852570, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5376.44095413404, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624852570, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624852571, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5376.44095413404, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634624852571, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624852571, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624853189, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5435.697622124081, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624853190, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624853190, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5435.697622124081, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634624853190, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624853190, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624853813, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5398.606086353264, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624853813, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624853813, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5398.606086353264, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634624853813, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624853813, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624854429, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5460.412853580998, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624854429, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624854429, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5460.412853580998, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634624854429, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624854430, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624855050, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5418.5994553279315, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624855050, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624855050, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5418.5994553279315, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634624855050, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624855051, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624855666, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5458.889982511083, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624855667, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624855667, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5458.889982511083, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634624855667, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624855667, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624856286, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5432.724202787204, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624856286, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624856286, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5432.724202787204, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634624856286, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624856286, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624856906, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5421.3175858531185, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624856907, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624856907, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5421.3175858531185, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634624856907, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624856907, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624857521, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5472.137734746455, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624857522, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624857522, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5472.137734746455, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634624857522, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624857522, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624858141, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5432.632055735428, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624858141, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624858141, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5432.632055735428, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634624858141, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624858141, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624858757, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5456.453042216395, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624858758, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624858758, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5456.453042216395, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634624858758, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624858758, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624859374, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5456.772066689976, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624859374, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624859375, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5456.772066689976, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634624859375, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624859375, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624859980, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5549.646627413547, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624859981, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624859981, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5549.646627413547, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634624859981, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624859981, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624860593, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5491.62529235311, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624860593, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624860594, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5491.62529235311, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634624860594, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624860594, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624861210, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5449.937058080124, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624861211, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624861211, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5449.937058080124, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634624861211, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624861211, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624861822, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5499.026624088104, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624861823, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624861823, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5499.026624088104, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634624861823, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624861823, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624862443, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5417.639173149343, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624862444, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624862444, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5417.639173149343, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634624862444, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624862444, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624863058, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5471.366546727398, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624863059, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624863059, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5471.366546727398, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634624863059, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624863059, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624863669, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5509.117101846763, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624863670, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624863670, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5509.117101846763, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634624863670, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624863670, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624864279, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5518.808085507318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624864279, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624864280, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5518.808085507318, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634624864280, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624864280, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624864891, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5503.222554491928, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624864891, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624864891, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5503.222554491928, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634624864891, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624864892, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624865501, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5517.876773124273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624865501, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624865501, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5517.876773124273, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634624865501, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624865501, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624866120, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5434.364524391062, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624866120, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624866120, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5434.364524391062, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634624866121, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624866121, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624866735, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5473.140816796528, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624866735, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624866736, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5473.140816796528, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634624866736, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624866736, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624867349, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5483.283637141801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624867349, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624867349, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5483.283637141801, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634624867349, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624867349, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624867985, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5287.42577533791, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624867985, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624867986, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5287.42577533791, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634624867986, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624867986, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624868594, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5525.539724641687, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624868594, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624868595, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5525.539724641687, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634624868595, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624868595, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624869225, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5335.93529722016, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624869225, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624869225, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5335.93529722016, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634624869295, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624869295, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624869315, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624869744, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8885350823402405, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624869744, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624869897, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5580.482587994538, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624869898, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624869898, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5580.482587994538, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634624870012, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624870013, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624870026, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624870434, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8893991112709045, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624870434, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624870645, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5313.4011681830725, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624870646, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624870646, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5313.4011681830725, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634624870681, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624870681, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624870698, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624871120, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8934081792831421, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624871120, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624871313, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5316.046841121326, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624871314, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624871314, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5316.046841121326, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634624871350, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624871350, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624871366, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624871850, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8895254135131836, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624871850, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624872064, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4709.337311230341, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624872064, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624872064, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4709.337311230341, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634624872100, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624872100, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624872116, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624872545, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9001337289810181, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624872546, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624872738, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5264.940649753786, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624872739, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624872739, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5264.940649753786, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634624872775, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624872775, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624872791, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624873205, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8940987586975098, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624873205, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624873387, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5489.167595948423, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624873388, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624873388, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5489.167595948423, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634624873424, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624873425, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624873440, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624873841, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8995957970619202, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624873841, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624874024, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5603.922283136652, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624874025, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624874025, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5603.922283136652, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634624874060, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624874060, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624874077, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624874474, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.897789478302002, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624874474, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624874666, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5549.843320629175, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624874666, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624874667, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5549.843320629175, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634624874702, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624874702, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624874718, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624875138, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8998246192932129, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624875138, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624875323, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5412.242102764868, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624875324, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624875324, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5412.242102764868, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634624875359, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624875359, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624875374, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624875873, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8633401393890381, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624875873, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624876093, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4581.426345037167, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624876093, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624876093, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4581.426345037167, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634624876135, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624876136, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624876151, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624876560, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8822689652442932, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624876560, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624876747, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5495.985679800016, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624876748, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624876748, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5495.985679800016, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634624876790, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624876790, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624876806, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624877229, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8650476932525635, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624877229, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624877423, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5309.555613743268, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624877424, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624877424, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5309.555613743268, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634624877463, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624877463, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624877477, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624877900, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9011954069137573, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624877900, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624878093, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5340.638722872582, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624878093, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624878093, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5340.638722872582, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634624878129, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624878129, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624878145, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624878556, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8796900510787964, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624878557, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624878748, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5427.422459266973, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624878749, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624878749, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5427.422459266973, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634624878786, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624878786, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624878802, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624879221, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8391713500022888, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624879221, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624879412, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5365.322324155762, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624879413, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624879413, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5365.322324155762, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634624879494, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624879494, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624879509, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624879908, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8699479103088379, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624879908, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624880106, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5498.548170806647, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624880106, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624880106, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5498.548170806647, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634624880148, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624880148, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624880163, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624880606, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8865792751312256, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624880606, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624880801, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5147.844057474656, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624880801, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624880801, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5147.844057474656, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634624880839, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624880839, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624880855, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624881273, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8940876126289368, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624881274, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624881466, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5364.678968789923, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624881466, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624881466, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5364.678968789923, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634624881501, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624881502, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624881517, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624881916, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8978025913238525, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624881916, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624882109, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5530.377323542616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624882110, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624882110, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5530.377323542616, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634624882147, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624882147, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624882162, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624882583, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.886263906955719, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624882583, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624882775, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5349.276472213703, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624882776, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624882776, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5349.276472213703, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634624882811, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624882811, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624882827, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624883250, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8969777822494507, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624883250, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624883437, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5367.208346313831, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624883438, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624883438, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5367.208346313831, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634624883474, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624883475, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624883491, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624883915, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8919404745101929, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624883916, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624884120, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5205.638767300894, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624884121, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624884121, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5205.638767300894, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634624884157, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624884157, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624884174, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624884591, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8958641290664673, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624884591, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624884780, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5391.387864686426, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624884781, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624884781, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5391.387864686426, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634624884818, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624884818, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624884834, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624885244, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8948286175727844, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624885244, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624885442, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5382.52529349622, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624885443, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624885443, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5382.52529349622, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634624885525, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624885525, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624885541, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624885940, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8920742273330688, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624885940, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624886138, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5482.347213031721, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624886139, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624886139, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5482.347213031721, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634624886175, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624886175, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624886191, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624886610, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8991543650627136, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624886610, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624886802, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5366.145635570664, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624886802, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624886802, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5366.145635570664, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634624886843, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624886843, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624886859, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624887278, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.876765787601471, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624887278, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624887475, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5320.811680000604, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624887476, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624887476, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5320.811680000604, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634624887510, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624887511, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624887526, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624887937, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8991326093673706, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624887937, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624888183, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4998.794517127238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624888183, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624888183, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4998.794517127238, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634624888221, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624888222, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624888237, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624888653, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8817493915557861, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624888654, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624888856, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5298.751470584365, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624888856, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624888857, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5298.751470584365, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634624888912, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624888912, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624888928, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624889330, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.883661687374115, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624889331, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624889535, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5396.751669628086, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624889535, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624889536, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5396.751669628086, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634624889571, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624889572, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624889588, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624890002, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8964254856109619, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624890002, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624890196, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5385.474872632057, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624890196, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624890197, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5385.474872632057, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634624890233, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624890233, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624890249, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624890668, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8867486715316772, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624890668, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624890855, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5399.63617902922, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624890856, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624890856, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5399.63617902922, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634624890892, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624890892, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624890907, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624891331, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8681786060333252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624891331, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624891521, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5340.642770658577, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624891522, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624891522, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5340.642770658577, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634624891558, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624891558, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624891575, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624891998, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8969491720199585, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624891998, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624892193, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5294.447684083703, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624892193, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624892194, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5294.447684083703, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634624892229, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624892229, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624892245, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624892656, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8937641382217407, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624892656, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624892844, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5459.059148458971, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624892845, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624892845, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5459.059148458971, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634624892881, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624892881, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624892898, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624893304, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8961058855056763, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624893305, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624893490, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5518.285128950509, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624893491, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624893491, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5518.285128950509, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634624893526, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624893527, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624893543, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624893956, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8956310749053955, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624893956, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624894141, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5467.356900913511, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624894142, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624894142, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5467.356900913511, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634624894178, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624894178, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624894195, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624894611, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8888269662857056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624894611, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624894794, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5457.475742592662, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624894795, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624894795, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5457.475742592662, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634624894830, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624894831, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624894846, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624895269, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9034776091575623, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624895269, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624895463, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5310.447942741707, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624895464, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624895464, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5310.447942741707, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634624895500, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624895500, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624895517, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624895943, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8990538120269775, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624895944, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624896145, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5215.531445045419, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624896145, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624896145, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5215.531445045419, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634624896181, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624896181, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624896197, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624896615, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8556517362594604, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624896615, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624896804, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5398.972158606375, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624896804, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624896804, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5398.972158606375, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634624896840, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624896840, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624896856, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624897255, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8874980211257935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624897255, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624897438, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5616.484897951375, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624897439, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624897439, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5616.484897951375, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634624897475, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624897475, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624897492, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624897891, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8935107588768005, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624897891, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624898071, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5634.16522942668, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624898072, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624898072, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5634.16522942668, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634624898107, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624898107, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624898123, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634624898523, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8947128057479858, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634624898523, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634624898702, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5650.2962054478, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624898703, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624898703, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5650.2962054478, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634624898738, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624898738, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624898755, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634624899175, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8920550346374512, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634624899175, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634624899361, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5397.0410153108505, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624899362, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624899362, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5397.0410153108505, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634624899397, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624899397, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634624899414, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634624899811, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9081028699874878, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634624899811, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634624899812, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634624899994, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5627.009207016322, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634624899995, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634624899995, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5627.009207016322, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1900}}
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:26 AM
RESULT,image_segmentation,,196,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:29 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:30 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:32 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:33 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:34 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:35 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:36 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:37 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:38 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
ENDING TIMING RUN AT 2021-10-19 06:28:39 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:25:10 AM
