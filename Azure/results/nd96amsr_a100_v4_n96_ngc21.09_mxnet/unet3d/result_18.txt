+ : DGXA100_96x8x1
+ : /mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh
+ : 1
++ date +%y%m%d%H%M%S%N
+ : 211019064536361941125
+ : 1
+ : /mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data
+ : /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ : ./api_logs
+ TIME_TAGS=0
+ NVTX_FLAG=0
+ LOGBASE=unet3d_96x8x1_211019064536361941125
+ '[' 0 -gt 0 ']'
+ '[' 0 -gt 0 ']'
+ readonly _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019064536361941125
+ _logfile_base=/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019064536361941125
+ readonly _cont_name=image_segmentation
+ _cont_name=image_segmentation
+ _cont_mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh
+ _cont_mounts+=,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh
+ _cont_mounts+=,/opt/microsoft:/opt/microsoft
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job07378/slurm_script: line 39: [: : integer expression expected
++ srun -N1 -n1 bash
/bin/bash: line 2: /etc/dgx-release: No such file or directory
+ MLPERF_HOST_OS='Ubuntu 18.04.5 LTS / ??? ???'
+ export MLPERF_HOST_OS
+ mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 mkdir -p /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347
+ srun --ntasks=96 --container-image=/mnt/resource_nvme/mlcommons/v1.1/sqsh_files/unet3dv11.sqsh --container-name=image_segmentation true
++ seq 1 1
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee /mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347/unet3d_96x8x1_211019064536361941125_1.log
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=96 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on ip-0A0C040C
Clearing cache on ip-0A0C041F
Clearing cache on ip-0A0C0433
Clearing cache on ip-0A0C040A
Clearing cache on ip-0A0C041A
Clearing cache on ip-0A0C0408
Clearing cache on ip-0A0C0425
Clearing cache on ip-0A0C0414
Clearing cache on ip-0A0C042E
Clearing cache on ip-0A0C0409
Clearing cache on ip-0A0C0411
Clearing cache on ip-0A0C0451
Clearing cache on ip-0A0C0412
Clearing cache on ip-0A0C041E
Clearing cache on ip-0A0C0434
Clearing cache on ip-0A0C0417
Clearing cache on ip-0A0C0421
Clearing cache on ip-0A0C040E
Clearing cache on ip-0A0C0419
Clearing cache on ip-0A0C044B
Clearing cache on ip-0A0C0407
Clearing cache on ip-0A0C0448
Clearing cache on ip-0A0C041B
Clearing cache on ip-0A0C0418
Clearing cache on ip-0A0C041C
Clearing cache on ip-0A0C0416
Clearing cache on ip-0A0C0413
Clearing cache on ip-0A0C041D
Clearing cache on ip-0A0C0445
Clearing cache on ip-0A0C0420
Clearing cache on ip-0A0C0410
Clearing cache on ip-0A0C044F
Clearing cache on ip-0A0C040B
Clearing cache on ip-0A0C042A
Clearing cache on ip-0A0C043F
Clearing cache on ip-0A0C0446
Clearing cache on ip-0A0C042C
Clearing cache on ip-0A0C046D
Clearing cache on ip-0A0C0452
Clearing cache on ip-0A0C0469
Clearing cache on ip-0A0C044E
Clearing cache on ip-0A0C045A
Clearing cache on ip-0A0C047F
Clearing cache on ip-0A0C0454
Clearing cache on ip-0A0C045C
Clearing cache on ip-0A0C0439
Clearing cache on ip-0A0C0424
Clearing cache on ip-0A0C0455
Clearing cache on ip-0A0C044C
Clearing cache on ip-0A0C045D
Clearing cache on ip-0A0C042F
Clearing cache on ip-0A0C0518
Clearing cache on ip-0A0C0426
Clearing cache on ip-0A0C046A
Clearing cache on ip-0A0C0476
Clearing cache on ip-0A0C0462
Clearing cache on ip-0A0C0440
Clearing cache on ip-0A0C0470
Clearing cache on ip-0A0C0474
Clearing cache on ip-0A0C047A
Clearing cache on ip-0A0C0431
Clearing cache on ip-0A0C0430
Clearing cache on ip-0A0C0453
Clearing cache on ip-0A0C043D
Clearing cache on ip-0A0C0422
Clearing cache on ip-0A0C0457
Clearing cache on ip-0A0C0479
Clearing cache on ip-0A0C0432
Clearing cache on ip-0A0C047B
Clearing cache on ip-0A0C0423
Clearing cache on ip-0A0C0428
Clearing cache on ip-0A0C042B
Clearing cache on ip-0A0C0480
Clearing cache on ip-0A0C0481
Clearing cache on ip-0A0C0477
Clearing cache on ip-0A0C0458
Clearing cache on ip-0A0C0450
Clearing cache on ip-0A0C043A
Clearing cache on ip-0A0C0427
Clearing cache on ip-0A0C047C
Clearing cache on ip-0A0C0442
Clearing cache on ip-0A0C045E
Clearing cache on ip-0A0C0471
Clearing cache on ip-0A0C047D
Clearing cache on ip-0A0C0447
Clearing cache on ip-0A0C044A
Clearing cache on ip-0A0C0459
Clearing cache on ip-0A0C043B
Clearing cache on ip-0A0C0456
Clearing cache on ip-0A0C043C
Clearing cache on ip-0A0C0465
Clearing cache on ip-0A0C043E
Clearing cache on ip-0A0C045B
Clearing cache on ip-0A0C0467
Clearing cache on ip-0A0C0463
Clearing cache on ip-0A0C0475
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=768 --ntasks-per-node=8 --container-name=image_segmentation --container-mounts=/mnt/resource_nvme/mlcommons/v1.1/bm_data/unet3d_data:/data,/mnt/resource_nvme/mlcommons/logs/unet3d/96N-m19.051347:/results,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/bind.sh:/bm_utils/bind.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/azure.sh:/bm_utils/azure.sh,/shared/data/mlcommons/training_results_v1.1/Azure-submission/benchmarks/unet3d/implementations/mxnet/run_and_time.sh:/bm_utils/run_and_time.sh,/opt/microsoft:/opt/microsoft /bm_utils/run_and_time.sh
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
running benchmark
STARTING TIMING RUN AT 2021-10-19 06:45:38 AM
running benchmark
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=48-71 --membind=2 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
num_sockets = 2 num_nodes=4 cores_per_socket=48
+ exec numactl --physcpubind=0-23 --membind=0 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=24-47 --membind=1 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
+ exec numactl --physcpubind=72-95 --membind=3 -- python -u main.py --data_dir /data --epochs 10000 --quality_threshold 0.908 --batch_size 1 --evaluate_every 20 --start_eval_at 1000 --lr_warmup_epochs 1500 --optimizer nag --learning_rate 1.5 --static_cast -sls 512 -gpf 512 --loss_scale_inc_cycles 70 --warmup --val_batch_size 1 --nodes_for_eval 12 -sgs 8 -ucl -sts --shard_eval --num_workers 4 --input_batch_multiplier 4
[1634625943.761011] [ip-0A0C040C:7211 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.776992] [ip-0A0C040C:7209 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.796684] [ip-0A0C0410:10938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.804672] [ip-0A0C040E:80265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.849061] [ip-0A0C040E:80264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.850509] [ip-0A0C044E:69115:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.854745] [ip-0A0C040C:7213 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.859535] [ip-0A0C0440:71121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.864039] [ip-0A0C044A:71004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.883413] [ip-0A0C040C:7217 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.883759] [ip-0A0C0410:10940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.885292] [ip-0A0C047F:70023:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.898629] [ip-0A0C0423:85662:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.899949] [ip-0A0C044A:71002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.899131] [ip-0A0C0410:10933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.902099] [ip-0A0C047F:70025:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.905554] [ip-0A0C041A:18167:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.911885] [ip-0A0C0423:85664:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.911357] [ip-0A0C040B:38033:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.912811] [ip-0A0C0440:71119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.916631] [ip-0A0C040E:80266:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.923932] [ip-0A0C041A:18152:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.928053] [ip-0A0C040C:7208 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.928381] [ip-0A0C040C:7214 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.927616] [ip-0A0C040A:93384:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.929869] [ip-0A0C040A:93382:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.930162] [ip-0A0C0462:6189 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.934027] [ip-0A0C040C:7212 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.934942] [ip-0A0C044E:69118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.938542] [ip-0A0C0458:80306:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.938536] [ip-0A0C043F:72875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.938825] [ip-0A0C047D:67005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.944098] [ip-0A0C040C:7210 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.943238] [ip-0A0C0459:72112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.953252] [ip-0A0C0410:10937:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.954977] [ip-0A0C0440:71118:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.955729] [ip-0A0C044A:71006:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.955321] [ip-0A0C040A:93385:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.958052] [ip-0A0C0447:78826:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.960365] [ip-0A0C0433:86310:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.961047] [ip-0A0C040E:80267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.962103] [ip-0A0C040E:80263:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.966636] [ip-0A0C047D:67007:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.967478] [ip-0A0C0421:94106:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.973286] [ip-0A0C045B:80186:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.975103] [ip-0A0C0423:85666:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.976821] [ip-0A0C043C:78261:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.976728] [ip-0A0C0459:72108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.980499] [ip-0A0C0408:19104:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.981662] [ip-0A0C040E:80269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.981331] [ip-0A0C0414:10969:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.982672] [ip-0A0C040E:80268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.984770] [ip-0A0C043F:72873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.986848] [ip-0A0C0455:70814:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.986713] [ip-0A0C0410:10936:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.989512] [ip-0A0C0428:80676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.991260] [ip-0A0C0455:70815:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.990924] [ip-0A0C0442:71619:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.994863] [ip-0A0C0410:10935:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.995808] [ip-0A0C0458:80301:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.998665] [ip-0A0C0475:58535:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625943.999869] [ip-0A0C0411:91946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.000344] [ip-0A0C0447:78828:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.000159] [ip-0A0C0414:10966:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.001524] [ip-0A0C0450:47615:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.001225] [ip-0A0C0445:82629:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.001897] [ip-0A0C043C:78269:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.002405] [ip-0A0C040E:80270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.002640] [ip-0A0C0458:80303:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.001770] [ip-0A0C0413:26741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.003327] [ip-0A0C0480:65941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.004628] [ip-0A0C041B:12371:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.004541] [ip-0A0C0440:71123:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.004323] [ip-0A0C0410:10941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.004315] [ip-0A0C0409:62907:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.004842] [ip-0A0C0417:25332:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.005708] [ip-0A0C0433:86316:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.007060] [ip-0A0C041A:18154:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.005979] [ip-0A0C0471:61743:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.007415] [ip-0A0C044E:69117:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.011750] [ip-0A0C044A:70999:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.012412] [ip-0A0C0410:10934:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.016611] [ip-0A0C0445:82632:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.017791] [ip-0A0C047F:70028:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.019875] [ip-0A0C0470:56882:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.020269] [ip-0A0C0414:10967:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.023055] [ip-0A0C043D:68111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.024182] [ip-0A0C047F:70024:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.024182] [ip-0A0C0421:94128:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.024358] [ip-0A0C0440:71122:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.028213] [ip-0A0C0467:62344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.026900] [ip-0A0C0442:71624:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.030845] [ip-0A0C0423:85663:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.031057] [ip-0A0C041A:18148:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.031627] [ip-0A0C0408:19107:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.031896] [ip-0A0C041B:12373:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.032933] [ip-0A0C042E:91288:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.034867] [ip-0A0C0416:79238:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.037747] [ip-0A0C045B:80161:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.037616] [ip-0A0C0442:71618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.039578] [ip-0A0C0418:88875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.039667] [ip-0A0C040B:38035:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.039953] [ip-0A0C0462:6193 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.040833] [ip-0A0C044E:69120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.042232] [ip-0A0C044A:71003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.042412] [ip-0A0C041E:92110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.043832] [ip-0A0C0480:65937:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.044056] [ip-0A0C0479:67365:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.045080] [ip-0A0C0417:25328:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.045138] [ip-0A0C044E:69121:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.045437] [ip-0A0C0413:26740:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.046913] [ip-0A0C044A:71000:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.046778] [ip-0A0C0413:26737:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.047795] [ip-0A0C0453:77871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.049203] [ip-0A0C0463:66787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.049155] [ip-0A0C0474:57868:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.050118] [ip-0A0C042B:90132:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.049300] [ip-0A0C047D:67001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.050285] [ip-0A0C0459:72106:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.050705] [ip-0A0C0412:71649:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.050778] [ip-0A0C0440:71125:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.054294] [ip-0A0C0470:56878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.054560] [ip-0A0C041B:12369:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.054521] [ip-0A0C0452:75801:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.054534] [ip-0A0C0452:75798:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.055838] [ip-0A0C041A:18155:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.057838] [ip-0A0C0465:64316:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.058591] [ip-0A0C043D:68112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.059415] [ip-0A0C0426:90776:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.059689] [ip-0A0C0407:96679:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.061580] [ip-0A0C045B:80159:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.062092] [ip-0A0C0440:71124:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.062219] [ip-0A0C0440:71120:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.061848] [ip-0A0C044E:69114:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.063813] [ip-0A0C0471:61742:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.063773] [ip-0A0C044E:69119:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.065413] [ip-0A0C042A:82367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.064944] [ip-0A0C0462:6190 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.067404] [ip-0A0C0421:94107:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.066946] [ip-0A0C043A:77452:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.068425] [ip-0A0C047B:73199:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.069070] [ip-0A0C047F:70026:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.069431] [ip-0A0C0447:78829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.070557] [ip-0A0C0420:93307:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.070247] [ip-0A0C040B:38061:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.071380] [ip-0A0C0467:62346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.072115] [ip-0A0C0458:80304:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.071606] [ip-0A0C0422:27665:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.071386] [ip-0A0C044E:69116:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.073991] [ip-0A0C0480:65942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.073950] [ip-0A0C047D:67006:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.076058] [ip-0A0C044A:71005:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.076427] [ip-0A0C0409:62910:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.076594] [ip-0A0C0409:62905:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.076766] [ip-0A0C0462:6194 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.078449] [ip-0A0C0479:67367:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.078848] [ip-0A0C044A:71001:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.078965] [ip-0A0C0428:80682:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.079802] [ip-0A0C0424:80539:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.079705] [ip-0A0C0445:82628:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.081353] [ip-0A0C041F:96010:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.080183] [ip-0A0C0477:71526:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.081064] [ip-0A0C0462:6192 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.082825] [ip-0A0C045B:80160:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.082870] [ip-0A0C0463:66784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.083300] [ip-0A0C0423:85668:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.083956] [ip-0A0C0481:64842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.084062] [ip-0A0C044F:69763:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.084514] [ip-0A0C0470:56879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.084965] [ip-0A0C0428:80677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.084468] [ip-0A0C0433:86311:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.084690] [ip-0A0C0476:52183:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.084147] [ip-0A0C0434:80746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.084619] [ip-0A0C040B:38038:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.086300] [ip-0A0C0408:19109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.086407] [ip-0A0C0420:93305:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.086799] [ip-0A0C047F:70022:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.086774] [ip-0A0C041D:88628:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.085921] [ip-0A0C040B:38037:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.085895] [ip-0A0C0469:61494:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.087471] [ip-0A0C040A:93380:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.087417] [ip-0A0C043A:77453:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.088784] [ip-0A0C0431:82350:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.088916] [ip-0A0C043C:78270:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.090187] [ip-0A0C0416:79239:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.090618] [ip-0A0C045A:76733:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.090660] [ip-0A0C045A:76730:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.090560] [ip-0A0C0453:77873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.090948] [ip-0A0C047B:73200:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.091940] [ip-0A0C042C:85396:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.091865] [ip-0A0C043F:72880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.092736] [ip-0A0C0479:67370:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.093712] [ip-0A0C0423:85661:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.094721] [ip-0A0C0448:84381:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.095765] [ip-0A0C047F:70027:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.095467] [ip-0A0C043F:72878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.096387] [ip-0A0C0423:85665:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.095984] [ip-0A0C040A:93379:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.098250] [ip-0A0C047F:70021:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.098392] [ip-0A0C0454:76749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.099349] [ip-0A0C0425:95069:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.099674] [ip-0A0C0421:94108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.099881] [ip-0A0C0421:94105:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.099670] [ip-0A0C0459:72105:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.100569] [ip-0A0C0423:85667:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.102714] [ip-0A0C041E:92105:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.102524] [ip-0A0C0475:58532:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.102770] [ip-0A0C0417:25329:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.103847] [ip-0A0C041E:92107:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.103249] [ip-0A0C0475:58537:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.103857] [ip-0A0C043F:72874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.103467] [ip-0A0C0418:88878:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.105866] [ip-0A0C041A:18151:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.106206] [ip-0A0C041C:98178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.106870] [ip-0A0C041A:18149:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.107814] [ip-0A0C046D:60713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.109426] [ip-0A0C041D:88617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.108714] [ip-0A0C0462:6196 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.109236] [ip-0A0C040B:38040:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.110726] [ip-0A0C0465:64318:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.111338] [ip-0A0C0407:96678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.113244] [ip-0A0C047A:71602:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.113263] [ip-0A0C0518:90932:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.113532] [ip-0A0C0424:80541:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.112830] [ip-0A0C040B:38034:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.114604] [ip-0A0C0450:47612:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.113772] [ip-0A0C040B:38036:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.114965] [ip-0A0C0447:78825:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.114710] [ip-0A0C040A:93381:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.115618] [ip-0A0C0428:80681:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.115881] [ip-0A0C041A:18150:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.116347] [ip-0A0C0481:64839:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.116143] [ip-0A0C0476:52179:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.116886] [ip-0A0C045C:72831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.116546] [ip-0A0C0433:86313:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.117433] [ip-0A0C045E:73557:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.118262] [ip-0A0C040A:93383:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.119390] [ip-0A0C0414:10964:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.120104] [ip-0A0C0433:86309:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.121617] [ip-0A0C040A:93378:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.122921] [ip-0A0C0463:66786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.122952] [ip-0A0C0425:95067:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.124127] [ip-0A0C042B:90139:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.123281] [ip-0A0C042E:91283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.124696] [ip-0A0C0456:79845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.124357] [ip-0A0C0409:62911:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.125422] [ip-0A0C0452:75800:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.126804] [ip-0A0C045A:76732:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.128142] [ip-0A0C0455:70820:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.127447] [ip-0A0C0471:61748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.128481] [ip-0A0C0411:91940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.129476] [ip-0A0C0411:91942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.129002] [ip-0A0C0477:71523:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.128895] [ip-0A0C0442:71617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.130509] [ip-0A0C0411:91943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.130664] [ip-0A0C044C:73946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.130180] [ip-0A0C0413:26738:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.130261] [ip-0A0C0469:61496:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.131008] [ip-0A0C0459:72110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.131344] [ip-0A0C0459:72109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.131771] [ip-0A0C0447:78831:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.132649] [ip-0A0C0459:72111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.134094] [ip-0A0C042B:90135:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.133601] [ip-0A0C0471:61746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.134410] [ip-0A0C043F:72877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.134883] [ip-0A0C047D:67008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.137038] [ip-0A0C0480:65939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.137226] [ip-0A0C047C:76697:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.137278] [ip-0A0C047C:76696:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.137909] [ip-0A0C0450:47613:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.137822] [ip-0A0C0458:80299:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.136720] [ip-0A0C046A:67383:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.138568] [ip-0A0C0417:25331:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.139609] [ip-0A0C0458:80302:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.140336] [ip-0A0C0454:76746:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.140550] [ip-0A0C0457:72095:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.140536] [ip-0A0C0457:72099:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.140954] [ip-0A0C0518:90935:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.140054] [ip-0A0C047D:67003:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.141764] [ip-0A0C0450:47614:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.141970] [ip-0A0C0432:23308:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.141110] [ip-0A0C0434:80747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.141453] [ip-0A0C0462:6191 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.142895] [ip-0A0C0445:82634:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.143310] [ip-0A0C0453:77869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.143480] [ip-0A0C0451:74784:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.142852] [ip-0A0C0462:6195 :0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.144128] [ip-0A0C043C:78268:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.144691] [ip-0A0C0432:23283:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.145989] [ip-0A0C0407:96677:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.145959] [ip-0A0C0474:57864:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.146264] [ip-0A0C0459:72107:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.146835] [ip-0A0C0422:27680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.148098] [ip-0A0C045B:80163:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.147705] [ip-0A0C042F:87276:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.150059] [ip-0A0C042A:82370:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.149916] [ip-0A0C045E:73554:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.150430] [ip-0A0C043C:78264:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.149980] [ip-0A0C043F:72879:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.150163] [ip-0A0C043F:72876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.151192] [ip-0A0C046D:60714:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.151577] [ip-0A0C0431:82348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.151485] [ip-0A0C0458:80300:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.151953] [ip-0A0C0455:70821:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.152319] [ip-0A0C043D:68113:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.153005] [ip-0A0C043D:68108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.154708] [ip-0A0C044B:75337:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.154151] [ip-0A0C0427:89011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.155990] [ip-0A0C0458:80298:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.154872] [ip-0A0C047D:67002:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.155007] [ip-0A0C047D:67004:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.155851] [ip-0A0C0409:62904:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.157093] [ip-0A0C0426:90775:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.156122] [ip-0A0C042E:91282:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.157980] [ip-0A0C0426:90780:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.157085] [ip-0A0C0414:10965:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.158064] [ip-0A0C0408:19105:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.158365] [ip-0A0C0411:91941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.161228] [ip-0A0C043C:78265:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.161496] [ip-0A0C0421:94103:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.162133] [ip-0A0C042A:82368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.160313] [ip-0A0C0442:71622:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.161932] [ip-0A0C043B:77665:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.161644] [ip-0A0C0414:10963:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.162071] [ip-0A0C0421:94102:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.162957] [ip-0A0C0453:77870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.163482] [ip-0A0C0465:64320:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.164016] [ip-0A0C0448:84382:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.164350] [ip-0A0C045B:80158:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.163855] [ip-0A0C041C:98170:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.163498] [ip-0A0C0417:25335:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.163910] [ip-0A0C0451:74780:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.164224] [ip-0A0C043B:77658:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.164208] [ip-0A0C042E:91278:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.166581] [ip-0A0C047A:71606:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.166034] [ip-0A0C0421:94104:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.166112] [ip-0A0C0433:86315:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.166539] [ip-0A0C0447:78832:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.166543] [ip-0A0C0467:62343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.167820] [ip-0A0C045B:80162:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.167874] [ip-0A0C0445:82631:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.167947] [ip-0A0C0447:78827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.168090] [ip-0A0C0447:78824:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.168195] [ip-0A0C045D:63478:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.168059] [ip-0A0C0433:86312:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.169259] [ip-0A0C045E:73556:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.169826] [ip-0A0C0452:75799:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.170613] [ip-0A0C041D:88615:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.170848] [ip-0A0C0448:84393:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.171195] [ip-0A0C045B:80157:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.171141] [ip-0A0C0433:86314:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.173286] [ip-0A0C0409:62906:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.174420] [ip-0A0C043C:78262:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.174188] [ip-0A0C0475:58539:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.175031] [ip-0A0C0426:90781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.175412] [ip-0A0C043C:78267:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.175781] [ip-0A0C041F:96008:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.178743] [ip-0A0C0412:71651:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.178778] [ip-0A0C0412:71654:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.179544] [ip-0A0C041B:12370:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.178470] [ip-0A0C0477:71527:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.180105] [ip-0A0C0455:70819:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.180531] [ip-0A0C045D:63473:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.181594] [ip-0A0C0416:79232:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.180951] [ip-0A0C0414:10962:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.181994] [ip-0A0C046D:60715:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.181480] [ip-0A0C0427:89012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.182127] [ip-0A0C0413:26739:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.183086] [ip-0A0C0420:93333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.183134] [ip-0A0C0412:71650:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.183609] [ip-0A0C0450:47616:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.183920] [ip-0A0C0450:47611:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.184422] [ip-0A0C0463:66789:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.184534] [ip-0A0C045C:72827:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.183433] [ip-0A0C0413:26742:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.184702] [ip-0A0C0428:80683:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.184913] [ip-0A0C045C:72824:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.185069] [ip-0A0C0428:80679:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.185760] [ip-0A0C0408:19108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.184913] [ip-0A0C0442:71620:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.186031] [ip-0A0C0418:88877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.187321] [ip-0A0C0463:66788:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.186358] [ip-0A0C0414:10968:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.187583] [ip-0A0C0416:79237:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.187446] [ip-0A0C042B:90131:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.188112] [ip-0A0C0480:65940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.188373] [ip-0A0C045A:76728:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.188675] [ip-0A0C041F:96015:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.187677] [ip-0A0C0409:62909:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.187027] [ip-0A0C0442:71623:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.189040] [ip-0A0C044F:69757:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.190639] [ip-0A0C0455:70813:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.190059] [ip-0A0C0409:62908:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.191829] [ip-0A0C0456:79844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.192050] [ip-0A0C041B:12375:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.192176] [ip-0A0C0475:58534:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.192849] [ip-0A0C044F:69764:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.192368] [ip-0A0C0475:58533:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.193600] [ip-0A0C045A:76731:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.193112] [ip-0A0C0474:57867:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.193480] [ip-0A0C047B:73201:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.192840] [ip-0A0C0469:61492:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.194952] [ip-0A0C0411:91945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.195082] [ip-0A0C047B:73196:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.195385] [ip-0A0C0419:96685:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.196710] [ip-0A0C0454:76748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.196580] [ip-0A0C0417:25333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.198494] [ip-0A0C0408:19103:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.198157] [ip-0A0C0445:82635:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.198825] [ip-0A0C0471:61745:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.199096] [ip-0A0C0417:25330:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.198907] [ip-0A0C0477:71529:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.200615] [ip-0A0C0479:67372:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.200904] [ip-0A0C041E:92108:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.201391] [ip-0A0C041B:12372:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.201148] [ip-0A0C0455:70817:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.200717] [ip-0A0C0417:25334:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.201950] [ip-0A0C041B:12376:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.201937] [ip-0A0C0408:19102:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.201379] [ip-0A0C0442:71621:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.203680] [ip-0A0C0452:75802:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.204265] [ip-0A0C0450:47610:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.204023] [ip-0A0C0455:70816:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.203187] [ip-0A0C0427:89018:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.204321] [ip-0A0C0445:82633:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.205443] [ip-0A0C0428:80678:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.205383] [ip-0A0C0408:19106:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.205588] [ip-0A0C0428:80680:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.205232] [ip-0A0C0467:62347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.205033] [ip-0A0C0413:26743:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.206200] [ip-0A0C0450:47617:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.206651] [ip-0A0C0470:56877:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.206730] [ip-0A0C042C:85395:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.205842] [ip-0A0C0471:61744:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.207378] [ip-0A0C041F:96013:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.207266] [ip-0A0C0470:56906:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.207591] [ip-0A0C0467:62348:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.207832] [ip-0A0C0445:82630:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.209015] [ip-0A0C0454:76747:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.208160] [ip-0A0C0422:27662:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.209517] [ip-0A0C041D:88616:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.209823] [ip-0A0C0431:82344:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.209414] [ip-0A0C042F:87275:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.210297] [ip-0A0C042F:87280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.211415] [ip-0A0C0416:79236:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.211060] [ip-0A0C0453:77875:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.210570] [ip-0A0C042E:91281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.210865] [ip-0A0C0413:26736:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.212129] [ip-0A0C0425:95064:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.212121] [ip-0A0C044C:73947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.211496] [ip-0A0C043A:77454:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.212780] [ip-0A0C0465:64315:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.214537] [ip-0A0C0424:80538:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.214082] [ip-0A0C0475:58545:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.214432] [ip-0A0C0474:57871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.215072] [ip-0A0C0471:61741:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.215228] [ip-0A0C0434:80748:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.216488] [ip-0A0C0411:91939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.216701] [ip-0A0C0411:91944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.218668] [ip-0A0C0471:61767:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.219320] [ip-0A0C0475:58536:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.218967] [ip-0A0C0418:88871:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.219031] [ip-0A0C042E:91276:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.220767] [ip-0A0C0430:30944:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.220807] [ip-0A0C0476:52181:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.221613] [ip-0A0C0430:30940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.223166] [ip-0A0C0439:69104:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.223788] [ip-0A0C041B:12374:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.224211] [ip-0A0C0432:23280:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.224636] [ip-0A0C0480:65935:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.224724] [ip-0A0C0480:65936:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.224739] [ip-0A0C0470:56880:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.223940] [ip-0A0C0434:80751:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.225148] [ip-0A0C0480:65938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.225115] [ip-0A0C042B:90134:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.226408] [ip-0A0C0419:96686:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.225902] [ip-0A0C0469:61489:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.227366] [ip-0A0C0454:76743:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.227567] [ip-0A0C042C:85398:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.227503] [ip-0A0C043D:68114:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.226994] [ip-0A0C043A:77458:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.228670] [ip-0A0C045A:76727:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.229753] [ip-0A0C0474:57866:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.230427] [ip-0A0C0479:67368:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.230084] [ip-0A0C042E:91277:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.230626] [ip-0A0C042E:91279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.231249] [ip-0A0C0418:88874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.232558] [ip-0A0C042B:90133:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.233718] [ip-0A0C0479:67366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.233555] [ip-0A0C0474:57869:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.233928] [ip-0A0C0467:62342:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.234157] [ip-0A0C041C:98172:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.235008] [ip-0A0C0416:79233:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.235337] [ip-0A0C0407:96676:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.235868] [ip-0A0C0470:56881:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.235927] [ip-0A0C0470:56883:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.236325] [ip-0A0C0422:27661:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.236980] [ip-0A0C0467:62345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.237424] [ip-0A0C0456:79846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.237574] [ip-0A0C0481:64840:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.237561] [ip-0A0C043D:68107:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.238160] [ip-0A0C0416:79234:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.238792] [ip-0A0C045A:76726:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.238360] [ip-0A0C041C:98176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.238788] [ip-0A0C043A:77455:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.239963] [ip-0A0C0416:79235:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.240759] [ip-0A0C044F:69758:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.240996] [ip-0A0C042B:90136:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.241587] [ip-0A0C0439:69092:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.241456] [ip-0A0C0477:71522:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.243335] [ip-0A0C0481:64843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.243476] [ip-0A0C0424:80542:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.243026] [ip-0A0C043D:68109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.243081] [ip-0A0C043D:68110:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.244046] [ip-0A0C042A:82366:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.244444] [ip-0A0C042A:82364:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.244523] [ip-0A0C0431:82343:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.245048] [ip-0A0C044B:75335:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.245270] [ip-0A0C0479:67371:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.245324] [ip-0A0C042C:85400:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.246080] [ip-0A0C0446:80034:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.246222] [ip-0A0C0463:66783:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.246551] [ip-0A0C0448:84384:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.246824] [ip-0A0C0476:52180:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.247708] [ip-0A0C0463:66782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.248019] [ip-0A0C0463:66794:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.248052] [ip-0A0C0474:57865:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.248255] [ip-0A0C0452:75796:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.247289] [ip-0A0C046A:67380:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.247372] [ip-0A0C046A:67378:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.247811] [ip-0A0C0469:61493:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.249426] [ip-0A0C041E:92109:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.249477] [ip-0A0C0425:95068:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.249725] [ip-0A0C0465:64321:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.250122] [ip-0A0C0431:82345:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.250107] [ip-0A0C0467:62341:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.250104] [ip-0A0C0453:77872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.251282] [ip-0A0C047A:71608:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.250945] [ip-0A0C0420:93304:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.251843] [ip-0A0C041E:92106:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.252254] [ip-0A0C0479:67369:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.251621] [ip-0A0C0453:77876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.252501] [ip-0A0C047A:71609:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.252463] [ip-0A0C045A:76729:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.252110] [ip-0A0C042B:90138:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.252922] [ip-0A0C043E:65855:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.253243] [ip-0A0C0420:93306:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.253402] [ip-0A0C0412:71653:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.254154] [ip-0A0C0448:84385:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.255345] [ip-0A0C047B:73195:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.255608] [ip-0A0C045D:63477:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.255783] [ip-0A0C0453:77874:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.257015] [ip-0A0C045E:73559:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.257264] [ip-0A0C041F:96009:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.256955] [ip-0A0C0452:75797:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.257065] [ip-0A0C0451:74786:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.258266] [ip-0A0C0452:75795:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.262473] [ip-0A0C041E:92112:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.263434] [ip-0A0C041E:92111:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.262707] [ip-0A0C0474:57870:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.262911] [ip-0A0C0434:80750:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.263091] [ip-0A0C0418:88873:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.265146] [ip-0A0C041F:96011:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.264849] [ip-0A0C0418:88876:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.265408] [ip-0A0C042F:87274:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.266815] [ip-0A0C0412:71647:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.266244] [ip-0A0C043A:77460:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.267272] [ip-0A0C0518:90933:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.267906] [ip-0A0C0424:80540:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.267823] [ip-0A0C0419:96694:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.266975] [ip-0A0C043A:77457:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.267091] [ip-0A0C043A:77456:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.267598] [ip-0A0C0418:88872:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.268749] [ip-0A0C0457:72096:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.267986] [ip-0A0C0422:27663:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.269531] [ip-0A0C0424:80536:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.269608] [ip-0A0C042A:82372:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.268494] [ip-0A0C0477:71524:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.269711] [ip-0A0C042A:82373:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.270062] [ip-0A0C044F:69762:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.270300] [ip-0A0C046D:60716:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.270426] [ip-0A0C045C:72828:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.270528] [ip-0A0C0426:90777:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.270725] [ip-0A0C0481:64841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.271075] [ip-0A0C042C:85401:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.271714] [ip-0A0C041F:96014:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.272026] [ip-0A0C041D:88620:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.272511] [ip-0A0C045D:63474:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.273017] [ip-0A0C0456:79848:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.273849] [ip-0A0C047C:76694:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.274209] [ip-0A0C0518:90939:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.274417] [ip-0A0C0407:96672:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.273261] [ip-0A0C0434:80752:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.274591] [ip-0A0C0425:95063:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.275038] [ip-0A0C0426:90782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.274349] [ip-0A0C0434:80753:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.275634] [ip-0A0C0407:96674:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.275744] [ip-0A0C0407:96675:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.276282] [ip-0A0C0465:64317:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.276493] [ip-0A0C0465:64319:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.276603] [ip-0A0C042A:82369:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.276315] [ip-0A0C047B:73197:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.276568] [ip-0A0C0465:64322:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.276481] [ip-0A0C0412:71648:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.276806] [ip-0A0C041C:98171:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.277006] [ip-0A0C0412:71646:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.277719] [ip-0A0C0420:93309:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.277015] [ip-0A0C0434:80749:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.278357] [ip-0A0C0431:82346:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.279315] [ip-0A0C0407:96673:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.279941] [ip-0A0C0446:80030:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.279440] [ip-0A0C047B:73194:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.280053] [ip-0A0C047B:73198:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.280607] [ip-0A0C044C:73948:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.280816] [ip-0A0C0425:95065:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.280621] [ip-0A0C0420:93308:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.281614] [ip-0A0C044B:75333:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.283286] [ip-0A0C043B:77664:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.283350] [ip-0A0C043B:77661:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.283070] [ip-0A0C0427:89017:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.283772] [ip-0A0C0476:52182:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.285226] [ip-0A0C047C:76692:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.285279] [ip-0A0C0420:93310:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.285888] [ip-0A0C0518:90940:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.286490] [ip-0A0C0424:80535:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.287096] [ip-0A0C0448:84394:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.286081] [ip-0A0C0477:71525:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.286797] [ip-0A0C0477:71528:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.288280] [ip-0A0C044F:69759:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.289524] [ip-0A0C0426:90779:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.288134] [ip-0A0C0469:61491:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.289625] [ip-0A0C0426:90778:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.289338] [ip-0A0C0476:52177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.289386] [ip-0A0C0476:52176:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.288693] [ip-0A0C0469:61495:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.290014] [ip-0A0C0481:64845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.290736] [ip-0A0C041D:88614:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.290872] [ip-0A0C041F:96012:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.289769] [ip-0A0C0469:61490:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.291019] [ip-0A0C0457:72100:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.291494] [ip-0A0C0431:82347:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.291672] [ip-0A0C042F:87278:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.292721] [ip-0A0C0424:80537:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.293099] [ip-0A0C0481:64844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.293682] [ip-0A0C046D:60720:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.294756] [ip-0A0C044F:69760:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.295522] [ip-0A0C0454:76745:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.295671] [ip-0A0C041C:98173:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.296288] [ip-0A0C045E:73555:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.296423] [ip-0A0C041D:88619:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.296664] [ip-0A0C046D:60719:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.297870] [ip-0A0C0422:27660:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.300209] [ip-0A0C0481:64846:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.300408] [ip-0A0C0476:52178:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.301048] [ip-0A0C041C:98174:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.301160] [ip-0A0C0422:27666:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.301671] [ip-0A0C041C:98177:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.302102] [ip-0A0C041D:88618:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.302208] [ip-0A0C0448:84383:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.302467] [ip-0A0C0518:90938:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.302666] [ip-0A0C0454:76744:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.302765] [ip-0A0C0454:76742:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.302197] [ip-0A0C046A:67390:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.303616] [ip-0A0C045E:73553:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.303585] [ip-0A0C0431:82349:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.303715] [ip-0A0C046D:60717:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.305036] [ip-0A0C0456:79850:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.305121] [ip-0A0C046D:60718:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.305114] [ip-0A0C042C:85402:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.305290] [ip-0A0C042C:85399:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.305417] [ip-0A0C047C:76699:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.305634] [ip-0A0C0457:72098:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.306168] [ip-0A0C047A:71603:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.306222] [ip-0A0C0432:23281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.306729] [ip-0A0C044F:69761:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.307232] [ip-0A0C0456:79847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.307548] [ip-0A0C045C:72829:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.307541] [ip-0A0C044C:73949:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.306917] [ip-0A0C0422:27667:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.307946] [ip-0A0C0456:79849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.306849] [ip-0A0C046A:67381:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.308617] [ip-0A0C042C:85397:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.308756] [ip-0A0C0518:90937:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.308947] [ip-0A0C0425:95066:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.309096] [ip-0A0C0448:84388:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.314929] [ip-0A0C044B:75334:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.314916] [ip-0A0C0425:95072:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.318064] [ip-0A0C047A:71607:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.318917] [ip-0A0C0432:23279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.318963] [ip-0A0C0518:90941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.321537] [ip-0A0C044C:73953:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.322306] [ip-0A0C047C:76693:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.322396] [ip-0A0C0451:74785:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.324791] [ip-0A0C0456:79862:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.326067] [ip-0A0C047A:71604:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.324637] [ip-0A0C046A:67379:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.326233] [ip-0A0C0419:96688:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.328882] [ip-0A0C047A:71605:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.328864] [ip-0A0C043E:65842:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.329571] [ip-0A0C042F:87279:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.332044] [ip-0A0C0446:80036:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.333592] [ip-0A0C045C:72826:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.333144] [ip-0A0C042F:87277:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.334268] [ip-0A0C0457:72097:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.335497] [ip-0A0C044C:73957:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.335898] [ip-0A0C0451:74787:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.336370] [ip-0A0C047C:76695:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.336419] [ip-0A0C047C:76698:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.336324] [ip-0A0C042F:87281:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.338491] [ip-0A0C045C:72830:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.338097] [ip-0A0C043B:77663:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.339547] [ip-0A0C0457:72093:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.339964] [ip-0A0C044C:73952:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.340152] [ip-0A0C043B:77659:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.340956] [ip-0A0C0451:74782:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.341380] [ip-0A0C0451:74788:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.342943] [ip-0A0C045C:72823:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.345330] [ip-0A0C045E:73552:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.345693] [ip-0A0C0432:23284:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.346059] [ip-0A0C0432:23278:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.346639] [ip-0A0C0432:23282:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.349092] [ip-0A0C045E:73558:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.348242] [ip-0A0C0427:89013:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.349393] [ip-0A0C044C:73950:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.348690] [ip-0A0C0427:89014:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.349958] [ip-0A0C0457:72094:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.349888] [ip-0A0C045D:63475:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.349647] [ip-0A0C046A:67385:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.351469] [ip-0A0C044B:75338:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.350754] [ip-0A0C046A:67382:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.352761] [ip-0A0C044B:75340:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.352193] [ip-0A0C0427:89016:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.353466] [ip-0A0C0427:89015:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.354787] [ip-0A0C0439:69089:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.356674] [ip-0A0C045D:63472:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.358484] [ip-0A0C043B:77662:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.359861] [ip-0A0C0430:30942:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.359949] [ip-0A0C043B:77660:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.360498] [ip-0A0C0430:30945:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.363792] [ip-0A0C0439:69088:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.364314] [ip-0A0C0451:74781:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.365417] [ip-0A0C045D:63476:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.368658] [ip-0A0C045D:63479:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.368983] [ip-0A0C043E:65845:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.370063] [ip-0A0C044B:75336:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.372017] [ip-0A0C044B:75339:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.375359] [ip-0A0C0446:80035:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.381541] [ip-0A0C043E:65844:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.389145] [ip-0A0C0419:96713:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.389589] [ip-0A0C0419:96690:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.397641] [ip-0A0C0439:69090:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.402713] [ip-0A0C0430:30943:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.406293] [ip-0A0C0419:96689:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.408663] [ip-0A0C0419:96691:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.410108] [ip-0A0C0439:69087:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.410556] [ip-0A0C0430:30947:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.424134] [ip-0A0C0430:30941:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.424907] [ip-0A0C0430:30946:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.429418] [ip-0A0C043E:65841:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.430392] [ip-0A0C0446:80033:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.432337] [ip-0A0C0446:80031:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.433083] [ip-0A0C0439:69093:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.433272] [ip-0A0C0439:69086:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.441247] [ip-0A0C043E:65849:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.443400] [ip-0A0C0446:80032:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.449286] [ip-0A0C0446:80038:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.453065] [ip-0A0C043E:65847:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
[1634625944.457359] [ip-0A0C043E:65843:0]          parser.c:1885 UCX  WARN  unused env variable: UCX_IB_ENABLE_CUDA_AFFINITY (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
:::MLLOG {"namespace": "", "time_ms": 1634625945359, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "main.py", "lineno": 46}}
:::MLLOG {"namespace": "", "time_ms": 1634625945399, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 47}}
:::MLLOG {"namespace": "", "time_ms": 1634625945400, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "unet3d", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 115}}
:::MLLOG {"namespace": "", "time_ms": 1634625945400, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Azure", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634625945400, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 124}}
:::MLLOG {"namespace": "", "time_ms": 1634625945400, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "cloud", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 128}}
:::MLLOG {"namespace": "", "time_ms": 1634625945400, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "96xND96amsr_A100_v4", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 132}}
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:56] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[06:45:57] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for GPU
[ip-0A0C0409:0:62906 - context.c:584] INFO job (ID: 867537852388356119) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:62906 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:62906 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:62905 - context.c:584] INFO job (ID: 867538033685343280) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:62905 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:62905 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:62909 - context.c:584] INFO job (ID: 867538504206960471) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:62909 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:62909 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:62908 - context.c:584] INFO job (ID: 867537899108062552) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:62908 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:62908 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:62911 - context.c:584] INFO job (ID: 867538504904687155) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:62911 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:62911 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:62910 - context.c:584] INFO job (ID: 867538336335484870) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:62910 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:62910 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:62904 - context.c:584] INFO job (ID: 867538621808416267) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:62904 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:62904 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[ip-0A0C0409:0:62907 - context.c:584] INFO job (ID: 867538721247602854) resource request quota: ( osts:0 user_data_per_ost:0 max_groups:0 max_qps:1 max_group_channels:1, num_trees:1)
[ip-0A0C0409:0:62907 unique id 0] ERROR No Aggregation Manager (sharp_am) detected in sharp_create_job.

[ip-0A0C0409:0:62907 - context.c:601] ERROR sharp_create_job failed: No Aggregation Manager (sharp_am) detected(-52)
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037537, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2185941952, "metadata": {"file": "main.py", "lineno": 72}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037538, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "nag", "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 138}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037538, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 139}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037538, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1500, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 140}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037538, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_boundary_epochs", "value": [], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 142}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037538, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 1.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 143}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037538, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0.0, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 144}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037539, "event_type": "POINT_IN_TIME", "key": "opt_momentum", "value": 0.9, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 145}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037539, "event_type": "POINT_IN_TIME", "key": "oversampling", "value": 0.4, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 146}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037539, "event_type": "POINT_IN_TIME", "key": "training_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 147}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037539, "event_type": "POINT_IN_TIME", "key": "validation_input_shape", "value": [128, 128, 128], "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 148}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626037539, "event_type": "POINT_IN_TIME", "key": "validation_overlap", "value": 0.5, "metadata": {"file": "/workspace/unet3d/mlperf_logger.py", "lineno": 149}}
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:17] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
[06:47:19] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
:::MLLOG {"namespace": "", "time_ms": 1634626061572, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1634626061588, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "main.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626061593, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 168, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 94}}
:::MLLOG {"namespace": "", "time_ms": 1634626061593, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 42, "metadata": {"file": "/workspace/unet3d/data_loading/data_loader.py", "lineno": 95}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634626064116, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 84, "metadata": {"file": "main.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1634626064116, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1634626064116, "event_type": "POINT_IN_TIME", "key": "samples_per_epoch", "value": 168, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 40}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634626064117, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1, "epoch_count": 20}}
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/usr/local/lib/python3.8/dist-packages/nvidia/dali/pipeline.py:200: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
:::MLLOG {"namespace": "", "time_ms": 1634626065558, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 2332.6577514004275, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626065558, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.02989933774834437, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626065558, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 2332.6577514004275, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1}}
:::MLLOG {"namespace": "", "time_ms": 1634626065558, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626065558, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 20, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626066234, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4969.668411633504, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626066235, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.04976556291390729, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626066235, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4969.668411633504, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626066235, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626066235, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 40, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626066894, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5102.005430414666, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626066894, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.0696317880794702, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626066894, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5102.005430414666, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 40}}
:::MLLOG {"namespace": "", "time_ms": 1634626066894, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626066894, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 60, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626067539, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5213.488184542945, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626067539, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.08949801324503312, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626067540, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5213.488184542945, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 60}}
:::MLLOG {"namespace": "", "time_ms": 1634626067540, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626067540, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 80, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626068169, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5343.337933712081, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626068169, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.10936423841059603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626068170, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5343.337933712081, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 80}}
:::MLLOG {"namespace": "", "time_ms": 1634626068170, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626068170, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626068794, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5382.7000389966, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626068795, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.12923046357615892, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626068795, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5382.7000389966, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 100}}
:::MLLOG {"namespace": "", "time_ms": 1634626068795, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626068795, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626069424, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5346.851192325772, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626069424, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.14909668874172186, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626069424, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5346.851192325772, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 120}}
:::MLLOG {"namespace": "", "time_ms": 1634626069424, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626069424, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626070062, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5265.733439847194, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626070063, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.16896291390728477, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626070063, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5265.733439847194, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 140}}
:::MLLOG {"namespace": "", "time_ms": 1634626070063, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626070063, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626070695, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5317.547225626506, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626070696, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.18882913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626070696, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5317.547225626506, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 160}}
:::MLLOG {"namespace": "", "time_ms": 1634626070696, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626070696, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626071321, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5374.062710090242, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626071322, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.20869536423841056, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626071322, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5374.062710090242, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 180}}
:::MLLOG {"namespace": "", "time_ms": 1634626071322, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626071322, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626071953, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5327.789665733012, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626071953, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.22856158940397348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626071953, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5327.789665733012, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 200}}
:::MLLOG {"namespace": "", "time_ms": 1634626071953, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626071954, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626072582, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5351.000874439522, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626072582, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.24842781456953641, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626072582, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5351.000874439522, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 220}}
:::MLLOG {"namespace": "", "time_ms": 1634626072582, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626072582, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626073215, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5309.4795995754785, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626073216, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.26829403973509935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626073216, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5309.4795995754785, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 240}}
:::MLLOG {"namespace": "", "time_ms": 1634626073216, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626073216, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626073846, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5336.266651621627, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626073846, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.28816026490066227, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626073847, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5336.266651621627, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 260}}
:::MLLOG {"namespace": "", "time_ms": 1634626073847, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626073847, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626074476, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5343.814071512182, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626074476, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3080264900662252, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626074476, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5343.814071512182, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 280}}
:::MLLOG {"namespace": "", "time_ms": 1634626074476, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626074476, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626075102, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5367.572217084686, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626075103, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.32789271523178803, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626075103, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5367.572217084686, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 300}}
:::MLLOG {"namespace": "", "time_ms": 1634626075103, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626075103, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626075729, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5369.85468900032, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626075730, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.347758940397351, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626075730, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5369.85468900032, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 320}}
:::MLLOG {"namespace": "", "time_ms": 1634626075730, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626075730, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626076365, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5293.300260366977, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626076365, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.36762516556291386, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626076365, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5293.300260366977, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 340}}
:::MLLOG {"namespace": "", "time_ms": 1634626076366, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626076366, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626076989, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5391.396114851294, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626076989, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.3874913907284768, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626076989, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5391.396114851294, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 360}}
:::MLLOG {"namespace": "", "time_ms": 1634626076990, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626076990, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626077618, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5351.067923121261, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626077618, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.40735761589403974, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626077618, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5351.067923121261, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 380}}
:::MLLOG {"namespace": "", "time_ms": 1634626077618, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626077619, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626078249, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5334.170114500508, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626078249, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.42722384105960265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626078249, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5334.170114500508, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 400}}
:::MLLOG {"namespace": "", "time_ms": 1634626078249, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626078249, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626078872, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5397.404808191666, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626078872, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.44709006622516556, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626078873, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5397.404808191666, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 420}}
:::MLLOG {"namespace": "", "time_ms": 1634626078873, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626078873, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626079494, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.052848938504, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626079494, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4669562913907285, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626079494, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.052848938504, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 440}}
:::MLLOG {"namespace": "", "time_ms": 1634626079495, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626079495, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626080121, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5366.619716057215, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626080121, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.4868225165562914, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626080122, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5366.619716057215, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 460}}
:::MLLOG {"namespace": "", "time_ms": 1634626080122, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626080122, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626080745, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5394.716792766652, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626080745, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5066887417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626080745, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5394.716792766652, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 480}}
:::MLLOG {"namespace": "", "time_ms": 1634626080746, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626080746, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626081367, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5412.932261055465, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626081367, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5265549668874172, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626081367, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5412.932261055465, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 500}}
:::MLLOG {"namespace": "", "time_ms": 1634626081367, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626081367, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626081984, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5453.653143785452, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626081984, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5464211920529801, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626081984, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5453.653143785452, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 520}}
:::MLLOG {"namespace": "", "time_ms": 1634626081984, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626081984, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626082605, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5413.098590575516, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626082606, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.566287417218543, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626082606, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5413.098590575516, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 540}}
:::MLLOG {"namespace": "", "time_ms": 1634626082606, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626082606, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626083225, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5431.980832739752, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626083225, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.5861536423841059, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626083226, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5431.980832739752, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 560}}
:::MLLOG {"namespace": "", "time_ms": 1634626083226, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626083226, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626083849, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5390.589780614265, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626083850, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6060198675496689, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626083850, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5390.589780614265, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 580}}
:::MLLOG {"namespace": "", "time_ms": 1634626083850, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626083850, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626084472, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5404.709633501629, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626084472, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6258860927152318, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626084472, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5404.709633501629, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 600}}
:::MLLOG {"namespace": "", "time_ms": 1634626084472, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626084472, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626085090, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5440.391721320114, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626085091, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6457523178807947, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626085091, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5440.391721320114, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 620}}
:::MLLOG {"namespace": "", "time_ms": 1634626085091, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626085091, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626085718, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5359.782275007112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626085718, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6656185430463576, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626085719, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5359.782275007112, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 640}}
:::MLLOG {"namespace": "", "time_ms": 1634626085719, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626085719, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626086340, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5405.21335710275, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626086341, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.6854847682119205, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626086341, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5405.21335710275, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 660}}
:::MLLOG {"namespace": "", "time_ms": 1634626086341, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626086341, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626086968, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5360.430573126303, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626086969, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7053509933774835, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626086969, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5360.430573126303, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 680}}
:::MLLOG {"namespace": "", "time_ms": 1634626086969, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626086969, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626087587, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5433.545287250494, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626087588, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7252172185430463, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626087588, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5433.545287250494, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 700}}
:::MLLOG {"namespace": "", "time_ms": 1634626087588, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626087588, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626088216, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5355.9324157001465, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626088216, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7450834437086092, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626088216, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5355.9324157001465, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 720}}
:::MLLOG {"namespace": "", "time_ms": 1634626088216, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626088216, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626088837, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5418.126562345015, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626088837, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7649496688741722, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626088837, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5418.126562345015, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 740}}
:::MLLOG {"namespace": "", "time_ms": 1634626088837, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626088837, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626089457, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5428.904816606533, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626089457, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.7848158940397352, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626089457, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5428.904816606533, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 760}}
:::MLLOG {"namespace": "", "time_ms": 1634626089457, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626089457, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626090077, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5420.5147866543175, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626090078, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8046821192052981, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626090078, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5420.5147866543175, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 780}}
:::MLLOG {"namespace": "", "time_ms": 1634626090078, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626090078, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626090707, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5342.19757417191, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626090708, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8245483443708609, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626090708, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5342.19757417191, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 800}}
:::MLLOG {"namespace": "", "time_ms": 1634626090708, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626090708, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626091326, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5435.068720954897, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626091327, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8444145695364238, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626091327, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5435.068720954897, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 820}}
:::MLLOG {"namespace": "", "time_ms": 1634626091327, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626091327, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626091950, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5394.080821295442, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626091951, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8642807947019867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626091951, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5394.080821295442, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 840}}
:::MLLOG {"namespace": "", "time_ms": 1634626091951, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626091951, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626092572, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5414.7936552579495, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626092572, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.8841470198675497, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626092572, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5414.7936552579495, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 860}}
:::MLLOG {"namespace": "", "time_ms": 1634626092572, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626092572, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626093192, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5426.835176311302, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626093192, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9040132450331126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626093192, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5426.835176311302, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 880}}
:::MLLOG {"namespace": "", "time_ms": 1634626093193, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626093193, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626093812, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5426.18534226348, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626093813, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9238794701986754, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626093813, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5426.18534226348, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 900}}
:::MLLOG {"namespace": "", "time_ms": 1634626093813, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626093813, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626094443, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5332.930741603664, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626094444, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9437456953642384, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626094444, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5332.930741603664, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 920}}
:::MLLOG {"namespace": "", "time_ms": 1634626094444, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626094444, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626095066, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5404.058870424403, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626095066, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9636119205298014, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626095066, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5404.058870424403, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 940}}
:::MLLOG {"namespace": "", "time_ms": 1634626095066, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626095067, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626095683, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5452.176226219759, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626095683, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 0.9834781456953643, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626095683, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5452.176226219759, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 960}}
:::MLLOG {"namespace": "", "time_ms": 1634626095684, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626095684, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 980, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626096305, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5409.599509895369, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626096305, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0033443708609273, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626096305, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5409.599509895369, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 980}}
:::MLLOG {"namespace": "", "time_ms": 1634626096379, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626096380, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1000, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626096395, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634626096825, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8944250345230103, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634626096825, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634626097001, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5406.480336338327, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626097002, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0232105960264901, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626097002, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5406.480336338327, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1634626097090, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626097090, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1020, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626097091, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634626097506, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8850991725921631, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634626097507, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1020, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634626097729, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5262.791678336564, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626097729, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.043076821192053, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626097729, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5262.791678336564, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1020}}
:::MLLOG {"namespace": "", "time_ms": 1634626097805, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626097805, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1040, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626097819, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634626098234, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8834697008132935, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634626098234, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1040, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634626098449, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5223.35847498523, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626098449, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.062943046357616, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626098449, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5223.35847498523, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1040}}
:::MLLOG {"namespace": "", "time_ms": 1634626098478, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626098479, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1060, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626098495, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634626098927, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8839671611785889, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634626098928, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1060, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634626099123, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5213.065840340937, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626099124, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.0828092715231787, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626099124, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5213.065840340937, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1060}}
:::MLLOG {"namespace": "", "time_ms": 1634626099159, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626099160, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1080, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626099175, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634626099596, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8732030391693115, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634626099596, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1080, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634626099793, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5307.553966751103, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626099793, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1026754966887418, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626099793, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5307.553966751103, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1080}}
:::MLLOG {"namespace": "", "time_ms": 1634626099829, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626099829, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1100, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626099844, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634626100267, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8893669247627258, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634626100267, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1100, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634626100460, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5322.998252718937, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626100461, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1225417218543046, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626100461, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5322.998252718937, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1634626100498, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626100498, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1120, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626100513, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634626100944, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8781996965408325, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634626100944, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1120, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634626101147, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5181.439081369211, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626101147, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1424079470198676, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626101148, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5181.439081369211, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1120}}
:::MLLOG {"namespace": "", "time_ms": 1634626101175, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626101175, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1140, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626101192, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634626101628, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8940846920013428, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634626101628, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1140, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634626101836, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5087.6664265948975, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626101836, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1622741721854304, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626101836, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5087.6664265948975, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1140}}
:::MLLOG {"namespace": "", "time_ms": 1634626101879, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626101880, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1160, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626101895, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634626102316, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8825825452804565, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634626102316, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1160, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634626102510, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5333.782496255945, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626102510, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.1821403973509934, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626102510, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5333.782496255945, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1160}}
:::MLLOG {"namespace": "", "time_ms": 1634626102545, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626102546, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1180, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626102561, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634626102992, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8925870656967163, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634626102992, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1180, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634626103196, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5171.551833087836, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626103196, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2020066225165562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626103196, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5171.551833087836, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1180}}
:::MLLOG {"namespace": "", "time_ms": 1634626103251, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626103252, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1200, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626103266, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634626103685, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8821415901184082, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634626103685, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634626103885, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5304.533359831795, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626103886, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.2218728476821192, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626103886, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5304.533359831795, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1634626103990, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626103990, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1220, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626104005, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634626104445, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.894415020942688, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634626104445, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1220, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634626104667, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4966.216652230327, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626104667, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.241739072847682, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626104668, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4966.216652230327, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1220}}
:::MLLOG {"namespace": "", "time_ms": 1634626104728, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626104728, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1240, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626104743, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634626105216, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8821831941604614, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634626105217, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1240, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634626105435, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4751.722002816737, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626105436, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.261605298013245, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626105436, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4751.722002816737, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1240}}
:::MLLOG {"namespace": "", "time_ms": 1634626105470, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626105471, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1260, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626105486, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634626105910, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8845794200897217, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634626105910, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1260, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634626106113, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5235.325005321562, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626106113, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.281471523178808, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626106113, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5235.325005321562, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1260}}
:::MLLOG {"namespace": "", "time_ms": 1634626106168, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626106168, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1280, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626106182, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634626106595, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8744489550590515, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634626106595, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1280, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634626106790, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5405.155310090899, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626106790, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3013377483443709, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626106791, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5405.155310090899, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1280}}
:::MLLOG {"namespace": "", "time_ms": 1634626106851, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626106851, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1300, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626106866, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634626107301, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8554353713989258, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634626107301, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1300, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634626107515, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5059.5648867842565, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626107516, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3212039735099337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626107516, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5059.5648867842565, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1634626107568, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626107568, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1320, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626107582, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634626107997, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.883370041847229, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634626107997, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1320, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634626108195, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5360.989295414603, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626108196, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3410701986754967, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626108196, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5360.989295414603, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1320}}
:::MLLOG {"namespace": "", "time_ms": 1634626108247, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626108247, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1340, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626108261, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634626108696, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8787763714790344, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634626108696, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1340, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634626108900, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5151.409894766623, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626108900, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3609364238410595, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626108900, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5151.409894766623, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1340}}
:::MLLOG {"namespace": "", "time_ms": 1634626108954, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626108954, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1360, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626108968, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634626109388, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8863310217857361, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634626109388, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1360, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634626109585, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5322.413248871813, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626109586, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.3808026490066225, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626109586, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5322.413248871813, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1360}}
:::MLLOG {"namespace": "", "time_ms": 1634626109644, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626109644, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1380, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626109660, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634626110084, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.897045910358429, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634626110084, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1380, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634626110287, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5228.542356506767, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626110288, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4006688741721853, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626110288, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5228.542356506767, "iterations": 2, "loss_scale": 512.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1380}}
:::MLLOG {"namespace": "", "time_ms": 1634626110341, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626110341, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1400, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626110355, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634626110780, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8915781378746033, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634626110780, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634626110984, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5226.277624762705, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626110985, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4205350993377484, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626110985, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5226.277624762705, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1634626111042, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626111042, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1420, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626111056, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634626111468, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8840435743331909, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634626111468, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1420, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634626111663, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5414.620980489038, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626111663, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4404013245033112, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626111663, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5414.620980489038, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1420}}
:::MLLOG {"namespace": "", "time_ms": 1634626111718, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626111719, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1440, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626111733, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634626112144, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8912903070449829, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634626112144, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1440, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634626112352, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5305.557823167667, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626112353, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4602675496688742, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626112353, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5305.557823167667, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1440}}
:::MLLOG {"namespace": "", "time_ms": 1634626112399, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626112399, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1460, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626112413, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634626112858, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8921562433242798, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634626112858, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1460, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634626113075, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4970.061000489852, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626113076, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.4801337748344372, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626113076, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4970.061000489852, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1460}}
:::MLLOG {"namespace": "", "time_ms": 1634626113175, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626113175, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1480, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626113189, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634626113588, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8941863775253296, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634626113588, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1480, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634626113791, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5452.587572868573, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626113792, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626113792, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5452.587572868573, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1480}}
:::MLLOG {"namespace": "", "time_ms": 1634626113836, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626113837, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1500, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626113851, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634626114299, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8886476159095764, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634626114299, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634626114505, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5030.811099092924, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626114505, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626114505, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5030.811099092924, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1634626114550, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626114551, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1520, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626114565, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634626114980, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8871399164199829, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634626114980, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1520, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634626115180, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5341.871557296145, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626115180, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626115180, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5341.871557296145, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1520}}
:::MLLOG {"namespace": "", "time_ms": 1634626115234, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626115234, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1540, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626115248, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634626115662, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8963146209716797, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634626115662, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1540, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634626115879, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5210.883854901359, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626115880, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626115880, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5210.883854901359, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1540}}
:::MLLOG {"namespace": "", "time_ms": 1634626115942, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626115942, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1560, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626115957, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634626116399, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8875503540039062, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634626116399, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1560, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634626116638, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4831.893981861962, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626116638, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626116638, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4831.893981861962, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1560}}
:::MLLOG {"namespace": "", "time_ms": 1634626116699, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626116699, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1580, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626116714, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634626117125, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8919600248336792, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634626117125, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1580, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634626117357, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5109.682292327606, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626117357, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626117357, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5109.682292327606, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1580}}
:::MLLOG {"namespace": "", "time_ms": 1634626117410, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626117410, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1600, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626117424, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634626117851, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.899248480796814, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634626117851, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634626118053, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5229.184516741867, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626118053, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626118053, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5229.184516741867, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1634626118119, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626118119, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1620, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626118134, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634626118545, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8971461057662964, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634626118545, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1620, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634626118746, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5360.05543811553, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626118747, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626118747, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5360.05543811553, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1620}}
:::MLLOG {"namespace": "", "time_ms": 1634626118819, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626118819, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1640, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626118834, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634626119244, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8944173455238342, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634626119245, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1640, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634626119448, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5344.766601725831, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626119448, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626119449, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5344.766601725831, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1634626119502, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626119503, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1660, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626119517, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634626119929, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8702812194824219, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634626119929, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1660, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634626120126, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5387.331843484869, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626120127, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626120127, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5387.331843484869, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1660}}
:::MLLOG {"namespace": "", "time_ms": 1634626120183, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626120183, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1680, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626120197, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634626120637, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8959221243858337, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634626120637, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1680, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634626120840, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5115.749439432786, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626120840, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626120841, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5115.749439432786, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1680}}
:::MLLOG {"namespace": "", "time_ms": 1634626120901, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626120902, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1700, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626120917, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634626121338, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8828300833702087, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634626121339, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1700, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634626121548, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5198.453337031884, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626121549, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626121549, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5198.453337031884, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1634626121601, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626121602, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1720, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626121616, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634626122041, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9007320404052734, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634626122041, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1720, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634626122246, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5213.345466975039, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626122247, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626122247, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5213.345466975039, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1720}}
:::MLLOG {"namespace": "", "time_ms": 1634626122310, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626122310, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1740, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626122324, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634626122731, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8947912454605103, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634626122731, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1740, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634626122927, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5445.485276218224, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626122928, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626122928, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5445.485276218224, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1740}}
:::MLLOG {"namespace": "", "time_ms": 1634626122999, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626122999, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1760, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626123014, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634626123413, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8604660034179688, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634626123413, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1760, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634626123608, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5520.788438862533, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626123608, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626123609, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5520.788438862533, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1760}}
:::MLLOG {"namespace": "", "time_ms": 1634626123664, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626123665, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1780, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626123678, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634626124109, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8988381624221802, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634626124109, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1780, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634626124316, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5159.478035666757, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626124316, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626124316, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5159.478035666757, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1780}}
:::MLLOG {"namespace": "", "time_ms": 1634626124381, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626124382, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1800, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626124396, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634626124802, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8895918130874634, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634626124802, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1800, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634626125000, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5440.188009310908, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626125000, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626125000, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5440.188009310908, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1634626125068, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626125068, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1820, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626125083, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634626125499, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8922407627105713, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634626125499, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1820, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634626125696, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5356.133937730197, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626125696, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626125696, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5356.133937730197, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1820}}
:::MLLOG {"namespace": "", "time_ms": 1634626125734, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626125734, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1840, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626125749, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634626126176, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9000484943389893, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634626126176, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1840, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634626126386, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5158.6942551355005, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626126386, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626126386, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5158.6942551355005, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1840}}
:::MLLOG {"namespace": "", "time_ms": 1634626126439, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626126440, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1860, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626126454, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634626126918, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8971630334854126, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634626126918, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1860, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634626127134, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4839.815883942183, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626127134, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626127135, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4839.815883942183, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1860}}
:::MLLOG {"namespace": "", "time_ms": 1634626127190, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626127191, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1880, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626127205, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634626127654, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8908202052116394, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634626127655, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1880, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634626127867, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 4972.935447536833, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626127867, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626127867, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 4972.935447536833, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1880}}
:::MLLOG {"namespace": "", "time_ms": 1634626127923, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626127924, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1900, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626127938, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634626128368, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8892289996147156, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634626128368, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1900, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634626128570, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5197.090308859848, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626128571, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626128571, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5197.090308859848, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1634626128630, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626128630, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1920, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626128644, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634626129064, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.8943610191345215, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634626129064, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1920, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634626129266, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5286.055351653854, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626129266, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626129266, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5286.055351653854, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1634626129319, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626129319, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1940, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626129333, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634626129750, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9058365821838379, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634626129751, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1940, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634626129945, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5376.077930817173, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626129945, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626129945, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5376.077930817173, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1940}}
:::MLLOG {"namespace": "", "time_ms": 1634626130006, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 176, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626130007, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 51, "first_epoch_num": 1960, "epoch_count": 20}}
:::MLLOG {"namespace": "", "time_ms": 1634626130021, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 152, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634626130419, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9091204404830933, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 155, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634626130419, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1960, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 160, "epoch_num": 1960}}
:::MLLOG {"namespace": "", "time_ms": 1634626130419, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 165, "status": "success"}}
:::MLLOG {"namespace": "", "time_ms": 1634626130616, "event_type": "POINT_IN_TIME", "key": "throughput", "value": 5519.465163149522, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 117}}
:::MLLOG {"namespace": "", "time_ms": 1634626130616, "event_type": "POINT_IN_TIME", "key": "current_lr", "value": 1.5, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 118}}
:::MLLOG {"namespace": "", "time_ms": 1634626130616, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 5519.465163149522, "iterations": 2, "loss_scale": 1024.0}, "metadata": {"file": "/workspace/unet3d/runtime/training.py", "lineno": 119, "step": 1960}}
ENDING TIMING RUN AT 2021-10-19 06:48:56 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:56 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:56 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:56 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:56 AM
RESULT,image_segmentation,,198,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:57 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:57 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:57 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:57 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:57 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:57 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:57 AM
RESULT,image_segmentation,,199,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:48:58 AM
RESULT,image_segmentation,,200,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:00 AM
RESULT,image_segmentation,,202,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:01 AM
ENDING TIMING RUN AT 2021-10-19 06:49:01 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:01 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:01 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:01 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:01 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:01 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:01 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:01 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:01 AM
RESULT,image_segmentation,,203,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:02 AM
RESULT,image_segmentation,,204,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:03 AM
RESULT,image_segmentation,,205,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:04 AM
RESULT,image_segmentation,,206,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:05 AM
RESULT,image_segmentation,,207,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:06 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:06 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:06 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:06 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:06 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:06 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:06 AM
RESULT,image_segmentation,,208,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:07 AM
RESULT,image_segmentation,,209,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:08 AM
RESULT,image_segmentation,,210,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:09 AM
RESULT,image_segmentation,,211,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 06:45:38 AM
ENDING TIMING RUN AT 2021-10-19 06:49:10 AM
RESULT,image_segmentation,,212,nvidia,2021-10-19 06:45:38 AM
